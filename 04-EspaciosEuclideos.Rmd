# Espacios Euclídeos

```{r echo = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      results = "asis",
                      message = FALSE,
                      warning = FALSE)

set.seed(1234)
``` 

En este capítulo, damos un paso para acercarnos a los conceptos geométricos clásicos de distancias y ángulos, abstrayéndolos en el contexto de los espacios vectoriales.

Para ello, precisaremos de la noción de _producto escalar_: Una aplicación $g:V\times V\to \mathbb{R}$, donde $V$ es un espacio vectorial sobre $\mathbb{R}$, es un __producto escalar__ si verifica:

- Es simétrica: $g(u, v) = g(v, u)$ para todo $u,v\in V$.
- Es bilineal: $g(\alpha u + \beta v, w) = \alpha g(u, w) + \beta g(v, w)$ para todos los $\alpha,\beta\in\mathbb{R}$, $u,v,w\in V$. Por simetría, se tiene lo mismo para la linealidad en la segunda componente.
- Es definida positiva: $g(v, v) > 0$ para todo $v\in V\setminus\{0\}$, $g(v, v) = 0$ si, y sólo si $v = 0$.

En resumen, se suele definir un producto escalar como una _forma bilineal, simétrica y definida positiva_.

__Notación__: El producto escalar de dos vectores $u$ y $v$ suele recibir diversas notaciones, aunque las más usadas son $u\cdot v$ y $\langle u, v \rangle$.

A un espacio vectorial $V$ donde podemos definir un producto vectorial lo llamamos __espacio euclídeo__.

---

__Ejemplo__

En $\mathbb{R}^n$, disponemos de un _producto escalar usual_, de forma que si $u, v\in\mathbb{R}^n$, entonces
$$\langle u,v \rangle = u_1v_1+u_2v_2+\ldots+u_nv_n$$
siendo $u_i, v_i$, $i=1,\ldots,n$ las coordenadas de $u$ y de $v$, respectivamente, en una base dada (la canónica como caso particular). A este producto vectorial es al que nos referiremos cuando usemos la notación $u\cdot v$.

Podemos generalizar este producto escalar, definiendo unos pesos $\alpha_i\in\mathbb{R}^{+}$ para cada componente:
$$\langle u,v \rangle = \alpha_1u_1v_1+\alpha_2u_2v_2+\ldots+\alpha_nu_nv_n$$

Podemos construir productos escalares de la siguiente forma: Tomemos una matriz $A\in\mathcal{M}_n(\mathbb{R})$ simétrica y definida positiva. Entonces la aplicación $\langle\cdot,\cdot\rangle:\mathbb{R}^n\times\mathbb{R}^n\to\mathbb{R}$ definida por
$$\langle u, v \rangle = u^{\text{t}}\ A\ v$$
es un producto escalar. Los dos ejemplos anteriores se correspondían con tomar como $A$ la matriz identidad o una matriz diagonal con elementos positivos $\alpha_i\in\mathbb{R}^{+}$ en la diagonal.

---

__¿Qué preguntas vamos a responder en este capítulo?__

- [¿Qué son las normas y distancias?](#norm)
- [¿Qué significa el concepto de ortogonalidad?](#ortho)
- [¿Cómo construimos una base ortonormal?](#ortho-basis)
- [¿Qué es el complemento ortogonal de un subespacio?](#compl-ortho)
- [¿Cómo calculamos la proyección de un vector sobre un subespacio?](#proyec)
- [¿Qué es la diagonalización ortogonal?](#diag-ortho)

## Qué son las normas y distancias {#norm}

Una __norma__ vectorial sobre un espacio $V$ (sobre un cuerpo $\mathbb{K}$) es un operador que sirve para _medir_ la longitud de un vector (es decir, su distancia al origen), y se define formalmente como una aplicación $\|\cdot\|:V\to\mathbb{R}$ que verifica:

- Para todo $v\in V$, $\|v\| > 0$, con $\|v\| = 0$ si, y solo si, $v = 0$.
- Para todo $c\in\mathbb{K}$, $v\in V$, se tiene $\|cv\| = |c|\cdot \|v\|$.
- Para todo $u,v\in V$, $\|u+v\|\le\|u\|+\|v\|$. Esta propiedad se llama _desigualdad triangular_ (en un triángulo, la suma de la longitud de dos de sus lados -- $\|u\|$ y $\|v\|$ -- es siempre menor que la longitud de su otro lado -- $\|u+v\|$ -- ).

Cualquier operador $\|\cdot\|$ que verifique lo anterior es una norma vectorial. Sin embargo, podemos construir normas a partir de un producto escalar:

> Si $\langle\cdot,\cdot\rangle$ es un producto escalar en el espacio $V$, entonces la aplicación $\|\cdot\|:V\to\mathbb{R}$ dada por $\|v\| = \sqrt{\langle v, v\rangle}$ es una __norma derivada del producto escalar__ sobre $V$.

Al definir la norma vectorial a partir de un producto escalar, además, tenemos la siguiente propiedad que será de gran interés en la [siguiente sección](#ortho):

> (Desigualdad de Cauchy-Schwarz) Si $\|\cdot\|$ es una norma derivada del producto escalar $\langle\cdot,\cdot\rangle$, entonces
$$|\langle u, v \rangle| \le \|u\|\cdot\|v\|$$
para todo $u,v\in V$.

Un concepto implícitamente relacionado con el de norma vectorial es el de _distancia_. Una __distancia__ (también llamada _métrica_) es un operador $d:V\times V\to R$ que verifica las siguientes propiedades:

- $d(u, v) = 0$ si, y solo si, $u = v$.
- Es simétrica: $d(u, v) = d(v, u)$.
- Cumple la desigualdad triangular: $d(u, v) \le d(u, w) + d(w, v)$.

Además, de aquí se deduce que $d(u, v) \ge 0$ para todo $u,v\in V$.

Podemos deducir una distancia a partir de una norma, para así completar las relaciones existentes entre los conceptos aquí explicados:

> Si $\|\cdot\|$ es una norma sobre un espacio vectorial $V$, entonces la aplicación $d:V\times V\to \mathbb{R}$ definida por
$$d(u, v) = \|u-v\|$$
para todo $u,v\in V$ es una _distancia_.

__Nota__: Existen muchas más normas de las que se deducen a partir de un producto escalar, al igual que más _distancias_.

---

__Ejemplo__

Podemos deducir la expresión de la norma (a veces llamada _módulo_) de un vector $v\in\mathbb{R}^n$.

Si $v=\left(\begin{array}{c}v_1\\ v_2\\\vdots\\ v_n\end{array}\right)$, entonces
$$\|v\| = \sqrt{v\cdot v} = \sqrt{v_1^2+v_2^2+\ldots+v_n^2}$$

Y así, además, deducimos la expresión para la distancia euclídea usual en $\mathbb{R}^n$:
$$d(u,v) = \|u - v\| = \sqrt{(u_1-v_1)^2+\ldots+(u_n-v_n)^2}$$

Como ejemplo, podemos calcular la norma del vector
```{r}
v <- rVector(n = n)
glue_latex(
  "v = [to_latex(v)]"
) %>% 
  cat()
v_str <- as.character(v)
v_str[v < 0] <- paste0("(", v_str[v < 0], ")")
glue_latex(
  "\\|v\\| = \\sqrt{[str_flatten(glue::glue('{v_str}^2'), '+')]} = \\sqrt{[sum(v^2)]}"
) %>% 
  cat()
```


---

## Qué significa el concepto de ortogonalidad {#ortho}

Si partimos de la _desigualdad de Cauchy-Schwarz_ que [hemos visto antes](#norm), podemos llegar a que 
$$-1 \le \frac{\langle u, v\rangle}{\|u\|\cdot\|v|} \le 1$$
para todo $u,v\in V$.

Por otro lado, la función coseno restringida a $[0, \pi]$, es decir, $\cos:[0,\pi]\to[-1,1]$, es biyectiva. Eso quiere decir que existe un único $\theta\in[0, \pi]$ tal que $\cos\theta = \frac{\langle u, v\rangle}{\|u\|\cdot\|v|}$. 

Llamamos entonces __ángulo entre los vectores $u,v\in V$__ al único $\theta\in[0,\pi]$ tal que $\cos\theta = \frac{\langle u, v\rangle}{\|u\|\cdot\|v|}$.

Diremos que dos vectores $u$ y $v$ de $V$ son __ortogonales__ (o __perpendiculares__) si el ángulo que forman es $\frac{\pi}{2}$, es decir, si $\langle u, v \rangle = 0$.

La noción de _ortogonalidad_ en el espacio euclídeo se corresponde con la perpendicularidad de vectores que todos tenemos de forma intuitiva. Lo podemos extender también a ortogonalidad con respecto a un espacio vectorial: Un vector $v\in V$ es __ortogonal al subespacio $U$__ de $V$ si, y sólo si, es ortogonal a todos y cada uno de los vectores de $U$, es decir, $\langle v, u \rangle = 0$ para todo $u\in U$.

> Sea $U$ un subespacio de $V$, y sea $\mathcal{B} = \{u_1,\ldots,u_k\}$ una base de $U$. Un vector $v\in V$ es ortogonal a $U$ si, y solo si, es ortogonal a cada $u_i$: $\langle v, u_i \rangle = 0$ para todo $i=1,\ldots,k$.

Un __sistema ortogonal__ es un conjunto de vectores $\{v_1,\ldots,v_m\}\subset V$ donde los $v_i$ son ortogonales dos a dos, es decir, $\langle v_i, v_j \rangle = 0$ para todo $i\ne j$.

__Algunas propiedades y caracterizaciones de los vectores y sistemas ortogonales__

> - Dos vectores $u$ y $w$ son ortogonales si, y solo si, $\|u+w\|^2 = \|u\|^2 + \|v\|^2$ (generalización del Teorema de Pitágoras).
- Si un sistema de vectores es ortogonal, entonces es linealmente independiente. Por tanto, en un espacio $V$ de dimensión finita $n$, cualquier conjunto de $n$ vectores ortogonales entre sí es una base.

__Ortonormalidad__

Un vector $v\in V$ se dice __unitario__ si $\|v\| = 1$. Es fácil crear vectores unitarios: dado $u\in V$, si definimos $v = \frac{1}{\|u\|}u$, este vector tiene norma 1. A este proceso se le llama _normalizar_.

Un sistema de vectores $\{v_1,\ldots,v_k\}$ se llama __sistema ortonormal__ si es un sistema ortogonal y cada $v_i$ es un vector unitario ($i=1,\ldots, k$).

En relación a lo comentado de las propiedades de los sistemas ortogonales, en nuestro caso podemos decir que un sistema ortonormal $\{v_1,\ldots,v_n\}$, donde $\mathrm{dim}(V) = n$, es una __base ortonormal__.

El interés de una base ortonormal es que es sencillo calcular las [coordenadas](#coord) de cualquier vector con respecto a dicha base:

> Sea $\mathcal{B} = \{v_1,\ldots,v_n\}$ una base _ortonormal_ del espacio vectorial $V$. Entonces, para cada $v\in V$, tenemos:
$$v = \langle v, v_1 \rangle v_1 + \ldots \langle v, v_n \rangle v_n$$
luego las coordenadas de $v$ en la base $\mathcal{B}$ las podemos calcular como:
$$[v]_{\mathcal{B}} = \left(
\begin{array}{c}
\langle v, v_1 \rangle \\
\langle v, v_2 \rangle \\
\vdots \\
\langle v, v_n \rangle \\
\end{array}
\right)_{\mathcal{B}}$$

---

__Ejemplo__

```{r}
n <- 4
# Construimos un espacio U dando su base
# Nos aseguramos que tenga dimensión 2
repeat {
  
  bU <- rMatrix(n, m = 2)
  bU <- linearly_independents(bU)
  if (ncol(bU) == 2) break
  
}
# Vamos a construir sus ecuaciones cartesianas:
AU <- parametric_to_cartesian(bU)$A %>% remove_fraction()
bU <- solve_homogeneous(AU)

# Vamos a buscar un vector ortogonal a U
# Encontramos la base del complemento ortogonal
bUt <- solve_homogeneous(t(bU)) %>%  
  linearly_independents() %>% t() %>% 
  remove_fraction() %>% t()

# Vector v aleatorio en el ortogonal
scalars <- rVector(n = ncol(bUt))
v <- bUt %*% scalars

# Construyamos ahora una base aleatoria y la 
# ortonormalizamos usando GS
repeat {
  
  B <- rMatrix(n = n, m = n) %>% 
    linearly_independents()
  
  if (ncol(B) == n) break
  
}
c(B, s) %<-% gram_schmidt(B)

```

- Ángulo entre los vectores de la base de $U$
- Comprobar que $v$ es ortogonal a $U$
- Dar un sistema ortogonal -> base
- Normalizar el sistema ortogonal -> base ortonormal

---

## Cómo construimos una base ortonormal {#ortho-basis}

- Teorema de existencia
- Gram-Schmidt

---

__Ejemplo__

---

## Qué es el complemento ortogonal de un subespacio {#compl-ortho}

- Definición de complemento ortogonal
- Método de cálculo

---

__Ejemplo__

```{r}
bW <- rMatrix(n = n, m = 2)
bWt <- solve_homogeneous(bW)
```

---

## Cómo calculamos la proyección de un vector sobre un subespacio {#proyec}

- Definición de proyección ortogonal sobre otro vector
- Propiedades
- Definición de proyección sobre un subespacio
- Teorema de descomposición ortogonal

---

__Ejemplo__

---

## Qué es la diagonalización ortogonal {#diag-ortho}

- Definición de aplicación ortogonal y de matriz ortogonal 
- Teorema de relación entre matriz ortogonal y base ortonormal
- Definición de matriz diagonalizable ortogonalmente
- Teorema diagonalización ortogonal
- Teoremas que relacionan matrices simétricas con diagonabilidad ortogonal
- Procedimiento diagonalización ortogonal matriz simétrica


---

__Ejemplo__

---
