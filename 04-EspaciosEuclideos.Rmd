# Espacios Euclídeos

```{r echo = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      results = "asis",
                      message = FALSE,
                      warning = FALSE)

set.seed(1234)
``` 

En este capítulo, damos un paso para acercarnos a los conceptos geométricos clásicos de distancias y ángulos, abstrayéndolos en el contexto de los espacios vectoriales.

Para ello, precisaremos de la noción de _producto escalar_: Una aplicación $g:V\times V\to \mathbb{R}$, donde $V$ es un espacio vectorial sobre $\mathbb{R}$, es un __producto escalar__ si verifica:

- Es simétrica: $g(u, v) = g(v, u)$ para todo $u,v\in V$.
- Es bilineal: $g(\alpha u + \beta v, w) = \alpha g(u, w) + \beta g(v, w)$ para todos los $\alpha,\beta\in\mathbb{R}$, $u,v,w\in V$. Por simetría, se tiene lo mismo para la linealidad en la segunda componente.
- Es definida positiva: $g(v, v) > 0$ para todo $v\in V\setminus\{0\}$, $g(v, v) = 0$ si, y sólo si $v = 0$.

En resumen, se suele definir un producto escalar como una _forma bilineal, simétrica y definida positiva_.

__Notación__: El producto escalar de dos vectores $u$ y $v$ suele recibir diversas notaciones, aunque las más usadas son $u\cdot v$ y $\langle u, v \rangle$.

A un espacio vectorial $V$ donde podemos definir un producto vectorial lo llamamos __espacio euclídeo__.

---

__Ejemplo__

En $\mathbb{R}^n$, disponemos de un _producto escalar usual_, de forma que si $u, v\in\mathbb{R}^n$, entonces
$$\langle u,v \rangle = u_1v_1+u_2v_2+\ldots+u_nv_n$$
siendo $u_i, v_i$, $i=1,\ldots,n$ las coordenadas de $u$ y de $v$, respectivamente, en una base dada (la canónica como caso particular). A este producto vectorial es al que nos referiremos cuando usemos la notación $u\cdot v$.

Podemos generalizar este producto escalar, definiendo unos pesos $\alpha_i\in\mathbb{R}^{+}$ para cada componente:
$$\langle u,v \rangle = \alpha_1u_1v_1+\alpha_2u_2v_2+\ldots+\alpha_nu_nv_n$$

Podemos construir productos escalares de la siguiente forma: Tomemos una matriz $A\in\mathcal{M}_n(\mathbb{R})$ simétrica y definida positiva. Entonces la aplicación $\langle\cdot,\cdot\rangle:\mathbb{R}^n\times\mathbb{R}^n\to\mathbb{R}$ definida por
$$\langle u, v \rangle = u^{\text{t}}\ A\ v$$
es un producto escalar. Los dos ejemplos anteriores se correspondían con tomar como $A$ la matriz identidad o una matriz diagonal con elementos positivos $\alpha_i\in\mathbb{R}^{+}$ en la diagonal.

---

__¿Qué preguntas vamos a responder en este capítulo?__

- [¿Qué son las normas y distancias?](#norm)
- [¿Qué significa el concepto de ortogonalidad?](#ortho)
- [¿Qué es y qué utilidad tiene una base ortonormal?](#ortho-basis)
- [¿Cómo construimos una base ortonormal?](#gram-schmidt)
- [¿Qué es el complemento ortogonal de un subespacio?](#compl-ortho)
- [¿Cómo calculamos la proyección de un vector sobre un subespacio?](#proyec)
- [¿Qué es una matriz ortogonal y una aplicación ortogonal?](#ortho-matrix)
- [¿Qué es la diagonalización ortogonal?](#diag-ortho)
- [Qué matrices son diagonalizables ortogonalmente](#diag-ortho2)

## Qué son las normas y distancias {#norm}

Una __norma__ vectorial sobre un espacio $V$ (sobre un cuerpo $\mathbb{K}$) es un operador que sirve para _medir_ la longitud de un vector (es decir, su distancia al origen), y se define formalmente como una aplicación $\|\cdot\|:V\to\mathbb{R}$ que verifica:

- Para todo $v\in V$, $\|v\| > 0$, con $\|v\| = 0$ si, y solo si, $v = 0$.
- Para todo $c\in\mathbb{K}$, $v\in V$, se tiene $\|cv\| = |c|\cdot \|v\|$.
- Para todo $u,v\in V$, $\|u+v\|\le\|u\|+\|v\|$. Esta propiedad se llama _desigualdad triangular_ (en un triángulo, la suma de la longitud de dos de sus lados -- $\|u\|$ y $\|v\|$ -- es siempre menor que la longitud de su otro lado -- $\|u+v\|$ -- ).

Cualquier operador $\|\cdot\|$ que verifique lo anterior es una norma vectorial. Sin embargo, podemos construir normas a partir de un producto escalar:

> Si $\langle\cdot,\cdot\rangle$ es un producto escalar en el espacio $V$, entonces la aplicación $\|\cdot\|:V\to\mathbb{R}$ dada por $\|v\| = \sqrt{\langle v, v\rangle}$ es una __norma derivada del producto escalar__ sobre $V$.

Al definir la norma vectorial a partir de un producto escalar, además, tenemos la siguiente propiedad que será de gran interés en la [siguiente sección](#ortho):

> (Desigualdad de Cauchy-Schwarz) Si $\|\cdot\|$ es una norma derivada del producto escalar $\langle\cdot,\cdot\rangle$, entonces
$$|\langle u, v \rangle| \le \|u\|\cdot\|v\|$$
para todo $u,v\in V$.

Un concepto implícitamente relacionado con el de norma vectorial es el de _distancia_. Una __distancia__ (también llamada _métrica_) es un operador $d:V\times V\to R$ que verifica las siguientes propiedades:

- $d(u, v) = 0$ si, y solo si, $u = v$.
- Es simétrica: $d(u, v) = d(v, u)$.
- Cumple la desigualdad triangular: $d(u, v) \le d(u, w) + d(w, v)$.

Además, de aquí se deduce que $d(u, v) \ge 0$ para todo $u,v\in V$.

Podemos deducir una distancia a partir de una norma, para así completar las relaciones existentes entre los conceptos aquí explicados:

> Si $\|\cdot\|$ es una norma sobre un espacio vectorial $V$, entonces la aplicación $d:V\times V\to \mathbb{R}$ definida por
$$d(u, v) = \|u-v\|$$
para todo $u,v\in V$ es una _distancia_.

__Nota__: Existen muchas más normas de las que se deducen a partir de un producto escalar, al igual que más _distancias_.

---

__Ejemplo__

Podemos deducir la expresión de la norma (a veces llamada _módulo_) de un vector $v\in\mathbb{R}^n$.

Si $v=\left(\begin{array}{c}v_1\\ v_2\\\vdots\\ v_n\end{array}\right)$, entonces
$$\|v\| = \sqrt{v\cdot v} = \sqrt{v_1^2+v_2^2+\ldots+v_n^2}$$

Y así, además, deducimos la expresión para la distancia euclídea usual en $\mathbb{R}^n$:
$$d(u,v) = \|u - v\| = \sqrt{(u_1-v_1)^2+\ldots+(u_n-v_n)^2}$$

Como ejemplo, podemos calcular la norma del vector
```{r}
v <- rVector(n = 4)
glue_latex(
  "v = [to_latex(v)]"
) %>% 
  cat()
v_str <- as.character(v)
v_str[v < 0] <- paste0("(", v_str[v < 0], ")")
glue_latex(
  "\\|v\\| = \\sqrt{[str_flatten(glue::glue('{v_str}^2'), '+')]} = \\sqrt{[sum(v^2)]}"
) %>% 
  cat()
```


---

## Qué significa el concepto de ortogonalidad {#ortho}

Si partimos de la _desigualdad de Cauchy-Schwarz_ que [hemos visto antes](#norm), podemos llegar a que 
$$-1 \le \frac{\langle u, v\rangle}{\|u\|\cdot\|v|} \le 1$$
para todo $u,v\in V$.

Por otro lado, la función coseno restringida a $[0, \pi]$, es decir, $\cos:[0,\pi]\to[-1,1]$, es biyectiva. Eso quiere decir que existe un único $\theta\in[0, \pi]$ tal que $\cos\theta = \frac{\langle u, v\rangle}{\|u\|\cdot\|v|}$. 

Llamamos entonces __ángulo entre los vectores $u,v\in V$__ al único $\theta\in[0,\pi]$ tal que $\cos\theta = \frac{\langle u, v\rangle}{\|u\|\cdot\|v|}$.

Diremos que dos vectores $u$ y $v$ de $V$ son __ortogonales__ (o __perpendiculares__) si el ángulo que forman es $\frac{\pi}{2}$, es decir, si $\langle u, v \rangle = 0$.

La noción de _ortogonalidad_ en el espacio euclídeo se corresponde con la perpendicularidad de vectores que todos tenemos de forma intuitiva. Lo podemos extender también a ortogonalidad con respecto a un espacio vectorial: Un vector $v\in V$ es __ortogonal al subespacio $U$__ de $V$ si, y sólo si, es ortogonal a todos y cada uno de los vectores de $U$, es decir, $\langle v, u \rangle = 0$ para todo $u\in U$.

> Sea $U$ un subespacio de $V$, y sea $\mathcal{B} = \{u_1,\ldots,u_k\}$ una base de $U$. Un vector $v\in V$ es ortogonal a $U$ si, y solo si, es ortogonal a cada $u_i$: $\langle v, u_i \rangle = 0$ para todo $i=1,\ldots,k$.

Un __sistema ortogonal__ es un conjunto de vectores $\{v_1,\ldots,v_m\}\subset V$ donde los $v_i$ son ortogonales dos a dos, es decir, $\langle v_i, v_j \rangle = 0$ para todo $i\ne j$.

__Algunas propiedades y caracterizaciones de los vectores y sistemas ortogonales__

> - Dos vectores $u$ y $w$ son ortogonales si, y solo si, $\|u+w\|^2 = \|u\|^2 + \|v\|^2$ (generalización del Teorema de Pitágoras).
- Si un sistema de vectores es ortogonal, entonces es linealmente independiente. Por tanto, en un espacio $V$ de dimensión finita $n$, cualquier conjunto de $n$ vectores ortogonales entre sí es una base.

Este último resultado es interesante, pues nos constata que cualquier sistema ortogonal con tantos vectores como la dimensión del espacio vectorial es una base.

---

__Ejemplo__

```{r}
n <- 4
# Construimos un espacio U dando su base
# Nos aseguramos que tenga dimensión 2
repeat {
  
  bU <- rMatrix(n, m = 2)
  bU <- linearly_independents(bU)
  if (ncol(bU) == 2) break
  
}
# Vamos a construir sus ecuaciones cartesianas:
AU <- parametric_to_cartesian(bU)$A %>% remove_fraction()
bU <- solve_homogeneous(AU)

# Vamos a buscar un vector ortogonal a U
# Encontramos la base del complemento ortogonal
bUt <- solve_homogeneous(t(bU)) %>%  
  linearly_independents() %>% t() %>% 
  remove_fraction() %>% t()

# Vector v aleatorio en el ortogonal
scalars <- rVector(n = ncol(bUt))
v <- bUt %*% scalars
w <- rVector(n = n)

# Construyamos ahora una base aleatoria y la 
# ortonormalizamos usando GS
repeat {
  
  B <- rMatrix(n = n, m = n * 4, values = 0:1) %>% 
    linearly_independents()
  
  if (ncol(B) == n) break
  
}
c(B_ortho, s) %<-% gram_schmidt(B)

B_ortho[] <- B_ortho %>% t() %>% 
  remove_fraction() %>% t() %>% 
  round()

```

Consideremos el espacio $U$ dado por:
```{r}
glue_latex(
  "U = \\left\\{",
  "[to_latex(generic_vector(n))]\\in\\mathbb{R}^{[n]}:",
  "[write_system(AU, zero_vector(AU), fractions = TRUE, latex = TRUE, format = 'c')]",
  "\\right\\}"
)
```
y los vectores
```{r}
glue_latex(
  "v = [to_latex(v)]\\quad\\quad w = [to_latex(w)]"
) %>% 
  cat()
```

Calculemos el ángulo entre $v$ y $w$:
```{r}
v_str <- as.character(v)
v_str[v < 0] <- paste0("(", v_str[v < 0], ")")
w_str <- as.character(w)
w_str[w < 0] <- paste0("(", w_str[w < 0], ")")
glue_latex(
  "\\theta = \\arccos \\frac{v\\cdot w}{\\|v\\|\\cdot\\|w\\|}"
) %>% 
  cat()
prod_str <- "\\cdot"
glue_latex(
  "v\\cdot w = [glue::glue('{v_str}{prod_str}{w_str}') %>% str_flatten('+')] = [sum(v * w)]"
) %>% 
  cat()
glue_latex(
  "\\|v\\| = \\sqrt{[glue::glue('{v_str}^2') %>% str_flatten('+')]} = \\sqrt{[sum(v^2)]}"
) %>% 
  cat()
glue_latex(
  "\\|w\\| = \\sqrt{[glue::glue('{w_str}^2') %>% str_flatten('+')]} = \\sqrt{[sum(w^2)]}"
) %>% 
  cat()

vv <- Vector$new(v)
vw <- Vector$new(w)
dot_prod <- vv$prod(vw)
nv <- vv$norm()
nv_orig <- vv$norm()
nw <- vw$norm()
nvnw <- nv$prod(nw)
nvnw$inverse()
nvnw$prod(dot_prod)
nvnw$simplify()
nvnw$rationalize()
```

Luego
```{r}
glue_latex(
  "\\theta = \\arccos \\frac{[dot_prod$to_latex()]}{[nv_orig$to_latex()][nw$to_latex()]} = \\arccos [nvnw$to_latex()]"
) %>% 
  cat()
```

Vamos a comprobar ahora que $v$ es ortogonal al subespacio $U$. Para ello, a partir de las ecuaciones cartesianas de $U$, vamos a [hallar una base](#base) suya, resolviendo el sistema por Gauss-Jordan y pasando por las ecuaciones paramétricas:
```{r}
s <- gauss_elimination(AU, jordan = TRUE, diag1 = TRUE)
params <- c("\\alpha", "\\beta", "\\delta", "\\gamma")
glue_latex(
  "[glue_matrices(AU, latex = TRUE, fractions = TRUE)]",
  "\\sim",
  "[glue_matrices(s$splits, latex = TRUE, fractions = TRUE)]",
  "\\Rightarrow",
  "[to_latex(generic_vector(n))] = [write_linear_combination(bU, vars = params)]"
) %>% 
  cat()
```

De aquí que la base de $U$ sea:
```{r}
glue_latex(
  "\\mathcal{B}_U = \\left\\{u_i\\right\\} = \\left\\{",
  "[vectors_to_latex(bU)]",
  "\\right\\}"
)
```

Como hemos visto antes, para comprobar que $v$ es ortogonal a $U$, nos basta con comprobar que lo es a cada vector de $\mathcal{B}_U$, hallando su producto escalar:
```{r}
for (i in seq(ncol(bU))) {
  
  ui <- matrix(bU[, i], ncol = 1)
  ui_str <- as.character(ui)
  ui_str[ui < 0] <- paste0("(", ui_str[ui < 0], ")")
  glue_latex(
    "\\langle v, u_[i] \\rangle = ",
    "\\langle [to_latex(v)], [to_latex(ui)] \\rangle = ",
    "[glue::glue('{v_str}{prod_str}{ui_str}') %>% str_flatten('+')] = ",
    "[sum(v * ui)]"
  ) %>% 
    cat()
  
}
```

Consideremos ahora el siguiente sistema de vectores:
```{r}
glue_latex(
  "S = \\{s_i\\} = \\left\\{",
  "[vectors_to_latex(B_ortho)]",
  "\\right\\}"
) %>% 
  cat()
```

Podemos comprobar fácilmente que es un sistema ortogonal, hallando los productos escalares de los vectores de $S$ dos a dos:
```{r}
for (i in seq(ncol(B_ortho) - 1)) {
  
  vi <- matrix(B_ortho[, i], ncol = 1)
  vi_str <- as.character(vi)
  vi_str[vi < 0] <- paste0("(", vi_str[vi < 0], ")")
  
  for (j in seq(i + 1, ncol(B_ortho))) {
    
    vj <- matrix(B_ortho[, j], ncol = 1)
    vj_str <- as.character(vj)
    vj_str[vj < 0] <- paste0("(", vj_str[vj < 0], ")")

    glue_latex(
      "\\langle s_[i], s_[j] \\rangle = ",
      "\\langle [to_latex(vi)], [to_latex(vj)] \\rangle = ",
      "[glue::glue('{vi_str}{prod_str}{vj_str}') %>% str_flatten('+')] = ",
      "[sum(vi * vj)]"
    ) %>% 
      cat()
      
  }
  
}
```

Tenemos, por tanto, un sistema de `r ncol(B_ortho)` vectores ortogonales en \(\mathbb{R}^{`r n`}\), y, como hemos comentado antes, son linealmente independientes, luego forman una base del espacio vectorial.



## Qué es y qué utilidad tiene una base ortonormal {#ortho-basis}


Un vector $v\in V$ se dice __unitario__ si $\|v\| = 1$. Es fácil crear vectores unitarios: dado $u\in V$, si definimos $v = \frac{1}{\|u\|}u$, este vector tiene norma 1. A este proceso se le llama _normalizar_.

Un sistema de vectores $\{v_1,\ldots,v_k\}$ se llama __sistema ortonormal__ si es un sistema ortogonal y cada $v_i$ es un vector unitario ($i=1,\ldots, k$).

En relación a lo comentado de las propiedades de los sistemas ortogonales, en nuestro caso podemos decir que un sistema ortonormal $\{v_1,\ldots,v_n\}$, donde $\mathrm{dim}(V) = n$, es una __base ortonormal__.

El interés de una base ortonormal es que es sencillo calcular las [coordenadas](#coord) de cualquier vector con respecto a dicha base:

> Sea $\mathcal{B} = \{v_1,\ldots,v_n\}$ una base _ortonormal_ del espacio vectorial $V$. Entonces, para cada $v\in V$, tenemos:
$$v = \langle v, v_1 \rangle v_1 + \ldots \langle v, v_n \rangle v_n$$
luego las coordenadas de $v$ en la base $\mathcal{B}$ las podemos calcular como:
$$[v]_{\mathcal{B}} = \left(
\begin{array}{c}
\langle v, v_1 \rangle \\
\langle v, v_2 \rangle \\
\vdots \\
\langle v, v_n \rangle \\
\end{array}
\right)_{\mathcal{B}}$$


---

__Ejemplo__

Consideremos ahora el sistema de vectores ortogonales del [ejemplo anterior](#ortho):
```{r}
glue_latex(
  "S = \\{s_i\\} = \\left\\{",
  "[vectors_to_latex(B_ortho)]",
  "\\right\\}"
) %>% 
  cat()
```

A partir de $S$, formemos un sistema ortonormal $S'$, donde cada vector de $S'$ es un vector de $S$, que se ha normalizado.
```{r}
base <- lapply(seq(ncol(B_ortho)), function(i) Vector$new(matrix(B_ortho[, i], ncol = 1)))
base_norm <- lapply(base, 
                    function(v) {
                      norma <- v$norm()
                      v$prod(norma$inverse())
                    })

base <- lapply(base, 
                    function(v) v$simplify())

base_norm <- lapply(base_norm, 
                    function(v) v$rationalize())

str_vectors <- lapply(base_norm, 
       function(v) v$to_latex()) %>% 
  str_flatten(", ")
glue_latex(
  "S' = \\left\\{",
  "[str_vectors]",
  "\\right\\}"
) %>% 
  cat()
```

$S'$ forma una base _ortonormal_ de \(\mathbb{R}^{`r n`}\). 
Si consideramos el vector 
```{r}
glue_latex(
  "v = [glue_matrices(v, latex = TRUE, fractions = TRUE)]"
) %>% 
  cat()
```
podemos hallar sus coordenadas en la base $S'$, ya que cada coordenada de $v$ será el producto escalar de $v$ por el correspondiente vector de $S'$:
```{r}

v <- Vector$new(v)
producto <- list()
for (i in seq_along(base)) {
  
  si <- base[[i]]
  p <- v$prod(si)
  p$simplify()
  p$rationalize()
  producto[[i]] <- p
  
  glue_latex(
    "\\langle v, s'_[i] \\rangle = ",
    "\\langle [v$to_latex()], [si$to_latex()] \\rangle = ",
    "[p$to_latex()]"
  ) %>% 
    cat()
}
```

Entonces, podremos escribir:
```{r}
coord <- Vector$new(producto)
glue_latex(
  "v = [coord$to_latex()]_{S'}"
) %>% 
  cat()
```

---

## Cómo construimos una base ortonormal {#gram-schmidt}

- Teorema de existencia
- Gram-Schmidt

---

__Ejemplo__

---

## Qué es el complemento ortogonal de un subespacio {#compl-ortho}

- Definición de complemento ortogonal
- Método de cálculo

---

__Ejemplo__

```{r}
bW <- rMatrix(n = n, m = 2)
bWt <- solve_homogeneous(t(bW))
```

---

## Cómo calculamos la proyección de un vector sobre un subespacio {#proyec}

- Definición de proyección ortogonal sobre otro vector
- Propiedades
- Definición de proyección sobre un subespacio
- Teorema de descomposición ortogonal

---

__Ejemplo__

---

## Qué es una matriz ortogonal y una aplicación ortogonal {#ortho-matrix}

## Qué es la diagonalización ortogonal {#diag-ortho}

- Definición de aplicación ortogonal y de matriz ortogonal 
- Teorema de relación entre matriz ortogonal y base ortonormal
- Definición de matriz diagonalizable ortogonalmente
- Teorema diagonalización ortogonal


---

__Ejemplo__

---

## Qué matrices son diagonalizables ortogonalmente {#diag-ortho2}

- Teoremas que relacionan matrices simétricas con diagonabilidad ortogonal
- Procedimiento diagonalización ortogonal matriz simétrica

---

__Ejemplo__

---
