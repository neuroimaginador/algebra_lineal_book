<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Espacios Euclídeos | Preguntas y respuestas de Álgebra Lineal</title>
  <meta name="description" content="En este libro vamos a recopilar algunas de las preguntas y dudas más frecuentes en los temas de Álgebra Lineal, con ejemplos resueltos, y acompañados de la teoría necesaria para comprender los resultados." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Espacios Euclídeos | Preguntas y respuestas de Álgebra Lineal" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="En este libro vamos a recopilar algunas de las preguntas y dudas más frecuentes en los temas de Álgebra Lineal, con ejemplos resueltos, y acompañados de la teoría necesaria para comprender los resultados." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Espacios Euclídeos | Preguntas y respuestas de Álgebra Lineal" />
  
  <meta name="twitter:description" content="En este libro vamos a recopilar algunas de las preguntas y dudas más frecuentes en los temas de Álgebra Lineal, con ejemplos resueltos, y acompañados de la teoría necesaria para comprender los resultados." />
  

<meta name="author" content="Domingo López" />


<meta name="date" content="2020-05-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="diagonalización.html"/>
<link rel="next" href="problems.html"/>
<script src="libs/header-attrs-2.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Preguntas y respuestas de Álgebra Lineal</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="ev.html"><a href="ev.html"><i class="fa fa-check"></i><b>2</b> Espacios Vectoriales</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ev.html"><a href="ev.html#subesp"><i class="fa fa-check"></i><b>2.1</b> Qué es un subespacio vectorial</a></li>
<li class="chapter" data-level="2.2" data-path="ev.html"><a href="ev.html#indep"><i class="fa fa-check"></i><b>2.2</b> Qué es un sistema de vectores linealmente independiente</a></li>
<li class="chapter" data-level="2.3" data-path="ev.html"><a href="ev.html#gen"><i class="fa fa-check"></i><b>2.3</b> Qué es un sistema generador y el subespacio generado por un conjunto de vectores</a></li>
<li class="chapter" data-level="2.4" data-path="ev.html"><a href="ev.html#sistgenabase"><i class="fa fa-check"></i><b>2.4</b> Cómo extraemos una base a partir de un sistema generador de un (sub)espacio vectorial</a></li>
<li class="chapter" data-level="2.5" data-path="ev.html"><a href="ev.html#base"><i class="fa fa-check"></i><b>2.5</b> Cómo encontrar la base (y la dimensión) para un subespacio vectorial</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ev.html"><a href="ev.html#partiendo-de-las-ecuaciones-paramétricas"><i class="fa fa-check"></i><b>2.5.1</b> Partiendo de las ecuaciones paramétricas</a></li>
<li class="chapter" data-level="2.5.2" data-path="ev.html"><a href="ev.html#partiendo-de-las-ecuaciones-cartesianas"><i class="fa fa-check"></i><b>2.5.2</b> Partiendo de las ecuaciones cartesianas</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ev.html"><a href="ev.html#param2cartesian"><i class="fa fa-check"></i><b>2.6</b> Cómo convertir entre ecuaciones cartesianas y paramétricas de un subespacio vectorial</a></li>
<li class="chapter" data-level="2.7" data-path="ev.html"><a href="ev.html#interseccion"><i class="fa fa-check"></i><b>2.7</b> Cómo hallar la intersección de dos subespacios vectoriales</a></li>
<li class="chapter" data-level="2.8" data-path="ev.html"><a href="ev.html#union"><i class="fa fa-check"></i><b>2.8</b> ¿Es la unión de subespacios un subespacio?</a></li>
<li class="chapter" data-level="2.9" data-path="ev.html"><a href="ev.html#suma"><i class="fa fa-check"></i><b>2.9</b> Cómo hallar la suma de dos subespacios vectoriales</a></li>
<li class="chapter" data-level="2.10" data-path="ev.html"><a href="ev.html#th-dim"><i class="fa fa-check"></i><b>2.10</b> Qué dice el teorema de la dimensión</a></li>
<li class="chapter" data-level="2.11" data-path="ev.html"><a href="ev.html#supl"><i class="fa fa-check"></i><b>2.11</b> Cómo calcular el subespacio suplementario de uno dado</a></li>
<li class="chapter" data-level="2.12" data-path="ev.html"><a href="ev.html#coord"><i class="fa fa-check"></i><b>2.12</b> Cómo calcular las coordenadas de un vector con respecto a una base dada</a></li>
<li class="chapter" data-level="2.13" data-path="ev.html"><a href="ev.html#cambiobase"><i class="fa fa-check"></i><b>2.13</b> Cómo calcular la matriz de cambio de base entre dos bases de un mismo espacio vectorial</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="apli.html"><a href="apli.html"><i class="fa fa-check"></i><b>3</b> Aplicaciones lineales</a>
<ul>
<li class="chapter" data-level="3.1" data-path="apli.html"><a href="apli.html#prop-apli"><i class="fa fa-check"></i><b>3.1</b> Qué propiedades básicas tienen las aplicaciones lineales</a></li>
<li class="chapter" data-level="3.2" data-path="apli.html"><a href="apli.html#matriz-apli"><i class="fa fa-check"></i><b>3.2</b> Cuál es la matriz asociada a una aplicación lineal en unas bases dadas</a></li>
<li class="chapter" data-level="3.3" data-path="apli.html"><a href="apli.html#cambiobase-apli"><i class="fa fa-check"></i><b>3.3</b> Cómo influye un cambio de base en la matriz asociada a una aplicación lineal</a></li>
<li class="chapter" data-level="3.4" data-path="apli.html"><a href="apli.html#nucleo"><i class="fa fa-check"></i><b>3.4</b> Qué es y cómo determinar el núcleo de una aplicación lineal</a></li>
<li class="chapter" data-level="3.5" data-path="apli.html"><a href="apli.html#inyectividad"><i class="fa fa-check"></i><b>3.5</b> Cómo podemos identificar una aplicación lineal inyectiva</a></li>
<li class="chapter" data-level="3.6" data-path="apli.html"><a href="apli.html#imagen"><i class="fa fa-check"></i><b>3.6</b> Cómo calcular el subespacio imagen de una aplicación lineal</a></li>
<li class="chapter" data-level="3.7" data-path="apli.html"><a href="apli.html#sobrey"><i class="fa fa-check"></i><b>3.7</b> Cómo identificar si una aplicación lineal es sobreyectiva</a></li>
<li class="chapter" data-level="3.8" data-path="apli.html"><a href="apli.html#isomorfismo"><i class="fa fa-check"></i><b>3.8</b> Qué es un isomorfismo</a></li>
<li class="chapter" data-level="3.9" data-path="apli.html"><a href="apli.html#imagenU"><i class="fa fa-check"></i><b>3.9</b> Cómo determinar la imagen de un subespacio vectorial</a></li>
<li class="chapter" data-level="3.10" data-path="apli.html"><a href="apli.html#rango-nulidad"><i class="fa fa-check"></i><b>3.10</b> A qué se llama rango y nulidad de la aplicación lineal</a></li>
<li class="chapter" data-level="3.11" data-path="apli.html"><a href="apli.html#th-dim-nucleo"><i class="fa fa-check"></i><b>3.11</b> Qué dice el teorema de la dimensión para núcleo e imagen</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="diagonalización.html"><a href="diagonalización.html"><i class="fa fa-check"></i><b>4</b> Diagonalización</a>
<ul>
<li class="chapter" data-level="4.1" data-path="diagonalización.html"><a href="diagonalización.html#eigenvalue"><i class="fa fa-check"></i><b>4.1</b> Qué son los autovalores de un endomorfismo y de una matriz</a></li>
<li class="chapter" data-level="4.2" data-path="diagonalización.html"><a href="diagonalización.html#eigenspace"><i class="fa fa-check"></i><b>4.2</b> Cómo calculamos los autovectores asociados a un autovalor</a></li>
<li class="chapter" data-level="4.3" data-path="diagonalización.html"><a href="diagonalización.html#diagonalizable"><i class="fa fa-check"></i><b>4.3</b> Cómo sabemos si un endomorfismo o una matriz es diagonalizable</a></li>
<li class="chapter" data-level="4.4" data-path="diagonalización.html"><a href="diagonalización.html#pot-matrix"><i class="fa fa-check"></i><b>4.4</b> Si una matriz es diagonalizable, cómo podemos hallar sus potencias</a></li>
<li class="chapter" data-level="4.5" data-path="diagonalización.html"><a href="diagonalización.html#th-cayley"><i class="fa fa-check"></i><b>4.5</b> Para qué podemos utilizar el Teorema de Cayley-Hamilton</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="espeuc.html"><a href="espeuc.html"><i class="fa fa-check"></i><b>5</b> Espacios Euclídeos</a>
<ul>
<li class="chapter" data-level="5.1" data-path="espeuc.html"><a href="espeuc.html#norm"><i class="fa fa-check"></i><b>5.1</b> Qué son las normas y distancias</a></li>
<li class="chapter" data-level="5.2" data-path="espeuc.html"><a href="espeuc.html#ortho"><i class="fa fa-check"></i><b>5.2</b> Qué significa el concepto de ortogonalidad</a></li>
<li class="chapter" data-level="5.3" data-path="espeuc.html"><a href="espeuc.html#compl-ortho"><i class="fa fa-check"></i><b>5.3</b> Qué es el complemento ortogonal de un subespacio</a></li>
<li class="chapter" data-level="5.4" data-path="espeuc.html"><a href="espeuc.html#proyec"><i class="fa fa-check"></i><b>5.4</b> Cómo calculamos la proyección de un vector sobre un subespacio</a></li>
<li class="chapter" data-level="5.5" data-path="espeuc.html"><a href="espeuc.html#ortho-basis"><i class="fa fa-check"></i><b>5.5</b> Qué es y qué utilidad tiene una base ortonormal</a></li>
<li class="chapter" data-level="5.6" data-path="espeuc.html"><a href="espeuc.html#gram-schmidt"><i class="fa fa-check"></i><b>5.6</b> Cómo construimos una base ortonormal</a></li>
<li class="chapter" data-level="5.7" data-path="espeuc.html"><a href="espeuc.html#ortho-matrix"><i class="fa fa-check"></i><b>5.7</b> Qué es una matriz ortogonal y una aplicación ortogonal</a></li>
<li class="chapter" data-level="5.8" data-path="espeuc.html"><a href="espeuc.html#diag-ortho"><i class="fa fa-check"></i><b>5.8</b> Qué es la diagonalización ortogonal</a></li>
<li class="chapter" data-level="5.9" data-path="espeuc.html"><a href="espeuc.html#diag-ortho2"><i class="fa fa-check"></i><b>5.9</b> Qué matrices son diagonalizables ortogonalmente</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="problems.html"><a href="problems.html"><i class="fa fa-check"></i><b>6</b> Problemas Resueltos</a>
<ul>
<li class="chapter" data-level="6.1" data-path="problems.html"><a href="problems.html#prob-ev"><i class="fa fa-check"></i><b>6.1</b> Espacios Vectoriales</a></li>
<li class="chapter" data-level="6.2" data-path="problems.html"><a href="problems.html#prob-apli"><i class="fa fa-check"></i><b>6.2</b> Aplicaciones lineales</a></li>
<li class="chapter" data-level="6.3" data-path="problems.html"><a href="problems.html#prob-diag"><i class="fa fa-check"></i><b>6.3</b> Diagonalización</a></li>
<li class="chapter" data-level="6.4" data-path="problems.html"><a href="problems.html#prob-espeuclideo"><i class="fa fa-check"></i><b>6.4</b> Espacios Euclídeos</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://dominlopez.netlify.app" target="blank">Escrito por Domingo López</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Preguntas y respuestas de Álgebra Lineal</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="espeuc" class="section level1" number="5">
<h1 number="5"><span class="header-section-number">5</span> Espacios Euclídeos</h1>
<p>En este capítulo, damos un paso para acercarnos a los conceptos geométricos clásicos de distancias y ángulos, abstrayéndolos en el contexto de los espacios vectoriales.</p>
<p>Para ello, precisaremos de la noción de <em>producto escalar</em>: Una aplicación <span class="math inline">\(g:V\times V\to \mathbb{R}\)</span>, donde <span class="math inline">\(V\)</span> es un espacio vectorial sobre <span class="math inline">\(\mathbb{R}\)</span>, es un <strong>producto escalar</strong> si verifica:</p>
<ul>
<li>Es simétrica: <span class="math inline">\(g(u, v) = g(v, u)\)</span> para todo <span class="math inline">\(u,v\in V\)</span>.</li>
<li>Es bilineal: <span class="math inline">\(g(\alpha u + \beta v, w) = \alpha g(u, w) + \beta g(v, w)\)</span> para todos los <span class="math inline">\(\alpha,\beta\in\mathbb{R}\)</span>, <span class="math inline">\(u,v,w\in V\)</span>. Por simetría, se tiene lo mismo para la linealidad en la segunda componente.</li>
<li>Es definida positiva: <span class="math inline">\(g(v, v) &gt; 0\)</span> para todo <span class="math inline">\(v\in V\setminus\{0\}\)</span>, <span class="math inline">\(g(v, v) = 0\)</span> si, y sólo si <span class="math inline">\(v = 0\)</span>.</li>
</ul>
<p>En resumen, se suele definir un producto escalar como una <em>forma bilineal, simétrica y definida positiva</em>.</p>
<p><strong>Notación</strong>: El producto escalar de dos vectores <span class="math inline">\(u\)</span> y <span class="math inline">\(v\)</span> suele recibir diversas notaciones, aunque las más usadas son <span class="math inline">\(u\cdot v\)</span> y <span class="math inline">\(\langle u, v \rangle\)</span>.</p>
<p>A un espacio vectorial <span class="math inline">\(V\)</span> donde podemos definir un producto vectorial lo llamamos <strong>espacio euclídeo</strong>, y lo solemos denotar mediante el par <span class="math inline">\(\left(V, \langle\cdot,\cdot\rangle\right)\)</span>.</p>
<hr />
<p><strong>Ejemplo</strong></p>
<p>En <span class="math inline">\(\mathbb{R}^n\)</span>, disponemos de un <em>producto escalar usual</em>, de forma que si <span class="math inline">\(u, v\in\mathbb{R}^n\)</span>, entonces
<span class="math display">\[\langle u,v \rangle = u_1v_1+u_2v_2+\ldots+u_nv_n\]</span>
siendo <span class="math inline">\(u_i, v_i\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span> las coordenadas de <span class="math inline">\(u\)</span> y de <span class="math inline">\(v\)</span>, respectivamente, en una base dada (la canónica como caso particular). A este producto vectorial es al que nos referiremos cuando usemos la notación <span class="math inline">\(u\cdot v\)</span>.</p>
<p>Podemos generalizar este producto escalar, definiendo unos pesos <span class="math inline">\(\alpha_i\in\mathbb{R}^{+}\)</span> para cada componente:
<span class="math display">\[\langle u,v \rangle = \alpha_1u_1v_1+\alpha_2u_2v_2+\ldots+\alpha_nu_nv_n\]</span></p>
<p>Podemos construir productos escalares de la siguiente forma: Tomemos una matriz <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span> simétrica y definida positiva. Entonces la aplicación <span class="math inline">\(\langle\cdot,\cdot\rangle:\mathbb{R}^n\times\mathbb{R}^n\to\mathbb{R}\)</span> definida por
<span class="math display">\[\langle u, v \rangle = u^{\text{t}}\ A\ v\]</span>
es un producto escalar. A esta matriz <span class="math inline">\(A\)</span> se le denomina <em>matriz de Gram</em>, y todo producto escalar tiene asociada una. Los dos ejemplos anteriores se correspondían con tomar como <span class="math inline">\(A\)</span> la matriz identidad o una matriz diagonal con elementos positivos <span class="math inline">\(\alpha_i\in\mathbb{R}^{+}\)</span> en la diagonal.</p>
<hr />
<p><strong>¿Qué preguntas vamos a responder en este capítulo?</strong></p>
<ul>
<li><a href="espeuc.html#norm">¿Qué son las normas y distancias?</a></li>
<li><a href="espeuc.html#ortho">¿Qué significa el concepto de ortogonalidad?</a></li>
<li><a href="espeuc.html#compl-ortho">¿Qué es el complemento ortogonal de un subespacio?</a></li>
<li><a href="espeuc.html#proyec">¿Cómo calculamos la proyección de un vector sobre un subespacio?</a></li>
<li><a href="espeuc.html#ortho-basis">¿Qué es y qué utilidad tiene una base ortonormal?</a></li>
<li><a href="espeuc.html#gram-schmidt">¿Cómo construimos una base ortonormal?</a></li>
<li><a href="espeuc.html#ortho-matrix">¿Qué es una matriz ortogonal y una aplicación ortogonal?</a></li>
<li><a href="espeuc.html#diag-ortho">¿Qué es la diagonalización ortogonal?</a></li>
<li><a href="espeuc.html#diag-ortho2">Qué matrices son diagonalizables ortogonalmente</a></li>
</ul>
<div id="norm" class="section level2" number="5.1">
<h2 number="5.1"><span class="header-section-number">5.1</span> Qué son las normas y distancias</h2>
<p><strong>Definición (Norma) </strong> Una <strong>norma</strong> vectorial sobre un espacio <span class="math inline">\(V\)</span> (sobre un cuerpo <span class="math inline">\(\mathbb{K}\)</span>) es un operador que sirve para <em>medir</em> la longitud de un vector (es decir, su distancia al origen), y se define formalmente como una aplicación <span class="math inline">\(\|\cdot\|:V\to\mathbb{R}\)</span> que verifica:</p>
<ul>
<li>Para todo <span class="math inline">\(v\in V\)</span>, <span class="math inline">\(\|v\| \ge 0\)</span>, y <span class="math inline">\(\|v\| = 0\)</span> si, y solo si, <span class="math inline">\(v = 0\)</span>.</li>
<li>Para todo <span class="math inline">\(c\in\mathbb{K}\)</span>, <span class="math inline">\(v\in V\)</span>, se tiene <span class="math inline">\(\|cv\| = |c|\cdot \|v\|\)</span>.</li>
<li>Para todo <span class="math inline">\(u,v\in V\)</span>, <span class="math inline">\(\|u+v\|\le\|u\|+\|v\|\)</span>. Esta propiedad se llama <em>desigualdad triangular</em> (en un triángulo, la suma de la longitud de dos de sus lados –<span class="math inline">\(\|u\|\)</span> y <span class="math inline">\(\|v\|\)</span>– es siempre menor que la longitud de su otro lado –<span class="math inline">\(\|u+v\|\)</span>–).</li>
</ul>
<p>Cualquier operador <span class="math inline">\(\|\cdot\|\)</span> que verifique lo anterior es una norma vectorial. Sin embargo, podemos construir normas a partir de un producto escalar:</p>
<blockquote>
<p>Si <span class="math inline">\(\langle\cdot,\cdot\rangle\)</span> es un producto escalar en el espacio <span class="math inline">\(V\)</span>, entonces la aplicación <span class="math inline">\(\|\cdot\|:V\to\mathbb{R}\)</span> dada por <span class="math inline">\(\|v\| = \sqrt{\langle v, v\rangle}\)</span> es una <strong>norma derivada del producto escalar</strong> sobre <span class="math inline">\(V\)</span>.</p>
</blockquote>
<p>Al definir la norma vectorial a partir de un producto escalar, además, tenemos la siguiente propiedad que será de gran interés en la <a href="espeuc.html#ortho">siguiente sección</a>:</p>
<blockquote>
<p><strong>Desigualdad de Cauchy-Schwarz</strong>:
Si <span class="math inline">\(\|\cdot\|\)</span> es una norma derivada del producto escalar <span class="math inline">\(\langle\cdot,\cdot\rangle\)</span>, entonces
<span class="math display">\[|\langle u, v \rangle| \le \|u\|\cdot\|v\|\]</span>
para todo <span class="math inline">\(u,v\in V\)</span>.</p>
</blockquote>
<p>Un concepto implícitamente relacionado con el de norma vectorial es el de <em>distancia</em>.</p>
<p><strong>Definición (Distancia) </strong> Una <strong>distancia</strong> (también llamada <em>métrica</em>) es un operador <span class="math inline">\(d:V\times V\to R\)</span> que verifica las siguientes propiedades:</p>
<ul>
<li><span class="math inline">\(d(u, v) = 0\)</span> si, y solo si, <span class="math inline">\(u = v\)</span>.</li>
<li>Es simétrica: <span class="math inline">\(d(u, v) = d(v, u)\)</span>.</li>
<li>Cumple la desigualdad triangular: <span class="math inline">\(d(u, v) \le d(u, w) + d(w, v)\)</span>.</li>
</ul>
<p>Además, de esta definición se deduce que <span class="math inline">\(d(u, v) \ge 0\)</span> para todo <span class="math inline">\(u,v\in V\)</span>.</p>
<p>Podemos deducir una distancia a partir de una norma, para así completar las relaciones existentes entre los conceptos aquí explicados:</p>
<blockquote>
<p>Si <span class="math inline">\(\|\cdot\|\)</span> es una norma sobre un espacio vectorial <span class="math inline">\(V\)</span>, entonces la aplicación <span class="math inline">\(d:V\times V\to \mathbb{R}\)</span> definida por
<span class="math display">\[d(u, v) = \|u-v\|\]</span>
para todo <span class="math inline">\(u,v\in V\)</span> es una <em>distancia</em>.</p>
</blockquote>
<p><strong>Nota</strong>: Existen muchas más normas de las que se deducen a partir de un producto escalar, al igual que más <em>distancias</em>.</p>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Podemos deducir la expresión de la norma (a veces llamada <em>módulo</em>) de un vector <span class="math inline">\(v\in\mathbb{R}^n\)</span>.</p>
<p>Si <span class="math inline">\(v=\left(\begin{array}{c}v_1\\ v_2\\\vdots\\ v_n\end{array}\right)\)</span>, entonces
<span class="math display">\[\|v\| = \sqrt{v\cdot v} = \sqrt{v_1^2+v_2^2+\ldots+v_n^2}\]</span></p>
<p>Y así, además, deducimos la expresión para la distancia euclídea usual en <span class="math inline">\(\mathbb{R}^n\)</span>:
<span class="math display">\[d(u,v) = \|u - v\| = \sqrt{(u_1-v_1)^2+\ldots+(u_n-v_n)^2}\]</span></p>
<p>Como ejemplo, podemos calcular la norma del vector
<span class="math display">\[v = \left(\begin{array}{c} -2\\ 1\\ 1\\ 1 \end{array}\right) \]</span><span class="math display">\[\|v\| = \sqrt{(-2)^2+1^2+1^2+1^2} = \sqrt{7}\]</span></p>
<hr />
</div>
<div id="ortho" class="section level2" number="5.2">
<h2 number="5.2"><span class="header-section-number">5.2</span> Qué significa el concepto de ortogonalidad</h2>
<p>Si partimos de la <em>desigualdad de Cauchy-Schwarz</em> que <a href="espeuc.html#norm">hemos visto antes</a>, podemos llegar a que
<span class="math display">\[-1 \le \frac{\langle u, v\rangle}{\|u\|\cdot\|v\|} \le 1\]</span>
para todo <span class="math inline">\(u,v\in V\)</span>.</p>
<p>Por otro lado, la función coseno restringida a <span class="math inline">\([0, \pi]\)</span>, es decir, <span class="math inline">\(\cos:[0,\pi]\to[-1,1]\)</span>, es biyectiva. Eso quiere decir que existe un único <span class="math inline">\(\theta\in[0, \pi]\)</span> tal que <span class="math inline">\(\cos\theta = \frac{\langle u, v\rangle}{\|u\|\cdot\|v\|}\)</span>.</p>
<p>Esto nos permite la siguiente definición:</p>
<p><strong>Definición (Ángulo entre vectores) </strong> Llamamos <strong>ángulo entre los vectores <span class="math inline">\(u,v\in V\)</span></strong> al único <span class="math inline">\(\theta\in[0,\pi]\)</span> tal que <span class="math inline">\(\cos\theta = \frac{\langle u, v\rangle}{\|u\|\cdot\|v\|}\)</span>.</p>
<p><strong>Definición (Vectores ortogonales) </strong> Diremos que dos vectores <span class="math inline">\(u\)</span> y <span class="math inline">\(v\)</span> de <span class="math inline">\(V\)</span> son <strong>ortogonales</strong> (o <strong>perpendiculares</strong>) si el ángulo que forman es <span class="math inline">\(\frac{\pi}{2}\)</span>, es decir, si <span class="math inline">\(\langle u, v \rangle = 0\)</span>.</p>
<p>La noción de <em>ortogonalidad</em> en el espacio euclídeo se corresponde con la perpendicularidad de vectores que todos tenemos de forma intuitiva. Lo podemos extender también a ortogonalidad con respecto a un espacio vectorial: Un vector <span class="math inline">\(v\in V\)</span> es <strong>ortogonal al subespacio <span class="math inline">\(U\)</span></strong> de <span class="math inline">\(V\)</span> si, y sólo si, es ortogonal a todos y cada uno de los vectores de <span class="math inline">\(U\)</span>, es decir, <span class="math inline">\(\langle v, u \rangle = 0\)</span> para todo <span class="math inline">\(u\in U\)</span>.</p>
<p>El siguiente resultado nos proporciona una ayuda a la hora de determinar la ortogonalidad entre subespacios vectoriales.</p>
<blockquote>
<p>Sea <span class="math inline">\(U\)</span> un subespacio de <span class="math inline">\(V\)</span>, y sea <span class="math inline">\(\mathcal{B} = \{u_1,\ldots,u_k\}\)</span> una base de <span class="math inline">\(U\)</span>. Un vector <span class="math inline">\(v\in V\)</span> es ortogonal a <span class="math inline">\(U\)</span> si, y solo si, es ortogonal a cada <span class="math inline">\(u_i\)</span>: <span class="math inline">\(\langle v, u_i \rangle = 0\)</span> para todo <span class="math inline">\(i=1,\ldots,k\)</span>.</p>
</blockquote>
<p>Un <strong>sistema ortogonal</strong> es un conjunto de vectores <span class="math inline">\(\{v_1,\ldots,v_m\}\subset V\)</span> donde los <span class="math inline">\(v_i\)</span> son ortogonales dos a dos, es decir, <span class="math inline">\(\langle v_i, v_j \rangle = 0\)</span> para todo <span class="math inline">\(i\ne j\)</span>.</p>
<p><strong>Algunas propiedades y caracterizaciones de los vectores y sistemas ortogonales</strong></p>
<blockquote>
<p><strong>Generalización del teorema de Pitágoras</strong>:
Dos vectores <span class="math inline">\(u\)</span> y <span class="math inline">\(w\)</span> son ortogonales si, y solo si, <span class="math inline">\(\|u+w\|^2 = \|u\|^2 + \|v\|^2\)</span></p>
</blockquote>
<blockquote>
<p>Si un sistema de vectores es ortogonal, entonces es linealmente independiente. Por tanto, en un espacio <span class="math inline">\(V\)</span> de dimensión finita <span class="math inline">\(n\)</span>, cualquier conjunto de <span class="math inline">\(n\)</span> vectores ortogonales entre sí es una base.</p>
</blockquote>
<p>Este último resultado es interesante, pues nos constata que cualquier sistema ortogonal con tantos vectores como la dimensión del espacio vectorial es una base.</p>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Consideremos el espacio <span class="math inline">\(U\)</span> dado por:
<span class="math display">\[U = \left\{\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right) \in\mathbb{R}^{4}:\begin{array}{ccr} -x+ 4y+ z &amp; = &amp; 0\\ y+ t &amp; = &amp; 0\\ \end{array}\right\}\]</span>
y los vectores
<span class="math display">\[v = \left(\begin{array}{c} 1\\ -2\\ -1\\ 2 \end{array}\right) \quad\quad w = \left(\begin{array}{c} -1\\ 2\\ -1\\ -1 \end{array}\right) \]</span></p>
<p>Calculemos el ángulo entre <span class="math inline">\(v\)</span> y <span class="math inline">\(w\)</span>:
<span class="math display">\[\theta = \arccos \frac{v\cdot w}{\|v\|\cdot\|w\|}\]</span><span class="math display">\[v\cdot w = 1\cdot(-1)+(-2)\cdot2+(-1)\cdot(-1)+2\cdot(-1) = -6\]</span><span class="math display">\[\|v\| = \sqrt{1^2+(-2)^2+(-1)^2+2^2} = \sqrt{10}\]</span><span class="math display">\[\|w\| = \sqrt{(-1)^2+2^2+(-1)^2+(-1)^2} = \sqrt{7}\]</span></p>
<p>Luego
<span class="math display">\[\theta = \arccos \frac{-6}{\sqrt{10}\sqrt{7}} = \arccos \frac{-3\cdot\sqrt{70}}{35}\]</span></p>
<p>Vamos a comprobar ahora que <span class="math inline">\(v\)</span> es ortogonal al subespacio <span class="math inline">\(U\)</span>. Para ello, a partir de las ecuaciones cartesianas de <span class="math inline">\(U\)</span>, vamos a <a href="ev.html#base">hallar una base</a> suya, resolviendo el sistema por Gauss-Jordan y pasando por las ecuaciones paramétricas:
<span class="math display">\[\left( \begin{array}{cccc@{}} -1 &amp; 4 &amp; 1 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 1 \end{array} \right) \sim\left( \begin{array}{cccc@{}} 1 &amp; 0 &amp; -1 &amp; 4\\ 0 &amp; 1 &amp; 0 &amp; 1 \end{array} \right) \Rightarrow\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = \alpha\left(\begin{array}{c} 1\\ 0\\ 1\\ 0 \end{array}\right) {}+\beta\left(\begin{array}{c} -4\\ -1\\ 0\\ 1 \end{array}\right) {}\]</span></p>
<p>De aquí que la base de <span class="math inline">\(U\)</span> sea:
<span class="math display">\[\mathcal{B}_U = \left\{u_i\right\} = \left\{\left( \begin{array}{c@{}} 1\\ 0\\ 1\\ 0 \end{array} \right) , \left( \begin{array}{c@{}} -4\\ -1\\ 0\\ 1 \end{array} \right) \right\}\]</span></p>
<p>Como hemos visto antes, para comprobar que <span class="math inline">\(v\)</span> es ortogonal a <span class="math inline">\(U\)</span>, nos basta con comprobar que lo es a cada vector de <span class="math inline">\(\mathcal{B}_U\)</span>, hallando su producto escalar:
<span class="math display">\[\langle v, u_1 \rangle = \langle \left(\begin{array}{c} 1\\ -2\\ -1\\ 2 \end{array}\right) , \left(\begin{array}{c} 1\\ 0\\ 1\\ 0 \end{array}\right)  \rangle = 1\cdot1+(-2)\cdot0+(-1)\cdot1+2\cdot0 = 0\]</span><span class="math display">\[\langle v, u_2 \rangle = \langle \left(\begin{array}{c} 1\\ -2\\ -1\\ 2 \end{array}\right) , \left(\begin{array}{c} -4\\ -1\\ 0\\ 1 \end{array}\right)  \rangle = 1\cdot(-4)+(-2)\cdot(-1)+(-1)\cdot0+2\cdot1 = 0\]</span></p>
<p>Consideremos ahora el siguiente sistema de vectores:
<span class="math display">\[S = \{s_i\} = \left\{\left( \begin{array}{c@{}} 1\\ 1\\ 1\\ 0 \end{array} \right) , \left( \begin{array}{c@{}} -1\\ 2\\ -1\\ 0 \end{array} \right) , \left( \begin{array}{c@{}} 1\\ 0\\ -1\\ 2 \end{array} \right) , \left( \begin{array}{c@{}} -1\\ 0\\ 1\\ 1 \end{array} \right) \right\}\]</span></p>
<p>Podemos comprobar fácilmente que es un sistema ortogonal, hallando los productos escalares de los vectores de <span class="math inline">\(S\)</span> dos a dos:
<span class="math display">\[\langle s_1, s_2 \rangle = \langle \left(\begin{array}{c} 1\\ 1\\ 1\\ 0 \end{array}\right) , \left(\begin{array}{c} -1\\ 2\\ -1\\ 0 \end{array}\right)  \rangle = 1\cdot(-1)+1\cdot2+1\cdot(-1)+0\cdot0 = 0\]</span><span class="math display">\[\langle s_1, s_3 \rangle = \langle \left(\begin{array}{c} 1\\ 1\\ 1\\ 0 \end{array}\right) , \left(\begin{array}{c} 1\\ 0\\ -1\\ 2 \end{array}\right)  \rangle = 1\cdot1+1\cdot0+1\cdot(-1)+0\cdot2 = 0\]</span><span class="math display">\[\langle s_1, s_4 \rangle = \langle \left(\begin{array}{c} 1\\ 1\\ 1\\ 0 \end{array}\right) , \left(\begin{array}{c} -1\\ 0\\ 1\\ 1 \end{array}\right)  \rangle = 1\cdot(-1)+1\cdot0+1\cdot1+0\cdot1 = 0\]</span><span class="math display">\[\langle s_2, s_3 \rangle = \langle \left(\begin{array}{c} -1\\ 2\\ -1\\ 0 \end{array}\right) , \left(\begin{array}{c} 1\\ 0\\ -1\\ 2 \end{array}\right)  \rangle = (-1)\cdot1+2\cdot0+(-1)\cdot(-1)+0\cdot2 = 0\]</span><span class="math display">\[\langle s_2, s_4 \rangle = \langle \left(\begin{array}{c} -1\\ 2\\ -1\\ 0 \end{array}\right) , \left(\begin{array}{c} -1\\ 0\\ 1\\ 1 \end{array}\right)  \rangle = (-1)\cdot(-1)+2\cdot0+(-1)\cdot1+0\cdot1 = 0\]</span><span class="math display">\[\langle s_3, s_4 \rangle = \langle \left(\begin{array}{c} 1\\ 0\\ -1\\ 2 \end{array}\right) , \left(\begin{array}{c} -1\\ 0\\ 1\\ 1 \end{array}\right)  \rangle = 1\cdot(-1)+0\cdot0+(-1)\cdot1+2\cdot1 = 0\]</span></p>
<p>Tenemos, por tanto, un sistema de 4 vectores ortogonales en <span class="math inline">\(\mathbb{R}^{4}\)</span>, y, como hemos comentado antes, son linealmente independientes, luego forman una base del espacio vectorial.</p>
</div>
<div id="compl-ortho" class="section level2" number="5.3">
<h2 number="5.3"><span class="header-section-number">5.3</span> Qué es el complemento ortogonal de un subespacio</h2>
<p>Consideremos un espacio vectorial <span class="math inline">\(V\)</span> con producto escalar <span class="math inline">\(\langle \cdot,\cdot \rangle\)</span>, y sea <span class="math inline">\(U\)</span> un subespacio de <span class="math inline">\(V\)</span>.</p>
<p><strong>Definición (Complemento ortogonal) </strong> Llamamos <strong>complemento ortogonal de <span class="math inline">\(U\)</span></strong> al conjunto de vectores que son ortogonales a todos los de <span class="math inline">\(U\)</span>:
<span class="math display">\[U^{\perp} = \{v\in V: \langle v, u \rangle = 0\,\,\text{para todo }u\in U\}\]</span></p>
<p>Tenemos las siguientes propiedades del complemento ortogonal de un subespacio:</p>
<blockquote>
<p><strong>Propiedades del complemento ortogonal</strong>:
- <span class="math inline">\(U^{\perp}\)</span> es un subespacio vectorial de <span class="math inline">\(V\)</span>.
- <span class="math inline">\(U\cap U^{\perp} = \{0\}\)</span>.
- Si <span class="math inline">\(V\)</span> es de dimensión finita, <span class="math inline">\(\mathrm{dim}(U) + \mathrm{dim}(U^{\perp}) = \mathrm{dim}(V)\)</span>.</p>
</blockquote>
<p>Como consecuencia, este resultado nos dice que <strong><span class="math inline">\(V\)</span> se puede descomponer como la suma directa de <span class="math inline">\(U\)</span> y de su complemento ortogonal</strong>: <span class="math inline">\(U\oplus U^{\perp} = V\)</span>.</p>
<p><strong>¿Cómo calculamos el complemento ortogonal de un subespacio?</strong></p>
<p>Hemos de hallar los vectores que son ortogonales a un subespacio <span class="math inline">\(U\)</span>. Como hemos <a href="espeuc.html#ortho">visto antes</a>, un vector <span class="math inline">\(v\)</span> es ortogonal a <span class="math inline">\(U\)</span> si, y solo si, es ortogonal a todos los vectores de una base de <span class="math inline">\(U\)</span>.</p>
<p>Así pues, partimos de que tenemos una base <span class="math inline">\(\mathcal{B} = \{u_1,\ldots,u_m\}\)</span> de <span class="math inline">\(U\)</span> y tomemos un vector genérico <span class="math inline">\(v\in V\)</span>. Para que <span class="math inline">\(v\in U^{\perp}\)</span>, debe cumplirse que <span class="math inline">\(\langle v, u_i\rangle = 0\)</span> para todo <span class="math inline">\(i=1,\ldots,m\)</span>.</p>
<p>Cada una de las restricciones <span class="math inline">\(\langle v, u_i\rangle = 0\)</span> nos proporciona una ecuación lineal que debe verificar un vector para poder pertenecer a <span class="math inline">\(U^{\perp}\)</span>.</p>
<p>Tomamos, por tanto, todas las ecuaciones lineales que se deducen de las restricciones anteriores: ese sistema de ecuaciones nos representa las ecuaciones cartesianas del espacio <span class="math inline">\(U^{\perp}\)</span>.</p>
<p>En el caso concreto de un espacio <span class="math inline">\(V=\mathbb{R}^n\)</span> con el <em>producto escalar euclídeo</em>, podemos ver qué aspecto tienen las ecuaciones del tipo <span class="math inline">\(\langle v, u_i \rangle = 0\)</span>. Supongamos que los vectores <span class="math inline">\(v\)</span> y <span class="math inline">\(u_i\)</span> tienen por coordenadas, en la base canónica, las siguientes:
<span class="math display">\[v = \left(
\begin{array}{c}
x_1\\ x_2\\ \vdots \\ x_n\\
\end{array}\right), \quad 
u_i = \left(
\begin{array}{c}
\alpha_{i,1}\\ \alpha_{i,2}\\ \vdots \\ \alpha_{i,n}\\
\end{array}\right)\]</span></p>
<p>Entonces
<span class="math display">\[
\langle v, u_i \rangle = 0 \Leftrightarrow 
\langle \left(
\begin{array}{c}
x_1\\ x_2\\ \vdots \\ x_n\\
\end{array}\right) , 
\left(
\begin{array}{c}
\alpha_{i,1}\\ \alpha_{i,2}\\ \vdots \\ \alpha_{i,n}\\
\end{array}\right) \rangle = 0 \Leftrightarrow \alpha_{i,1}x_1+\alpha_{i,2}x_2+\ldots+\alpha_{i,n}x_n = 0
\]</span></p>
<p>Si nos fijamos, nos queda una ecuación lineal cuyas incógnitas son las coordenadas canónicas de <span class="math inline">\(v\)</span> y cuyos coeficientes son las coordenadas de <span class="math inline">\(u_i\)</span>. Si repetimos el proceso para todo <span class="math inline">\(i=1,\ldots,m\)</span>, tenemos el siguiente sistema de ecuaciones:
<span class="math display">\[Av = 0\quad\text{donde}\quad A = \left(
\begin{array}{cccc}
\alpha_{1,1} &amp; \alpha_{1,2} &amp; \ldots &amp; \alpha_{1,n} \\
\alpha_{2,1} &amp; \alpha_{2,2} &amp; \ldots &amp; \alpha_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\alpha_{m,1} &amp; \alpha_{m,2} &amp; \ldots &amp; \alpha_{m,n} \\
\end{array}
\right)\]</span></p>
<p>Es decir, las <em>ecuaciones cartesianas</em> de <span class="math inline">\(U^{\perp}\)</span> son las que tienen por matriz de coeficientes a las coordenadas de los vectores de la base de <span class="math inline">\(U\)</span> puestos <em>por filas</em>. De hecho, si llamamos <span class="math inline">\(B\)</span> a la matriz que resulta de poner por columnas los vectores de <span class="math inline">\(\mathcal{B}\)</span>, entonces <span class="math inline">\(A = B^{\mathrm{t}}\)</span>.</p>
<p>A partir de esas ecuaciones cartesianas, ya podríamos <a href="#sisgena%20base">encontrar una base</a> del subespacio <span class="math inline">\(U^{\perp}\)</span>.</p>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Consideremos el espacio <span class="math inline">\(U\)</span> generado por la base siguiente:
<span class="math display">\[\mathcal{B} = \{u_i\} = \left\{\left( \begin{array}{c@{}} -2\\ 0\\ -2\\ 2 \end{array} \right) , \left( \begin{array}{c@{}} -1\\ -1\\ -2\\ 2 \end{array} \right) \right\}\]</span></p>
<p>Para hallar su complemento ortogonal, suponemos un vector genérico
<span class="math display">\[v = \left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right) \in \mathbb{R}^{4}\]</span>
y hacemos su producto escalar por todos los vectores de <span class="math inline">\(\mathcal{B}\)</span>, imponiendo que valga 0:
<span class="math display">\[\langle v, u_1 \rangle = 0\Leftrightarrow\langle \left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right) , \left(\begin{array}{c} -2\\ 0\\ -2\\ 2 \end{array}\right)  = 0\Leftrightarrow\ \begin{array}{ccr} -2x-2z+ 2t &amp; = &amp; 0\\ \end{array}\]</span><span class="math display">\[\langle v, u_2 \rangle = 0\Leftrightarrow\langle \left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right) , \left(\begin{array}{c} -1\\ -1\\ -2\\ 2 \end{array}\right)  = 0\Leftrightarrow\ \begin{array}{ccr} -x-y-2z+ 2t &amp; = &amp; 0\\ \end{array}\]</span></p>
<p>Hemos llegado, por tanto, al siguiente sistema de ecuaciones que representan las ecuaciones cartesianas de <span class="math inline">\(U^{\perp}\)</span>:
<span class="math display">\[\begin{array}{rrrrcr} -2x &amp;  &amp; -2z &amp; + 2t &amp; = &amp; 0\\ -x &amp; -y &amp; -2z &amp; + 2t &amp; = &amp; 0\\ \end{array}\]</span>
Nos podemos dar cuenta de que la matriz del sistema es la que resulta de poner los vectores de la base <span class="math inline">\(\mathcal{B}\)</span> <em>por filas</em>.</p>
<p>Como siempre, podemos resolver el sistema por Gauss-Jordan para <a href="ev.html#param2cartesian">deducir la forma paramétrica</a> de la solución, y, por tanto, un <a href="#sisgen">sistema generador</a> de <span class="math inline">\(U^{\perp}\)</span>:
<span class="math display">\[\left( \begin{array}{cccc@{}} -2 &amp; 0 &amp; -2 &amp; 2\\ -1 &amp; -1 &amp; -2 &amp; 2 \end{array} \right) \sim\left( \begin{array}{cccc@{}} 1 &amp; 0 &amp; 1 &amp; -1\\ 0 &amp; 1 &amp; 1 &amp; -1 \end{array} \right) \Rightarrow\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = \alpha\left(\begin{array}{c} -1\\ -1\\ 1\\ 0 \end{array}\right) {}+\beta\left(\begin{array}{c} 1\\ 1\\ 0\\ 1 \end{array}\right) {}\]</span></p>
<p>De aquí que una base del complemento ortogonal <span class="math inline">\(U^{\perp}\)</span> sea:
<span class="math display">\[\mathcal{B}&#39; = \left\{\left( \begin{array}{c@{}} -1\\ -1\\ 1\\ 0 \end{array} \right) , \left( \begin{array}{c@{}} 1\\ 1\\ 0\\ 1 \end{array} \right) \right\}\]</span></p>
<p>Podemos comprobar fácilmente que los vectores de <span class="math inline">\(\mathcal{B}&#39;\)</span> son ortogonales a los de <span class="math inline">\(\mathcal{B}\)</span>.</p>
<hr />
</div>
<div id="proyec" class="section level2" number="5.4">
<h2 number="5.4"><span class="header-section-number">5.4</span> Cómo calculamos la proyección de un vector sobre un subespacio</h2>
<p><strong>Definición (Proyección ortogonal) </strong> Consideremos un espacio euclídeo <span class="math inline">\(\left(V, \langle\cdot,\cdot\rangle\right)\)</span>. Consideremos un subespacio <span class="math inline">\(U\)</span>, del que conocemos una <a href="espeuc.html#ortho">base <em>ortogonal</em></a> <span class="math inline">\(\mathcal{B}_U = \{u_1,\ldots,u_m\}\)</span>. Definimos la <strong>proyección <em>ortogonal</em> de un vector <span class="math inline">\(v\)</span> sobre el subespacio <span class="math inline">\(U\)</span></strong>, como:
<span class="math display">\[\mathrm{proy}_U(v) = \frac{\langle v, u_1\rangle}{\langle u_1, u_1 \rangle}u_1 + \frac{\langle v, u_2\rangle}{\langle u_2, u_2 \rangle}u_2 + \ldots + \frac{\langle v, u_m\rangle}{\langle u_m, u_m \rangle}u_m\]</span></p>
<p>Es decir, la proyección <span class="math inline">\(\mathrm{proy}_U(v)\)</span> es un vector del subespacio <span class="math inline">\(U\)</span>, ya que es combinación lineal de los elementos de su base, y además esta combinación lineal tiene como coeficientes el producto escalar de <span class="math inline">\(v\)</span> por cada elemento de la base, dividido por la norma de <span class="math inline">\(u_i\)</span> al cuadrado (<a href="espeuc.html#norm">recordemos que <span class="math inline">\(\|x\| = \sqrt{\langle x,x \rangle}\)</span></a>).</p>
<p>Evidentemente, si los elementos de la base de <span class="math inline">\(U\)</span> tienen norma 1, el denominador desaparece.</p>
<p>La proyección de <span class="math inline">\(v\)</span> sobre el subespacio <span class="math inline">\(U\)</span> tiene la propiedad siguiente:</p>
<blockquote>
<p>Si <span class="math inline">\(u\)</span> es un vector del subespacio <span class="math inline">\(U\)</span>, y <span class="math inline">\(v\in V\)</span>, entonces <span class="math inline">\(d(u, v) \ge d(\mathrm{proy}_U(v), v)\)</span>, y la única forma para que se dé la igualdad es que <span class="math inline">\(u = \mathrm{proy}_U(v)\)</span>.</p>
</blockquote>
<p>Esto significa que la proyección ortogonal de <span class="math inline">\(v\)</span> sobre <span class="math inline">\(U\)</span> es el vector de <span class="math inline">\(U\)</span> más cercano (según la distancia <span class="math inline">\(d\)</span>) a <span class="math inline">\(v\)</span>.</p>
<p>Como caso particular, podríamos definir la <strong>proyección ortogonal de un vector <span class="math inline">\(v\)</span> sobre otro vector <span class="math inline">\(u\)</span></strong> como
<span class="math display">\[\mathrm{proy}_u(v) = \frac{\langle v, u \rangle}{\langle u, u\rangle}u\]</span>
(realmente es la proyección del vector <span class="math inline">\(v\)</span> sobre el <a href="#sisgen">subespacio generado</a> por el vector <span class="math inline">\(u\)</span>).</p>
<p>Podemos entonces concluir que, si la base ortogonal de <span class="math inline">\(U\)</span> es <span class="math inline">\(\mathcal{B}_U = \{u_1,\ldots,u_m\}\)</span>, entonces
<span class="math display">\[\mathrm{proy}_U(v) = \mathrm{proy}_{u_1}(v)+\ldots+\mathrm{proy}_{u_m}(v)\]</span></p>
<p>A partir del hecho de que la base de <span class="math inline">\(U\)</span> es <em>ortogonal</em>, podemos deducir que:</p>
<blockquote>
<p>Si <span class="math inline">\(U\)</span> es un subespacio de <span class="math inline">\(V\)</span>, <span class="math inline">\(v\in V\)</span>, y una base ortogonal de <span class="math inline">\(U\)</span> es <span class="math inline">\(\mathcal{B}_U = \{u_1,\ldots,u_m\}\)</span>, entonces el vector <span class="math inline">\(v - \mathrm{proy}_U(v)\)</span> es ortogonal a <span class="math inline">\(U\)</span>, es decir, <span class="math inline">\(v - \mathrm{proy}_U(v) \in U^{\perp}\)</span> está en el <a href="espeuc.html#compl-ortho">complemento ortogonal</a> de <span class="math inline">\(U\)</span>.</p>
</blockquote>
<p>Esto, junto con que <span class="math inline">\(\mathrm{proy}_U(v)\in U\)</span>, nos justifican el siguiente resultado, llamado <strong>teorema de la descomposición ortogonal</strong>:</p>
<blockquote>
<p><strong>Descomposición ortogonal</strong>:
En un espacio euclídeo <span class="math inline">\((V, \langle\cdot,\cdot\rangle)\)</span>, dado un subespacio <span class="math inline">\(U\)</span>, y <span class="math inline">\(v\in V\)</span>, existe una única descomposición de <span class="math inline">\(v\)</span> como suma de dos vectores ortogonales, <span class="math inline">\(v = v_1 + v_2\)</span>, tales que <span class="math inline">\(v_1\in U\)</span> y <span class="math inline">\(v_2\in U^{\perp}\)</span>. Concretamente, <span class="math inline">\(v_1 = \mathrm{proy}_U(v)\)</span> y <span class="math inline">\(v_2 = v - v_1 = \mathrm{proy}_{U^{\perp}}(v)\)</span>.</p>
</blockquote>
<p>Podemos siempre descomponer un vector de <span class="math inline">\(V\)</span> como suma de dos vectores, uno de <span class="math inline">\(U\)</span> y otro de <span class="math inline">\(U^{\perp}\)</span>, que coinciden con las proyecciones sobre ambos subespacios.</p>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Consideremos el subespacio <span class="math inline">\(U\)</span> cuya base es
<span class="math display">\[\mathcal{B} = \{u_i\} = \left\{\left( \begin{array}{c@{}} -1\\ 1\\ -1\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} -3\\ -5\\ 1\\ -3 \end{array} \right) \right\}\]</span></p>
<p>Notemos que <span class="math inline">\(\mathcal{B}\)</span> es una base <em>ortogonal</em>.</p>
<p>Consideremos el vector
<span class="math display">\[v = \left( \begin{array}{c@{}} -1\\ 1\\ -2\\ 0 \end{array} \right) \]</span></p>
<p>Buscamos la <em>descomposición ortogonal</em> de <span class="math inline">\(v\)</span> con respecto al subespacio <span class="math inline">\(U\)</span>. Sabemos que podemos escribir <span class="math inline">\(v = v_1 + v_2\)</span>, donde <span class="math inline">\(v_1 = \mathrm{proy}_U(v)\)</span> y <span class="math inline">\(v_2 = v - v_1 \in U^{\perp}\)</span>.
<span class="math display">\[v_1 = \mathrm{proy}_U(v) = \mathrm{proy}_{u_1}(v)+\mathrm{proy}_{u_2}(v) = \frac{\langle v, u_1\rangle}{\langle u_1, u_1 \rangle} u_1+\frac{\langle v, u_2\rangle}{\langle u_2, u_2 \rangle} u_2\]</span></p>
<p>Si calculamos esas cantidades
<span class="math display">\[\langle v, u_1 \rangle = \langle \left(\begin{array}{c} -1\\ 1\\ -2\\ 0 \end{array}\right) , \left(\begin{array}{c} -1\\ 1\\ -1\\ -1 \end{array}\right)  \rangle = (-1)\cdot(-1)+1\cdot1+(-2)\cdot(-1)+0\cdot(-1) = 4\]</span><span class="math display">\[\langle v, u_2 \rangle = \langle \left(\begin{array}{c} -1\\ 1\\ -2\\ 0 \end{array}\right) , \left(\begin{array}{c} -3\\ -5\\ 1\\ -3 \end{array}\right)  \rangle = (-1)\cdot(-3)+1\cdot(-5)+(-2)\cdot1+0\cdot(-3) = -4\]</span><span class="math display">\[\langle u_1, u_1 \rangle = \langle \left(\begin{array}{c} -1\\ 1\\ -1\\ -1 \end{array}\right) , \left(\begin{array}{c} -1\\ 1\\ -1\\ -1 \end{array}\right)  \rangle = (-1)^2+1^2+(-1)^2+(-1)^2 = 4\]</span><span class="math display">\[\langle u_2, u_2 \rangle = \langle \left(\begin{array}{c} -3\\ -5\\ 1\\ -3 \end{array}\right) , \left(\begin{array}{c} -3\\ -5\\ 1\\ -3 \end{array}\right)  \rangle = (-3)^2+(-5)^2+1^2+(-3)^2 = 44\]</span>
podemos sustituirlas en la expresión de más arriba y llegamos a
<span class="math display">\[v_1 = 1\cdot\left(\begin{array}{c} -1\\ 1\\ -1\\ -1 \end{array}\right) +(- \frac{1}{11})\cdot\left(\begin{array}{c} -3\\ -5\\ 1\\ -3 \end{array}\right)  = \left( \begin{array}{c@{}} - \frac{8}{11}\\ \frac{16}{11}\\ - \frac{12}{11}\\ - \frac{8}{11} \end{array} \right) \]</span></p>
<p>Por tanto,
<span class="math display">\[v_2 = \left( \begin{array}{c@{}} -1\\ 1\\ -2\\ 0 \end{array} \right)  - \left( \begin{array}{c@{}} - \frac{8}{11}\\ \frac{16}{11}\\ - \frac{12}{11}\\ - \frac{8}{11} \end{array} \right)  = \left( \begin{array}{c@{}} - \frac{3}{11}\\ - \frac{5}{11}\\ - \frac{10}{11}\\ \frac{8}{11} \end{array} \right) \]</span></p>
<p>Por lo comentado anteriormente, sabemos que <span class="math inline">\(v_2 = \mathrm{proy}_{U^{\perp}}(v)\in U^{\perp}\)</span>, lo cual se podría comprobar haciendo el producto escalar <span class="math inline">\(\langle v_2, u_i \rangle\)</span> y viendo que su valor es 0 para todo <span class="math inline">\(i\)</span>.</p>
<hr />
</div>
<div id="ortho-basis" class="section level2" number="5.5">
<h2 number="5.5"><span class="header-section-number">5.5</span> Qué es y qué utilidad tiene una base ortonormal</h2>
<p><strong>Definición (Vector unitario) </strong> Un vector <span class="math inline">\(v\in V\)</span> se dice <strong>unitario</strong> si <span class="math inline">\(\|v\| = 1\)</span>.</p>
<p>Es fácil crear vectores unitarios: dado <span class="math inline">\(u\in V\)</span>, si definimos <span class="math inline">\(v = \frac{1}{\|u\|}u\)</span>, este vector tiene norma 1. A este proceso se le llama <em>normalizar</em>.</p>
<p>Un sistema de vectores <span class="math inline">\(\{v_1,\ldots,v_k\}\)</span> se llama <strong>sistema ortonormal</strong> si es un <a href="espeuc.html#ortho">sistema ortogonal</a> y cada <span class="math inline">\(v_i\)</span> es un vector unitario (<span class="math inline">\(i=1,\ldots, k\)</span>).</p>
<p>En relación a lo comentado de las propiedades de los sistemas ortogonales, en nuestro caso podemos decir que un sistema ortonormal <span class="math inline">\(\{v_1,\ldots,v_n\}\)</span>, donde <span class="math inline">\(\mathrm{dim}(V) = n\)</span>, es una <strong>base ortonormal</strong>.</p>
<p>El interés de una base ortonormal es que es sencillo calcular las <a href="ev.html#coord">coordenadas</a> de cualquier vector con respecto a dicha base:</p>
<blockquote>
<p><strong>Coordenadas en base ortonormal</strong>:
Sea <span class="math inline">\(\mathcal{B} = \{v_1,\ldots,v_n\}\)</span> una base <em>ortonormal</em> del espacio vectorial <span class="math inline">\(V\)</span>. Entonces, para cada <span class="math inline">\(v\in V\)</span>, tenemos:
<span class="math display">\[v = \langle v, v_1 \rangle v_1 + \ldots \langle v, v_n \rangle v_n\]</span>
luego las coordenadas de <span class="math inline">\(v\)</span> en la base <span class="math inline">\(\mathcal{B}\)</span> las podemos calcular como:
<span class="math display">\[[v]_{\mathcal{B}} = \left(\begin{array}{c}\langle v, v_1\rangle \\\langle v, v_2 \rangle \\\vdots \\\langle v, v_n \rangle \\\end{array}\right)_{\mathcal{B}}\]</span></p>
</blockquote>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Consideremos ahora el sistema de vectores ortogonales del <a href="espeuc.html#ortho">ejemplo anterior</a>:
<span class="math display">\[S = \{s_i\} = \left\{\left( \begin{array}{c@{}} 1\\ 1\\ 1\\ 0 \end{array} \right) , \left( \begin{array}{c@{}} -1\\ 2\\ -1\\ 0 \end{array} \right) , \left( \begin{array}{c@{}} 1\\ 0\\ -1\\ 2 \end{array} \right) , \left( \begin{array}{c@{}} -1\\ 0\\ 1\\ 1 \end{array} \right) \right\}\]</span></p>
<p>A partir de <span class="math inline">\(S\)</span>, formemos un sistema ortonormal <span class="math inline">\(S&#39;\)</span>, donde cada vector de <span class="math inline">\(S&#39;\)</span> es un vector de <span class="math inline">\(S\)</span>, que se ha normalizado.
<span class="math display">\[S&#39; = \left\{\left( \begin{array}{c} \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{-\sqrt{6}}{6}\\ \frac{\sqrt{6}}{3}\\ \frac{-\sqrt{6}}{6}\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{\sqrt{6}}{6}\\ 0\\ \frac{-\sqrt{6}}{6}\\ \frac{\sqrt{6}}{3} \end{array} \right), \left( \begin{array}{c} \frac{-\sqrt{3}}{3}\\ 0\\ \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3} \end{array} \right)\right\}\]</span></p>
<p><span class="math inline">\(S&#39;\)</span> forma una base <em>ortonormal</em> de <span class="math inline">\(\mathbb{R}^{4}\)</span>.
Si consideramos el vector
<span class="math display">\[v = \left( \begin{array}{c@{}} -1\\ 1\\ -2\\ 0 \end{array} \right) \]</span>
podemos hallar sus coordenadas en la base <span class="math inline">\(S&#39;\)</span>, ya que cada coordenada de <span class="math inline">\(v\)</span> será el producto escalar de <span class="math inline">\(v\)</span> por el correspondiente vector de <span class="math inline">\(S&#39;\)</span>:
<span class="math display">\[\langle v, s&#39;_1 \rangle = \langle \left( \begin{array}{c} -1\\ 1\\ -2\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ 0 \end{array} \right) \rangle = \frac{-2\cdot\sqrt{3}}{3}\]</span><span class="math display">\[\langle v, s&#39;_2 \rangle = \langle \left( \begin{array}{c} -1\\ 1\\ -2\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{-\sqrt{6}}{6}\\ \frac{\sqrt{6}}{3}\\ \frac{-\sqrt{6}}{6}\\ 0 \end{array} \right) \rangle = \frac{5\cdot\sqrt{6}}{6}\]</span><span class="math display">\[\langle v, s&#39;_3 \rangle = \langle \left( \begin{array}{c} -1\\ 1\\ -2\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{\sqrt{6}}{6}\\ 0\\ \frac{-\sqrt{6}}{6}\\ \frac{\sqrt{6}}{3} \end{array} \right) \rangle = \frac{\sqrt{6}}{6}\]</span><span class="math display">\[\langle v, s&#39;_4 \rangle = \langle \left( \begin{array}{c} -1\\ 1\\ -2\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{-\sqrt{3}}{3}\\ 0\\ \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3} \end{array} \right) \rangle = \frac{-\sqrt{3}}{3}\]</span></p>
<p>Entonces, podremos escribir:
<span class="math display">\[v = \left( \begin{array}{c} \frac{-2\cdot\sqrt{3}}{3}\\ \frac{5\cdot\sqrt{6}}{6}\\ \frac{\sqrt{6}}{6}\\ \frac{-\sqrt{3}}{3} \end{array} \right)_{S&#39;}\]</span></p>
<hr />
</div>
<div id="gram-schmidt" class="section level2" number="5.6">
<h2 number="5.6"><span class="header-section-number">5.6</span> Cómo construimos una base ortonormal</h2>
<p>En las secciones anteriores hemos usado bases ortogonales y ortonormales, pero cabe preguntarnos si la existencia de tales bases está asegurada. La respuesta nos la da el siguiente resultado:</p>
<blockquote>
<p>Sea <span class="math inline">\(V\)</span> un espacio vectorial euclídeo de <em>dimensión finita</em>. Entonces <span class="math inline">\(V\)</span> tiene una base ortonormal.</p>
</blockquote>
<p>La demostración de este resultado teórico realmente nos presenta un método constructivo que permite hallar una base ortonormal del espacio <span class="math inline">\(V\)</span> a partir de una base cualquiera.</p>
<p><strong>Método de Ortonormalización de Gram-Schmidt</strong></p>
<p>Consideremos una base <span class="math inline">\(\mathcal{B} = \{v_1,\ldots,v_n\}\)</span> en <span class="math inline">\(V\)</span>. Vamos a utilizar el método de Gram-Schmidt para construir una base ortonormal a partir de ella.</p>
<p>El proceso se divide en dos fases:</p>
<ul>
<li>En la primera fase, construiremos una base ortogonal <span class="math inline">\(\mathcal{B}&#39;\)</span>.</li>
<li>En un segundo paso, cada vector de <span class="math inline">\(\mathcal{B}&#39;\)</span> se normaliza, de forma que obtendremos una base ortonormal.</li>
</ul>
<p><em>Primera fase</em></p>
<p>El proceso del primer paso es incremental.</p>
<p>Llamamos <span class="math inline">\(u_1 = v_1\)</span>, el primer vector de la base <span class="math inline">\(\mathcal{B}\)</span>.</p>
<p>Ahora consideramos el segundo vector <span class="math inline">\(v_2\)</span>. Por un <a href="espeuc.html#proyec">resultado teórico</a> sobre las proyecciones que vimos antes, si llamamos <span class="math inline">\(U_1 = \mathcal{L}(\{u_1\})\)</span>, entonces <span class="math inline">\(v_2 - \mathrm{proy}_{U_1}(v_2)\)</span> es un vector ortogonal a <span class="math inline">\(U_1\)</span>, en particular es ortogonal a <span class="math inline">\(u_1\)</span>.</p>
<p>Llamemos entonces <span class="math inline">\(u_2 = v_2 - \mathrm{proy}_{U_1}(v_2)\)</span>.
Entonces, el sistema <span class="math inline">\(\{u_1, u_2\}\)</span> es ortogonal.</p>
<p>Llamamos ahora <span class="math inline">\(U_2 = \mathcal{L}(\{u_1,u_2\})\)</span>. Entonces, por el mismo motivo que antes, si llamamos <span class="math inline">\(u_3 = v_3 - \mathrm{proy}_{U_2}(v_3)\)</span>, tenemos que <span class="math inline">\(u_3\)</span> es ortogonal a <span class="math inline">\(U_2\)</span>, luego <span class="math inline">\(\{u_1, u_2, u_3\}\)</span> es un sistema ortogonal.</p>
<p>De esta forma, sucesivamente, en el paso <span class="math inline">\(m\)</span>, vamos construyendo <span class="math inline">\(U_m = \mathcal{L}(\{u_1,\ldots,u_m\})\)</span> y consideramos <span class="math inline">\(u_{m+1} = v_{m+1} - \mathrm{proy}_{U_m}(v_{m+1})\)</span>, que es un vector ortogonal a <span class="math inline">\(U_m\)</span> y, por tanto, <span class="math inline">\(\{u_1,\ldots,u_{m+1}\}\)</span> es un sistema ortogonal.</p>
<p>Una vez se haya completado el proceso, se habrá construido el conjunto <span class="math inline">\(\mathcal{B}&#39; = \{u_1,\ldots,u_n\}\)</span>. Como estamos en un espacio <span class="math inline">\(V\)</span> de dimensión <span class="math inline">\(n\)</span>, y tenemos <span class="math inline">\(n\)</span> vectores ortogonales, luego <a href="espeuc.html#ortho">linealmente independientes</a>, éstos forman una base. Luego <span class="math inline">\(\mathcal{B}&#39;\)</span> es una base ortogonal.</p>
<p><em>Segunda fase</em></p>
<p>La segunda fase es más sencilla, ya que implica únicamente normalizar cada vector de <span class="math inline">\(\mathcal{B}&#39;\)</span>.</p>
<p>De esta forma, la base ortonormal es la formada por
<span class="math display">\[\left\{\frac{u_1}{\|u_1\|}, \ldots, \frac{u_n}{\|u_n\|}\right\}\]</span></p>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Consideremos la base <span class="math inline">\(\mathcal{B}\)</span> en <span class="math inline">\(\mathbb{R}^{4}\)</span> dada por
<span class="math display">\[\mathcal{B} = \{v_i\} = \left\{\left( \begin{array}{c@{}} -1\\ -1\\ -1\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} 1\\ -1\\ 1\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} -1\\ -1\\ 0\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} -1\\ 1\\ 1\\ -1 \end{array} \right) \right\}\]</span></p>
<p>Vamos a usar el método de Gram-Schmidt para encontrar una base ortonormal, a partir de <span class="math inline">\(\mathcal{B}\)</span>.</p>
<p><em>Fase 1</em></p>
<p>Llamamos
<span class="math display">\[u_1 = v_1 = \left(\begin{array}{c} -1\\ -1\\ -1\\ -1 \end{array}\right) \]</span></p>
<p>A partir de este momento, procedemos de forma recursiva:</p>
<ul>
<li>Construimos:
<span class="math display">\[U_1 = \mathcal{L}(\{u_1\}) = \mathcal{L}(\left\{\left( \begin{array}{c@{}} -1\\ -1\\ -1\\ -1 \end{array} \right) \right\})\]</span>
Entonces
<span class="math display">\[\begin{array}{rcl}\mathrm{proy}_{U_1}(v_2) &amp; = &amp; \frac{\langle v_2, u_1 \rangle}{\langle u_1, u_1 \rangle} u_1 \\ &amp; = &amp; \frac{0}{4} \left( \begin{array}{c@{}} -1\\ -1\\ -1\\ -1 \end{array} \right)  \\ &amp; = &amp; \left( \begin{array}{c@{}} 0\\ 0\\ 0\\ 0 \end{array} \right) \\\end{array}\]</span>
Y llamamos
<span class="math display">\[u_2  = v_2 - \mathrm{proy}_{U_1}(v_2) = \left(\begin{array}{c} 1\\ -1\\ 1\\ -1 \end{array}\right)  - \left( \begin{array}{c@{}} 0\\ 0\\ 0\\ 0 \end{array} \right)  =  \left( \begin{array}{c@{}} 1\\ -1\\ 1\\ -1 \end{array} \right) \]</span></li>
<li>Construimos:
<span class="math display">\[U_2 = \mathcal{L}(\{u_1, u_2\}) = \mathcal{L}(\left\{\left( \begin{array}{c@{}} -1\\ -1\\ -1\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} 1\\ -1\\ 1\\ -1 \end{array} \right) \right\})\]</span>
Entonces
<span class="math display">\[\begin{array}{rcl}\mathrm{proy}_{U_2}(v_3) &amp; = &amp; \frac{\langle v_3, u_1 \rangle}{\langle u_1, u_1 \rangle} u_1+\frac{\langle v_3, u_2 \rangle}{\langle u_2, u_2 \rangle} u_2 \\ &amp; = &amp; \frac{3}{4} \left( \begin{array}{c@{}} -1\\ -1\\ -1\\ -1 \end{array} \right) +\frac{1}{4} \left( \begin{array}{c@{}} 1\\ -1\\ 1\\ -1 \end{array} \right)  \\ &amp; = &amp; \left( \begin{array}{c@{}} - \frac{1}{2}\\ -1\\ - \frac{1}{2}\\ -1 \end{array} \right) \\\end{array}\]</span>
Y llamamos
<span class="math display">\[u_3  = v_3 - \mathrm{proy}_{U_2}(v_3) = \left(\begin{array}{c} -1\\ -1\\ 0\\ -1 \end{array}\right)  - \left( \begin{array}{c@{}} - \frac{1}{2}\\ -1\\ - \frac{1}{2}\\ -1 \end{array} \right)  =  \left( \begin{array}{c@{}} - \frac{1}{2}\\ 0\\ \frac{1}{2}\\ 0 \end{array} \right) \]</span></li>
<li>Construimos:
<span class="math display">\[U_3 = \mathcal{L}(\{u_1, u_2, u_3\}) = \mathcal{L}(\left\{\left( \begin{array}{c@{}} -1\\ -1\\ -1\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} 1\\ -1\\ 1\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} - \frac{1}{2}\\ 0\\ \frac{1}{2}\\ 0 \end{array} \right) \right\})\]</span>
Entonces
<span class="math display">\[\begin{array}{rcl}\mathrm{proy}_{U_3}(v_4) &amp; = &amp; \frac{\langle v_4, u_1 \rangle}{\langle u_1, u_1 \rangle} u_1+\frac{\langle v_4, u_2 \rangle}{\langle u_2, u_2 \rangle} u_2+\frac{\langle v_4, u_3 \rangle}{\langle u_3, u_3 \rangle} u_3 \\ &amp; = &amp; \frac{0}{4} \left( \begin{array}{c@{}} -1\\ -1\\ -1\\ -1 \end{array} \right) +\frac{0}{4} \left( \begin{array}{c@{}} 1\\ -1\\ 1\\ -1 \end{array} \right) +\frac{1}{\frac{1}{2}} \left( \begin{array}{c@{}} - \frac{1}{2}\\ 0\\ \frac{1}{2}\\ 0 \end{array} \right)  \\ &amp; = &amp; \left( \begin{array}{c@{}} -1\\ 0\\ 1\\ 0 \end{array} \right) \\\end{array}\]</span>
Y llamamos
<span class="math display">\[u_4  = v_4 - \mathrm{proy}_{U_3}(v_4) = \left(\begin{array}{c} -1\\ 1\\ 1\\ -1 \end{array}\right)  - \left( \begin{array}{c@{}} -1\\ 0\\ 1\\ 0 \end{array} \right)  =  \left( \begin{array}{c@{}} 0\\ 1\\ 0\\ -1 \end{array} \right) \]</span></li>
</ul>
<p>Por tanto, la base ortogonal a la que llegamos es
<span class="math display">\[\mathcal{B}&#39; = \{u_i\} = \left\{\left( \begin{array}{c@{}} -1\\ -1\\ -1\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} 1\\ -1\\ 1\\ -1 \end{array} \right) , \left( \begin{array}{c@{}} - \frac{1}{2}\\ 0\\ \frac{1}{2}\\ 0 \end{array} \right) , \left( \begin{array}{c@{}} 0\\ 1\\ 0\\ -1 \end{array} \right) \right\}\]</span></p>
<p><em>Fase 2</em></p>
<p>Ahora debemos multiplicar cada vector <span class="math inline">\(u_i\)</span> en <span class="math inline">\(\mathcal{B}&#39;\)</span> por el inverso de su norma, y llegamos a los vectores <span class="math inline">\(\frac{1}{\|u_i\|}u_i\)</span>, que nos forman la base ortonormal siguiente:
<span class="math display">\[\begin{array}{rcl}\mathcal{B}&#39;&#39; &amp; = &amp; \left\{\frac{1}{2}\left( \begin{array}{c} -1\\ -1\\ -1\\ -1 \end{array} \right), \frac{1}{2}\left( \begin{array}{c} 1\\ -1\\ 1\\ -1 \end{array} \right), \frac{1}{\frac{\sqrt{2}}{2}}\left( \begin{array}{c} \frac{-1}{2}\\ 0\\ \frac{1}{2}\\ 0 \end{array} \right), \frac{1}{\sqrt{2}}\left( \begin{array}{c} 0\\ 1\\ 0\\ -1 \end{array} \right)\right\} = \\ &amp; = &amp; \left\{\left( \begin{array}{c} \frac{-1}{2}\\ \frac{-1}{2}\\ \frac{-1}{2}\\ \frac{-1}{2} \end{array} \right), \left( \begin{array}{c} \frac{1}{2}\\ \frac{-1}{2}\\ \frac{1}{2}\\ \frac{-1}{2} \end{array} \right), \left( \begin{array}{c} \frac{-\sqrt{2}}{2}\\ 0\\ \frac{\sqrt{2}}{2}\\ 0 \end{array} \right), \left( \begin{array}{c} 0\\ \frac{\sqrt{2}}{2}\\ 0\\ \frac{-\sqrt{2}}{2} \end{array} \right)\right\} \\\end{array}\]</span></p>
<hr />
</div>
<div id="ortho-matrix" class="section level2" number="5.7">
<h2 number="5.7"><span class="header-section-number">5.7</span> Qué es una matriz ortogonal y una aplicación ortogonal</h2>
<p><strong>Definición (Matriz ortogonal) </strong> Una matriz cuadrada <span class="math inline">\(Q\)</span> se llama <strong>ortogonal</strong> si <span class="math inline">\(Q^{\text{t}}Q = I\)</span> o, equivalentemente, <span class="math inline">\(Q^{-1} = Q^{\text{t}}\)</span>.</p>
<p>Es decir, una matriz ortogonal es aquella cuya inversa es su propia traspuesta.</p>
<blockquote>
<p>Una matriz <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span> es ortogonal si, y sólo si, sus columnas forman una base <em>ortonormal</em> de <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</blockquote>
<blockquote>
<p>Sea <span class="math inline">\(A\)</span> una matriz ortogonal. Entonces <span class="math inline">\(\mathrm{det}(A) = \pm 1\)</span>.</p>
</blockquote>
<p><strong>Definición (Endomorfismo ortogonal) </strong> Un endomorfismo <span class="math inline">\(f:V\to V\)</span> se llama <strong>ortogonal</strong> si respeta el producto escalar, es decir, si <span class="math inline">\(\langle\cdot,\cdot\rangle\)</span> es el producto escalar en <span class="math inline">\(V\)</span>, entonces <span class="math inline">\(\langle f(u), f(v) \rangle = \langle u, v \rangle\)</span> para todo <span class="math inline">\(u,v\in V\)</span>.</p>
<p>Como la aplicación ortogonal conserva el producto escalar, también conserva todas las cantidades que hemos definido a partir de él: <a href="espeuc.html#norm">la norma, la distancia</a> y <a href="espeuc.html#ortho">el ángulo</a> entre vectores de <span class="math inline">\(V\)</span>: <span class="math inline">\(\|f(v)\| = \|v\|\)</span>, <span class="math inline">\(d(f(u), f(v)) = d(u,v)\)</span> y <span class="math inline">\(\mathrm{ang}(f(u),f(v)) = \mathrm{ang}(u, v)\)</span> para todo <span class="math inline">\(u,v\in V\)</span>.</p>
<blockquote>
<p>Una aplicación <span class="math inline">\(f:V\to V\)</span> es ortogonal si, y sólo si, la imagen de toda base ortonormal es de nuevo una base ortonormal de <span class="math inline">\(V\)</span>.</p>
</blockquote>
<p>¿Qué relación existe entre matrices ortogonales y aplicaciones ortogonales?</p>
<blockquote>
<p>Sea <span class="math inline">\(f:V\to V\)</span> una aplicación lineal y sea <span class="math inline">\(A\)</span> su matriz asociada respecto a una base ortonormal de <span class="math inline">\(V\)</span>. Entonces <span class="math inline">\(f\)</span> es ortogonal si, y solo si, la matriz <span class="math inline">\(A\)</span> es ortogonal.</p>
</blockquote>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Como hemos comentado, si tomamos una base ortonormal y ponemos sus vectores por columnas, nos forman una matriz <em>ortogonal</em>. Por tanto, podemos considerar la base <span class="math inline">\(\mathcal{B}&#39;&#39;\)</span> del <a href="espeuc.html#gram-schmidt">ejemplo anterior</a>, y construir la matriz ortogonal asociada:
<span class="math display">\[Q = \left( \begin{array}{c} \frac{-1}{2}\\ \frac{-1}{2}\\ \frac{-1}{2}\\ \frac{-1}{2} \end{array}   \begin{array}{c} \frac{1}{2}\\ \frac{-1}{2}\\ \frac{1}{2}\\ \frac{-1}{2} \end{array}   \begin{array}{c} \frac{-\sqrt{2}}{2}\\ 0\\ \frac{\sqrt{2}}{2}\\ 0 \end{array}   \begin{array}{c} 0\\ \frac{\sqrt{2}}{2}\\ 0\\ \frac{-\sqrt{2}}{2} \end{array} \right)\]</span></p>
<p>Es fácil comprobar que, en este caso, <span class="math inline">\(Q^{\text{t}}\ Q = I\)</span>.</p>
<p>Hay otros ejemplos de matrices ortogonales. En particular, una tipología concreta de matrices ortogonales en <span class="math inline">\(\mathbb{R}^2\)</span> es muy utilizada en la práctica. Llamemos:
<span class="math display">\[R(\alpha) = \left(
\begin{array}{cc}
\cos(\alpha) &amp; \sin(\alpha) \\
-\sin(\alpha) &amp; \cos(\alpha) \\
\end{array}\right)\]</span></p>
<p>Son las denominadas <strong>matrices de rotación</strong>. Están asociadas a la aplicación lineal que representa un giro o rotación de <span class="math inline">\(\alpha\)</span> radianes en sentido positivo (<em>antihorario</em>) alrededor del origen de coordenadas.</p>
<p>Así
<span class="math display">\[
\left(
\begin{array}{c}
x&#39; \\ y&#39; \\
\end{array}\right) = R(\alpha)\left(
\begin{array}{c}
x \\ y \\
\end{array}\right)\]</span>
son las coordenadas del vector girado, en función de las coordenadas del vector original.</p>
<p>Estas matrices de rotación se pueden extender a dimensiones superiores. Por ejemplo, en <span class="math inline">\(\mathbb{R}^3\)</span>, la rotación de <span class="math inline">\(\alpha\)</span> radianes alrededor del <em>eje <span class="math inline">\(X\)</span></em> se representa mediante la matriz ortogonal
<span class="math display">\[R(\alpha) = \left(
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; \cos(\alpha) &amp; \sin(\alpha) \\
0 &amp; -\sin(\alpha) &amp; \cos(\alpha) \\
\end{array}\right)
\]</span></p>
<hr />
</div>
<div id="diag-ortho" class="section level2" number="5.8">
<h2 number="5.8"><span class="header-section-number">5.8</span> Qué es la diagonalización ortogonal</h2>
<p>Recordemos <a href="diagonalización.html#diagonalizable">la definición de endomorfismo o de matriz diagonalizable</a>. Una aplicación lineal <span class="math inline">\(f:V\to V\)</span> es diagonalizable si y solo si existe una base de <span class="math inline">\(V\)</span> formada por autovectores de <span class="math inline">\(f\)</span>, lo cual equivale a que la representación matricial de <span class="math inline">\(f\)</span> en dicha base sea una matriz diagonal <span class="math inline">\(D\)</span>. En representación matricial, esto se traduce en que una matriz <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span> es diagonalizable si y solo si, existe una matriz regular <span class="math inline">\(P\)</span> tal que <span class="math inline">\(D = P^{-1}\ A\ P\)</span>. Las columnas de <span class="math inline">\(P\)</span> están formadas por los autovectores de la base de la definición.</p>
<p>Veamos cómo influye el hecho de tener bases ortonormales en un espacio euclídeo en la diagonalización, lo que da lugar a la denominada <em>diagonalización ortogonal</em>.</p>
<p><strong>Definición (Aplicación diagonalizable ortogonalmente) </strong> Diremos que una aplicación lineal <span class="math inline">\(f:V\to V\)</span> es <strong>diagonalizable ortogonalmente</strong> si y solo si existe una base ortonormal en <span class="math inline">\(V\)</span> formada por autovectores de <span class="math inline">\(f\)</span>.</p>
<p><strong>Definición (Matriz diagonalizable ortogonalmente) </strong> En matrices, una matriz <span class="math inline">\(A\)</span> se dice <strong>diagonalizable ortogonalmente</strong> si existe una matriz <span class="math inline">\(P\)</span> <em>ortogonal</em> tal que <span class="math inline">\(D = P^{\text{t}}\ A\ P\)</span>, donde <span class="math inline">\(D\)</span>, la matriz asociada a <span class="math inline">\(f\)</span> en la base de autovectores, es diagonal con autovalores en la diagonal principal.</p>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Consideremos la matriz <span class="math inline">\(A\)</span> dada por
<span class="math display">\[A = \left( \begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; 0 &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; -1 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 1 \end{array} \right) \]</span>
asociada a un endomorfismo <span class="math inline">\(f:\mathbb{R}^{4} \to \mathbb{R}^{4}\)</span> en la base canónica.</p>
<p>Si tomamos la base
<span class="math display">\[\mathcal{B} = \left\{\left( \begin{array}{c} \frac{-\sqrt{6}}{6}\\ \frac{-\sqrt{6}}{6}\\ \frac{\sqrt{6}}{3}\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{\sqrt{6}}{6}\\ \frac{-\sqrt{6}}{6}\\ 0\\ \frac{\sqrt{6}}{3} \end{array} \right), \left( \begin{array}{c} \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{-\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ 0\\ \frac{\sqrt{3}}{3} \end{array} \right)\right\}\]</span>
resulta ser una <a href="espeuc.html#ortho-basis">base ortonormal</a> formada por autovectores de <span class="math inline">\(f\)</span> (en <a href="espeuc.html#diag-ortho2">la siguiente sección</a> detallaremos el proceso completo).</p>
<p>Por tanto, esta matriz (y el endomorfismo <span class="math inline">\(f\)</span>) es diagonalizable ortogonalmente. De hecho, si consideramos como matriz <span class="math inline">\(P\)</span> la del <a href="ev.html#cambiobase">cambio de base</a> de <span class="math inline">\(\mathcal{B}\)</span> a la canónica, podemos calcular la expresión diagonal, teniendo en cuenta que <span class="math inline">\(P\)</span> es una matriz ortogonal:</p>
<p><span class="math display">\[\begin{array}{rcl}D &amp; = &amp; P^{\text{t}}\ A\ P = \\  &amp; = &amp; \left( \begin{array}{cccc@{}} \frac{-\sqrt{6}}{6} &amp; \frac{\sqrt{6}}{6} &amp; \frac{\sqrt{3}}{3} &amp; \frac{-\sqrt{3}}{3}\\ \frac{-\sqrt{6}}{6} &amp; \frac{-\sqrt{6}}{6} &amp; \frac{\sqrt{3}}{3} &amp; \frac{\sqrt{3}}{3}\\ \frac{\sqrt{6}}{3} &amp; 0 &amp; \frac{\sqrt{3}}{3} &amp; 0\\ 0 &amp; \frac{\sqrt{6}}{3} &amp; 0 &amp; \frac{\sqrt{3}}{3} \end{array} \right) ^{\text{t}} \left( \begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; 0 &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; -1 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 1 \end{array} \right)  \left( \begin{array}{cccc@{}} \frac{-\sqrt{6}}{6} &amp; \frac{\sqrt{6}}{6} &amp; \frac{\sqrt{3}}{3} &amp; \frac{-\sqrt{3}}{3}\\ \frac{-\sqrt{6}}{6} &amp; \frac{-\sqrt{6}}{6} &amp; \frac{\sqrt{3}}{3} &amp; \frac{\sqrt{3}}{3}\\ \frac{\sqrt{6}}{3} &amp; 0 &amp; \frac{\sqrt{3}}{3} &amp; 0\\ 0 &amp; \frac{\sqrt{6}}{3} &amp; 0 &amp; \frac{\sqrt{3}}{3} \end{array} \right)  \\  &amp; = &amp; \left( \begin{array}{cccc@{}} 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; -3 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 3 \end{array} \right) \end{array}\]</span>

donde podemos comprobar que los elementos de la diagonal de <span class="math inline">\(D\)</span> son autovalores de <span class="math inline">\(A\)</span>.</p>
<hr />
</div>
<div id="diag-ortho2" class="section level2" number="5.9">
<h2 number="5.9"><span class="header-section-number">5.9</span> Qué matrices son diagonalizables ortogonalmente</h2>
<p>Supongamos una matriz <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span> que sea <a href="espeuc.html#diag-ortho">diagonalizable ortogonalmente</a>, es decir, tal que existe <span class="math inline">\(P\)</span> ortogonal con <span class="math inline">\(P^{\text{t}}\ A\ P = D\)</span> una matriz diagonal.</p>
<p>Como <span class="math inline">\(D\)</span> es diagonal, en particular tenemos que <span class="math inline">\(D^{\text{t}} = D\)</span>, luego:
<span class="math display">\[P^{\text{t}}\ A\ P = D = D^{\text{t}} = (P^{\text{t}}\ A\ P)^{\text{t}} = P^{\text{t}}\ A^{\text{t}}\ \left(P^{\text{t}}\right)^{\text{t}} = P^{\text{t}}\ A^{\text{t}}\ P\]</span>
Luego, multiplicando por <span class="math inline">\(P\)</span> a la izquierda en los extremos de la igualdad, y por <span class="math inline">\(P^{\text{t}}\)</span> en la derecha, recordando que <span class="math inline">\(P\)</span> es una <a href="espeuc.html#ortho-matrix">matriz ortogonal</a>, llegamos a que <span class="math inline">\(A = A^{\text{t}}\)</span>.</p>
<p>Esto significa que:</p>
<blockquote>
<p>Si una matriz <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span> es <em>diagonalizable ortogonalmente</em>, entonces es simétrica.</p>
</blockquote>
<p>Por tanto, una matriz <em>no simétrica</em> podrá ser <a href="diagonalización.html#diagonalizable">diagonalizable</a> (dependiendo de si cumple los criterios ya estudiados), pero <em>nunca</em> puede ser <em>diagonalizable ortogonalmente</em>.</p>
<p>Podemos estudiar entonces algunas de las propiedades de las matrices simétricas en lo que se refiere a diagonalización:</p>
<blockquote>
<p>Sea <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span> una matriz simétrica. Entonces:</p>
<ul>
<li>Todos los autovalores de <span class="math inline">\(A\)</span> son reales.</li>
<li>Si <span class="math inline">\(\lambda_1\)</span> y <span class="math inline">\(\lambda_2\)</span> son dos autovalores distintos, y sus subespacios asociados son <span class="math inline">\(U_{\lambda_1}\)</span> y <span class="math inline">\(U_{\lambda_2}\)</span>, respectivamente, entonces <span class="math inline">\(U_{\lambda_1}\)</span> es ortogonal a <span class="math inline">\(U_{\lambda_2}\)</span>.</li>
<li>La multiplicidad geométrica de cada autovalor de <span class="math inline">\(A\)</span> coincide con su multiplicidad algebraica.</li>
</ul>
</blockquote>
<p>En consecuencia</p>
<blockquote>
<p>Una matriz <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span> es diagonalizable ortogonalmente si y solo si es simétrica.</p>
</blockquote>
<p><strong>¿Cómo diagonalizamos ortogonalmente una matriz simétrica?</strong></p>
<p>El procedimiento es idéntico al que se conoce para <a href="diagonalización.html#diagonalizable">diagonalizar</a> una matriz cualquiera, con las siguientes puntualizaciones:</p>
<ul>
<li>Al ser una matriz simétrica, tenemos asegurada que es diagonalizable (incluso ortogonalmente).</li>
<li>Al calcular la base del subespacio asociado a un autovalor <span class="math inline">\(\lambda\)</span>, podemos hacer un paso extra, usando el <a href="espeuc.html#gram-schmidt">método de Gram-Schmidt</a>, para tener dicha base en forma ortonormal.</li>
<li>Al finalizar, al igual que hacíamos en la digonalización <em>estándar</em>, podemos unir todas las bases de los distintos subespacios asociados a los autovalores y tenemos una base ortonormal del espacio vectorial <span class="math inline">\(V\)</span>.</li>
</ul>
<p>De forma esquemática, incluyendo todo lo que conocemos de diagonalización, queda:</p>
<ol style="list-style-type: decimal">
<li>Calcular el <a href="diagonalización.html#eigenvalue">polinomio característico</a> <span class="math inline">\(p(\lambda)\)</span> de <span class="math inline">\(A\)</span>, y sus raíces, los autovalores de <span class="math inline">\(A\)</span>. Sean <span class="math inline">\(\lambda_1,\ldots,\lambda_k\)</span> los autovalores.</li>
<li>Para cada autovalor <span class="math inline">\(\lambda\)</span>:
<ol style="list-style-type: decimal">
<li>Determinar una <a href="ev.html#base">base</a> <span class="math inline">\(\mathcal{B}_{\lambda}\)</span> del <a href="diagonalización.html#eigenspace">subespacio propio</a> <span class="math inline">\(U_{\lambda}\)</span>.</li>
<li>Aplicar <a href="espeuc.html#gram-schmidt">el método de Gram-Schmidt</a> a <span class="math inline">\(\mathcal{B}_{\lambda}\)</span> para obtener una <a href="espeuc.html#ortho-basis">base _ortonormal</a> de <span class="math inline">\(U_{\lambda}\)</span> a la que llamaremos <span class="math inline">\(\mathcal{B}&#39;_{\lambda}\)</span>.</li>
</ol></li>
<li>Llamamos <span class="math inline">\(\mathcal{B} = \mathcal{B}&#39;_{\lambda_1}\cup\ldots\cup\mathcal{B}_{\lambda_k}\)</span>. Entonces <span class="math inline">\(\mathcal{B}\)</span> es una base ortonormal de <span class="math inline">\(V\)</span>. Además, la matriz <span class="math inline">\(P\)</span> de <a href="ev.html#cambiobase">cambio de base</a> de <span class="math inline">\(\mathcal{B}\)</span> a la canónica es una matriz <a href="espeuc.html#ortho-matrix">ortogonal</a>, tiene por columnas las coordenadas de los vectores de <span class="math inline">\(\mathcal{B}\)</span>, y verifica que <span class="math inline">\(D = P^{\text{t}}\ A\ P\)</span> es una matriz diagonal con los autovalores en la diagonal principal (con multiplicidades incluidas).</li>
</ol>
<hr />
<p><strong>Ejemplo</strong></p>
<p>Consideremos la matriz
<span class="math display">\[A = \left( \begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; 0 &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; -1 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 1 \end{array} \right) \]</span></p>
<p>Como es simétrica, es diagonalizable ortogonalmente. Vamos a seguir los pasos vistos para encontrar su forma diagonal y la base ortonormal de autovectores.</p>
<p><em>Paso 1: Encontrar los autovalores de <span class="math inline">\(A\)</span></em></p>
<p>Para calcular los autovalores de <span class="math inline">\(A\)</span>, comenzamos por construir su <a href="diagonalización.html#eigenvalue">polinomio característico</a>:
<span class="math display">\[\begin{array}{rcl}p(\lambda) &amp; = &amp; \mathrm{det}(A - \lambda\ I) = \\ &amp; = &amp; \mathrm{det}\left(\left( \begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; 0 &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; -1 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 1 \end{array} \right)  - \lambda \left( \begin{array}{cccc@{}} 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 \end{array} \right) \right) = \\ &amp; = &amp; \mathrm{det}\left( \begin{array}{cccc@{}} -\lambda &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; -\lambda &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; -1-\lambda &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 1-\lambda \end{array} \right)  \\ &amp; = &amp; \lambda^{4}-9\lambda^{2}\end{array}\]</span></p>
<p>Igualándolo a 0 y resolviendo la ecuación que queda, llegamos a que los autovalores son: <span class="math inline">\(\lambda = 0, -3, 3\)</span>, ya que:
<span class="math display">\[p(\lambda) = \lambda ^{2}\cdot (\lambda +3)\cdot (\lambda -3)\]</span></p>
<p><em>Paso 2: Encontrar una base ortonormal de cada subespacio propio</em></p>
<p>Recorremos ahora cada autovalor, encontrando una base del subespacio propio asociado, y ortonormalizándola usando el <a href="espeuc.html#gram-schmidt">método de Gram-Schmidt</a>.</p>
<ul>
<li>Para el autovalor <span class="math inline">\(\lambda = 0\)</span>:
<span class="math display">\[\begin{array}{rcl}(A - 0 I)\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = 0&amp; \Leftrightarrow &amp; \left( \begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; 0 &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; -1 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 1 \end{array} \right) \left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = 0 \Leftrightarrow \\&amp; \Leftrightarrow &amp; \left\{\begin{array}{rrrrcr}  &amp; -2y &amp; -z &amp; -t &amp; = &amp; 0\\ -2x &amp;  &amp; -z &amp; + t &amp; = &amp; 0\\ -x &amp; -y &amp; -z &amp;  &amp; = &amp; 0\\ -x &amp; + y &amp;  &amp; + t &amp; = &amp; 0\\ \end{array}\right. \\\end{array}\]</span></li>
</ul>
<p>Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de <span class="math inline">\(U_{ 0 }\)</span>:
<span class="math display">\[\left( \begin{array}{cccc|c} 0 &amp; -2 &amp; -1 &amp; -1 &amp; 0\\ -2 &amp; 0 &amp; -1 &amp; 1 &amp; 0\\ -1 &amp; -1 &amp; -1 &amp; 0 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \end{array} \right) \sim\left( \begin{array}{cccc|c} -2 &amp; 0 &amp; -1 &amp; 1 &amp; 0\\ 0 &amp; -2 &amp; -1 &amp; -1 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{array} \right) \]</span><span class="math display">\[\Rightarrow\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = \alpha\left(\begin{array}{c} - \frac{1}{2}\\ - \frac{1}{2}\\ 1\\ 0 \end{array}\right) {}+\beta\left(\begin{array}{c} \frac{1}{2}\\ - \frac{1}{2}\\ 0\\ 1 \end{array}\right) {}\]</span></p>
<p>Luego una base de <span class="math inline">\(U_{ 0 }\)</span> será:
<span class="math display">\[\mathcal{B}_{U_{0}} = \left\{\left( \begin{array}{c@{}} - \frac{1}{2}\\ - \frac{1}{2}\\ 1\\ 0 \end{array} \right) , \left( \begin{array}{c@{}} \frac{1}{2}\\ - \frac{1}{2}\\ 0\\ 1 \end{array} \right) \right\}\]</span>
El siguiente paso es <em>ortonormalizar</em> esta base, mediante el método de Gram-Schmidt. Llamemos <span class="math inline">\(v_i\)</span> a los vectores de esta base hallada.
Llamamos
<span class="math display">\[u_1 = v_1 = \left(\begin{array}{c} - \frac{1}{2}\\ - \frac{1}{2}\\ 1\\ 0 \end{array}\right) \]</span>
Procedemos con el resto de vectores:</p>
<p>Construimos:
<span class="math display">\[U_1 = \mathcal{L}(\{u_1\}) = \mathcal{L}(\left\{\left( \begin{array}{c@{}} - \frac{1}{2}\\ - \frac{1}{2}\\ 1\\ 0 \end{array} \right) \right\})\]</span>
Entonces
<span class="math display">\[\mathrm{proy}_{U_1}(v_2)  = \frac{\langle v_2, u_1 \rangle}{\langle u_1, u_1 \rangle} u_1  =  \frac{0}{\frac{3}{2}} \left( \begin{array}{c@{}} - \frac{1}{2}\\ - \frac{1}{2}\\ 1\\ 0 \end{array} \right)   = \left( \begin{array}{c@{}} 0\\ 0\\ 0\\ 0 \end{array} \right) \]</span>
Y llamamos
<span class="math display">\[u_2  = v_2 - \mathrm{proy}_{U_1}(v_2) = \left(\begin{array}{c} \frac{1}{2}\\ - \frac{1}{2}\\ 0\\ 1 \end{array}\right)  - \left( \begin{array}{c@{}} 0\\ 0\\ 0\\ 0 \end{array} \right)  =  \left( \begin{array}{c@{}} \frac{1}{2}\\ - \frac{1}{2}\\ 0\\ 1 \end{array} \right) \]</span></p>
<p>Y así hemos llegado a un sistema ortogonal formado por los vectores <span class="math inline">\(u_i\)</span>.
Debemos normalizar ahora los vectores de este sistema ortogonal que tenemos, multiplicando cada uno por el inverso de su norma, llegando a la siguiente base ortonormal de <span class="math inline">\(U_{ 0 }\)</span>.
<span class="math display">\[\mathcal{B}&#39;_{U_{0}} = \left\{\frac{1}{\frac{\sqrt{6}}{2}}\left( \begin{array}{c} \frac{-1}{2}\\ \frac{-1}{2}\\ 1\\ 0 \end{array} \right), \frac{1}{\frac{\sqrt{6}}{2}}\left( \begin{array}{c} \frac{1}{2}\\ \frac{-1}{2}\\ 0\\ 1 \end{array} \right)\right\} = \left\{\left( \begin{array}{c} \frac{-\sqrt{6}}{6}\\ \frac{-\sqrt{6}}{6}\\ \frac{\sqrt{6}}{3}\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{\sqrt{6}}{6}\\ \frac{-\sqrt{6}}{6}\\ 0\\ \frac{\sqrt{6}}{3} \end{array} \right)\right\}\]</span></p>
<ul>
<li>Para el autovalor <span class="math inline">\(\lambda = -3\)</span>:
<span class="math display">\[\begin{array}{rcl}(A + 3 I)\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = 0&amp; \Leftrightarrow &amp; \left( \begin{array}{cccc@{}} 3 &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; 3 &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; 2 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 4 \end{array} \right) \left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = 0 \Leftrightarrow \\&amp; \Leftrightarrow &amp; \left\{\begin{array}{rrrrcr} 3x &amp; -2y &amp; -z &amp; -t &amp; = &amp; 0\\ -2x &amp; + 3y &amp; -z &amp; + t &amp; = &amp; 0\\ -x &amp; -y &amp; + 2z &amp;  &amp; = &amp; 0\\ -x &amp; + y &amp;  &amp; + 4t &amp; = &amp; 0\\ \end{array}\right. \\\end{array}\]</span></li>
</ul>
<p>Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de <span class="math inline">\(U_{ -3 }\)</span>:
<span class="math display">\[\left( \begin{array}{cccc|c} 3 &amp; -2 &amp; -1 &amp; -1 &amp; 0\\ -2 &amp; 3 &amp; -1 &amp; 1 &amp; 0\\ -1 &amp; -1 &amp; 2 &amp; 0 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 4 &amp; 0 \end{array} \right) \sim\left( \begin{array}{cccc|c} 3 &amp; 0 &amp; -3 &amp; 0 &amp; 0\\ 0 &amp; \frac{5}{3} &amp; - \frac{5}{3} &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; \frac{18}{5} &amp; 0 \end{array} \right) \]</span><span class="math display">\[\Rightarrow\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = \alpha\left(\begin{array}{c} 1\\ 1\\ 1\\ 0 \end{array}\right) {}\]</span></p>
<p>Luego una base de <span class="math inline">\(U_{ -3 }\)</span> será:
<span class="math display">\[\mathcal{B}_{U_{-3}} = \left\{\left( \begin{array}{c@{}} 1\\ 1\\ 1\\ 0 \end{array} \right) \right\}\]</span></p>
<p>Como solo tenemos un vector en la base, ya forma un sistema ortogonal.
Debemos normalizar ahora los vectores de este sistema ortogonal que tenemos, multiplicando cada uno por el inverso de su norma, llegando a la siguiente base ortonormal de <span class="math inline">\(U_{ -3 }\)</span>.
<span class="math display">\[\mathcal{B}&#39;_{U_{-3}} = \left\{\frac{1}{\sqrt{3}}\left( \begin{array}{c} 1\\ 1\\ 1\\ 0 \end{array} \right)\right\} = \left\{\left( \begin{array}{c} \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ 0 \end{array} \right)\right\}\]</span></p>
<ul>
<li>Para el autovalor <span class="math inline">\(\lambda = 3\)</span>:
<span class="math display">\[\begin{array}{rcl}(A -3 I)\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = 0&amp; \Leftrightarrow &amp; \left( \begin{array}{cccc@{}} -3 &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; -3 &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; -4 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; -2 \end{array} \right) \left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = 0 \Leftrightarrow \\&amp; \Leftrightarrow &amp; \left\{\begin{array}{rrrrcr} -3x &amp; -2y &amp; -z &amp; -t &amp; = &amp; 0\\ -2x &amp; -3y &amp; -z &amp; + t &amp; = &amp; 0\\ -x &amp; -y &amp; -4z &amp;  &amp; = &amp; 0\\ -x &amp; + y &amp;  &amp; -2t &amp; = &amp; 0\\ \end{array}\right. \\\end{array}\]</span></li>
</ul>
<p>Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de <span class="math inline">\(U_{ 3 }\)</span>:
<span class="math display">\[\left( \begin{array}{cccc|c} -3 &amp; -2 &amp; -1 &amp; -1 &amp; 0\\ -2 &amp; -3 &amp; -1 &amp; 1 &amp; 0\\ -1 &amp; -1 &amp; -4 &amp; 0 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; -2 &amp; 0 \end{array} \right) \sim\left( \begin{array}{cccc|c} -3 &amp; 0 &amp; 0 &amp; -3 &amp; 0\\ 0 &amp; - \frac{5}{3} &amp; 0 &amp; \frac{5}{3} &amp; 0\\ 0 &amp; 0 &amp; - \frac{18}{5} &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{array} \right) \]</span><span class="math display">\[\Rightarrow\left(\begin{array}{c} x\\ y\\ z\\ t \end{array}\right)  = \alpha\left(\begin{array}{c} -1\\ 1\\ 0\\ 1 \end{array}\right) {}\]</span></p>
<p>Luego una base de <span class="math inline">\(U_{ 3 }\)</span> será:
<span class="math display">\[\mathcal{B}_{U_{3}} = \left\{\left( \begin{array}{c@{}} -1\\ 1\\ 0\\ 1 \end{array} \right) \right\}\]</span></p>
<p>Como solo tenemos un vector en la base, ya forma un sistema ortogonal.
Debemos normalizar ahora los vectores de este sistema ortogonal que tenemos, multiplicando cada uno por el inverso de su norma, llegando a la siguiente base ortonormal de <span class="math inline">\(U_{ 3 }\)</span>.
<span class="math display">\[\mathcal{B}&#39;_{U_{3}} = \left\{\frac{1}{\sqrt{3}}\left( \begin{array}{c} -1\\ 1\\ 0\\ 1 \end{array} \right)\right\} = \left\{\left( \begin{array}{c} \frac{-\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ 0\\ \frac{\sqrt{3}}{3} \end{array} \right)\right\}\]</span></p>
<p><em>Paso 3: Hallar una base ortonormal formada por autovectores</em></p>
<p>Definimos
<span class="math display">\[\mathcal{B} = \mathcal{B}&#39;_{U_{0}} \cup \mathcal{B}&#39;_{U_{-3}} \cup \mathcal{B}&#39;_{U_{3}} = \left\{\left( \begin{array}{c} \frac{-\sqrt{6}}{6}\\ \frac{-\sqrt{6}}{6}\\ \frac{\sqrt{6}}{3}\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{\sqrt{6}}{6}\\ \frac{-\sqrt{6}}{6}\\ 0\\ \frac{\sqrt{6}}{3} \end{array} \right), \left( \begin{array}{c} \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ 0 \end{array} \right), \left( \begin{array}{c} \frac{-\sqrt{3}}{3}\\ \frac{\sqrt{3}}{3}\\ 0\\ \frac{\sqrt{3}}{3} \end{array} \right)\right\}\]</span>
Es una base ortonormal de <span class="math inline">\(\mathbb{R}^{4}\)</span>, formada únicamente por autovectores de la matriz <span class="math inline">\(A\)</span>.</p>
<p>Además, la matriz <span class="math inline">\(P\)</span> del <a href="ev.html#cambiobase">cambio de base</a> de esta base <span class="math inline">\(\mathcal{B}\)</span> a la canónica, formada por los vectores de <span class="math inline">\(\mathcal{B}\)</span> puestos por columnas, es <em>ortogonal</em> y nos proporciona la relación entre <span class="math inline">\(A\)</span> y la matriz diagonal <span class="math inline">\(D\)</span> de los autovalores:

<span class="math display">\[\begin{array}{rcl}D &amp; = &amp; P^{\text{t}}\ A\ P = \\ &amp; = &amp; \left( \begin{array}{cccc@{}} \frac{-\sqrt{6}}{6} &amp; \frac{-\sqrt{6}}{6} &amp; \frac{\sqrt{6}}{3} &amp; 0\\ \frac{\sqrt{6}}{6} &amp; \frac{-\sqrt{6}}{6} &amp; 0 &amp; \frac{\sqrt{6}}{3}\\ \frac{\sqrt{3}}{3} &amp; \frac{\sqrt{3}}{3} &amp; \frac{\sqrt{3}}{3} &amp; 0\\ \frac{-\sqrt{3}}{3} &amp; \frac{\sqrt{3}}{3} &amp; 0 &amp; \frac{\sqrt{3}}{3} \end{array} \right) \ \left( \begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\ -2 &amp; 0 &amp; -1 &amp; 1\\ -1 &amp; -1 &amp; -1 &amp; 0\\ -1 &amp; 1 &amp; 0 &amp; 1 \end{array} \right) \ \left( \begin{array}{cccc@{}} \frac{-\sqrt{6}}{6} &amp; \frac{\sqrt{6}}{6} &amp; \frac{\sqrt{3}}{3} &amp; \frac{-\sqrt{3}}{3}\\ \frac{-\sqrt{6}}{6} &amp; \frac{-\sqrt{6}}{6} &amp; \frac{\sqrt{3}}{3} &amp; \frac{\sqrt{3}}{3}\\ \frac{\sqrt{6}}{3} &amp; 0 &amp; \frac{\sqrt{3}}{3} &amp; 0\\ 0 &amp; \frac{\sqrt{6}}{3} &amp; 0 &amp; \frac{\sqrt{3}}{3} \end{array} \right)  \\ &amp; = &amp; \left( \begin{array}{cccc@{}} 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; -3 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 3 \end{array} \right) \end{array}\]</span>
</p>
<p>Recordemos que en la diagonal de <span class="math inline">\(D\)</span> se encuentran los autovalores (en el mismo orden en que se han puesto los autovectores en las columnas de <span class="math inline">\(P\)</span>), con sus respectivas multiplicidades.</p>
<hr />

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="diagonalización.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="problems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Algebra_Lineal.pdf", "Algebra_Lineal.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
