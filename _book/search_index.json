[
["espacios-euclídeos.html", "5 Espacios Euclídeos 5.1 Qué son las normas y distancias 5.2 Qué significa el concepto de ortogonalidad 5.3 Qué es el complemento ortogonal de un subespacio 5.4 Cómo calculamos la proyección de un vector sobre un subespacio 5.5 Qué es y qué utilidad tiene una base ortonormal 5.6 Cómo construimos una base ortonormal 5.7 Qué es una matriz ortogonal y una aplicación ortogonal 5.8 Qué es la diagonalización ortogonal 5.9 Qué matrices son diagonalizables ortogonalmente", " 5 Espacios Euclídeos En este capítulo, damos un paso para acercarnos a los conceptos geométricos clásicos de distancias y ángulos, abstrayéndolos en el contexto de los espacios vectoriales. Para ello, precisaremos de la noción de producto escalar: Una aplicación \\(g:V\\times V\\to \\mathbb{R}\\), donde \\(V\\) es un espacio vectorial sobre \\(\\mathbb{R}\\), es un producto escalar si verifica: Es simétrica: \\(g(u, v) = g(v, u)\\) para todo \\(u,v\\in V\\). Es bilineal: \\(g(\\alpha u + \\beta v, w) = \\alpha g(u, w) + \\beta g(v, w)\\) para todos los \\(\\alpha,\\beta\\in\\mathbb{R}\\), \\(u,v,w\\in V\\). Por simetría, se tiene lo mismo para la linealidad en la segunda componente. Es definida positiva: \\(g(v, v) &gt; 0\\) para todo \\(v\\in V\\setminus\\{0\\}\\), \\(g(v, v) = 0\\) si, y sólo si \\(v = 0\\). En resumen, se suele definir un producto escalar como una forma bilineal, simétrica y definida positiva. Notación: El producto escalar de dos vectores \\(u\\) y \\(v\\) suele recibir diversas notaciones, aunque las más usadas son \\(u\\cdot v\\) y \\(\\langle u, v \\rangle\\). A un espacio vectorial \\(V\\) donde podemos definir un producto vectorial lo llamamos espacio euclídeo, y lo solemos denotar mediante el par \\(\\left(V, \\langle\\cdot,\\cdot\\rangle\\right)\\). Ejemplo En \\(\\mathbb{R}^n\\), disponemos de un producto escalar usual, de forma que si \\(u, v\\in\\mathbb{R}^n\\), entonces \\[\\langle u,v \\rangle = u_1v_1+u_2v_2+\\ldots+u_nv_n\\] siendo \\(u_i, v_i\\), \\(i=1,\\ldots,n\\) las coordenadas de \\(u\\) y de \\(v\\), respectivamente, en una base dada (la canónica como caso particular). A este producto vectorial es al que nos referiremos cuando usemos la notación \\(u\\cdot v\\). Podemos generalizar este producto escalar, definiendo unos pesos \\(\\alpha_i\\in\\mathbb{R}^{+}\\) para cada componente: \\[\\langle u,v \\rangle = \\alpha_1u_1v_1+\\alpha_2u_2v_2+\\ldots+\\alpha_nu_nv_n\\] Podemos construir productos escalares de la siguiente forma: Tomemos una matriz \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) simétrica y definida positiva. Entonces la aplicación \\(\\langle\\cdot,\\cdot\\rangle:\\mathbb{R}^n\\times\\mathbb{R}^n\\to\\mathbb{R}\\) definida por \\[\\langle u, v \\rangle = u^{\\text{t}}\\ A\\ v\\] es un producto escalar. A esta matriz \\(A\\) se le denomina matriz de Gram, y todo producto escalar tiene asociada una. Los dos ejemplos anteriores se correspondían con tomar como \\(A\\) la matriz identidad o una matriz diagonal con elementos positivos \\(\\alpha_i\\in\\mathbb{R}^{+}\\) en la diagonal. ¿Qué preguntas vamos a responder en este capítulo? ¿Qué son las normas y distancias? ¿Qué significa el concepto de ortogonalidad? ¿Qué es el complemento ortogonal de un subespacio? ¿Cómo calculamos la proyección de un vector sobre un subespacio? ¿Qué es y qué utilidad tiene una base ortonormal? ¿Cómo construimos una base ortonormal? ¿Qué es una matriz ortogonal y una aplicación ortogonal? ¿Qué es la diagonalización ortogonal? Qué matrices son diagonalizables ortogonalmente 5.1 Qué son las normas y distancias Una norma vectorial sobre un espacio \\(V\\) (sobre un cuerpo \\(\\mathbb{K}\\)) es un operador que sirve para medir la longitud de un vector (es decir, su distancia al origen), y se define formalmente como una aplicación \\(\\|\\cdot\\|:V\\to\\mathbb{R}\\) que verifica: Para todo \\(v\\in V\\), \\(\\|v\\| &gt; 0\\), con \\(\\|v\\| = 0\\) si, y solo si, \\(v = 0\\). Para todo \\(c\\in\\mathbb{K}\\), \\(v\\in V\\), se tiene \\(\\|cv\\| = |c|\\cdot \\|v\\|\\). Para todo \\(u,v\\in V\\), \\(\\|u+v\\|\\le\\|u\\|+\\|v\\|\\). Esta propiedad se llama desigualdad triangular (en un triángulo, la suma de la longitud de dos de sus lados – \\(\\|u\\|\\) y \\(\\|v\\|\\) – es siempre menor que la longitud de su otro lado – \\(\\|u+v\\|\\) – ). Cualquier operador \\(\\|\\cdot\\|\\) que verifique lo anterior es una norma vectorial. Sin embargo, podemos construir normas a partir de un producto escalar: Si \\(\\langle\\cdot,\\cdot\\rangle\\) es un producto escalar en el espacio \\(V\\), entonces la aplicación \\(\\|\\cdot\\|:V\\to\\mathbb{R}\\) dada por \\(\\|v\\| = \\sqrt{\\langle v, v\\rangle}\\) es una norma derivada del producto escalar sobre \\(V\\). Al definir la norma vectorial a partir de un producto escalar, además, tenemos la siguiente propiedad que será de gran interés en la siguiente sección: (Desigualdad de Cauchy-Schwarz) Si \\(\\|\\cdot\\|\\) es una norma derivada del producto escalar \\(\\langle\\cdot,\\cdot\\rangle\\), entonces \\[|\\langle u, v \\rangle| \\le \\|u\\|\\cdot\\|v\\|\\] para todo \\(u,v\\in V\\). Un concepto implícitamente relacionado con el de norma vectorial es el de distancia. Una distancia (también llamada métrica) es un operador \\(d:V\\times V\\to R\\) que verifica las siguientes propiedades: \\(d(u, v) = 0\\) si, y solo si, \\(u = v\\). Es simétrica: \\(d(u, v) = d(v, u)\\). Cumple la desigualdad triangular: \\(d(u, v) \\le d(u, w) + d(w, v)\\). Además, de aquí se deduce que \\(d(u, v) \\ge 0\\) para todo \\(u,v\\in V\\). Podemos deducir una distancia a partir de una norma, para así completar las relaciones existentes entre los conceptos aquí explicados: Si \\(\\|\\cdot\\|\\) es una norma sobre un espacio vectorial \\(V\\), entonces la aplicación \\(d:V\\times V\\to \\mathbb{R}\\) definida por \\[d(u, v) = \\|u-v\\|\\] para todo \\(u,v\\in V\\) es una distancia. Nota: Existen muchas más normas de las que se deducen a partir de un producto escalar, al igual que más distancias. Ejemplo Podemos deducir la expresión de la norma (a veces llamada módulo) de un vector \\(v\\in\\mathbb{R}^n\\). Si \\(v=\\left(\\begin{array}{c}v_1\\\\ v_2\\\\\\vdots\\\\ v_n\\end{array}\\right)\\), entonces \\[\\|v\\| = \\sqrt{v\\cdot v} = \\sqrt{v_1^2+v_2^2+\\ldots+v_n^2}\\] Y así, además, deducimos la expresión para la distancia euclídea usual en \\(\\mathbb{R}^n\\): \\[d(u,v) = \\|u - v\\| = \\sqrt{(u_1-v_1)^2+\\ldots+(u_n-v_n)^2}\\] Como ejemplo, podemos calcular la norma del vector \\[v = \\left(\\begin{array}{c} -2\\\\ 1\\\\ 1\\\\ 1 \\end{array}\\right) \\]\\[\\|v\\| = \\sqrt{(-2)^2+1^2+1^2+1^2} = \\sqrt{7}\\] 5.2 Qué significa el concepto de ortogonalidad Si partimos de la desigualdad de Cauchy-Schwarz que hemos visto antes, podemos llegar a que \\[-1 \\le \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v|} \\le 1\\] para todo \\(u,v\\in V\\). Por otro lado, la función coseno restringida a \\([0, \\pi]\\), es decir, \\(\\cos:[0,\\pi]\\to[-1,1]\\), es biyectiva. Eso quiere decir que existe un único \\(\\theta\\in[0, \\pi]\\) tal que \\(\\cos\\theta = \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v|}\\). Llamamos entonces ángulo entre los vectores \\(u,v\\in V\\) al único \\(\\theta\\in[0,\\pi]\\) tal que \\(\\cos\\theta = \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v|}\\). Diremos que dos vectores \\(u\\) y \\(v\\) de \\(V\\) son ortogonales (o perpendiculares) si el ángulo que forman es \\(\\frac{\\pi}{2}\\), es decir, si \\(\\langle u, v \\rangle = 0\\). La noción de ortogonalidad en el espacio euclídeo se corresponde con la perpendicularidad de vectores que todos tenemos de forma intuitiva. Lo podemos extender también a ortogonalidad con respecto a un espacio vectorial: Un vector \\(v\\in V\\) es ortogonal al subespacio \\(U\\) de \\(V\\) si, y sólo si, es ortogonal a todos y cada uno de los vectores de \\(U\\), es decir, \\(\\langle v, u \\rangle = 0\\) para todo \\(u\\in U\\). Sea \\(U\\) un subespacio de \\(V\\), y sea \\(\\mathcal{B} = \\{u_1,\\ldots,u_k\\}\\) una base de \\(U\\). Un vector \\(v\\in V\\) es ortogonal a \\(U\\) si, y solo si, es ortogonal a cada \\(u_i\\): \\(\\langle v, u_i \\rangle = 0\\) para todo \\(i=1,\\ldots,k\\). Un sistema ortogonal es un conjunto de vectores \\(\\{v_1,\\ldots,v_m\\}\\subset V\\) donde los \\(v_i\\) son ortogonales dos a dos, es decir, \\(\\langle v_i, v_j \\rangle = 0\\) para todo \\(i\\ne j\\). Algunas propiedades y caracterizaciones de los vectores y sistemas ortogonales Dos vectores \\(u\\) y \\(w\\) son ortogonales si, y solo si, \\(\\|u+w\\|^2 = \\|u\\|^2 + \\|v\\|^2\\) (generalización del Teorema de Pitágoras). Si un sistema de vectores es ortogonal, entonces es linealmente independiente. Por tanto, en un espacio \\(V\\) de dimensión finita \\(n\\), cualquier conjunto de \\(n\\) vectores ortogonales entre sí es una base. Este último resultado es interesante, pues nos constata que cualquier sistema ortogonal con tantos vectores como la dimensión del espacio vectorial es una base. Ejemplo Consideremos el espacio \\(U\\) dado por: \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} -x+ 4y+ z &amp; = &amp; 0\\\\ y+ t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] y los vectores \\[v = \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) \\quad\\quad w = \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ -1 \\end{array}\\right) \\] Calculemos el ángulo entre \\(v\\) y \\(w\\): \\[\\theta = \\arccos \\frac{v\\cdot w}{\\|v\\|\\cdot\\|w\\|}\\]\\[v\\cdot w = 1\\cdot(-1)+(-2)\\cdot2+(-1)\\cdot(-1)+2\\cdot(-1) = -6\\]\\[\\|v\\| = \\sqrt{1^2+(-2)^2+(-1)^2+2^2} = \\sqrt{10}\\]\\[\\|w\\| = \\sqrt{(-1)^2+2^2+(-1)^2+(-1)^2} = \\sqrt{7}\\] Luego \\[\\theta = \\arccos \\frac{-6}{\\sqrt{10}\\sqrt{7}} = \\arccos \\frac{-3\\cdot\\sqrt{70}}{35}\\] Vamos a comprobar ahora que \\(v\\) es ortogonal al subespacio \\(U\\). Para ello, a partir de las ecuaciones cartesianas de \\(U\\), vamos a hallar una base suya, resolviendo el sistema por Gauss-Jordan y pasando por las ecuaciones paramétricas: \\[\\left( \\begin{array}{cccc@{}} -1 &amp; 4 &amp; 1 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc@{}} 1 &amp; 0 &amp; -1 &amp; 4\\\\ 0 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De aquí que la base de \\(U\\) sea: \\[\\mathcal{B}_U = \\left\\{u_i\\right\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Como hemos visto antes, para comprobar que \\(v\\) es ortogonal a \\(U\\), nos basta con comprobar que lo es a cada vector de \\(\\mathcal{B}_U\\), hallando su producto escalar: \\[\\langle v, u_1 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) \\rangle = 1\\cdot1+(-2)\\cdot0+(-1)\\cdot1+2\\cdot0 = 0\\]\\[\\langle v, u_2 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-4)+(-2)\\cdot(-1)+(-1)\\cdot0+2\\cdot1 = 0\\] Consideremos ahora el siguiente sistema de vectores: \\[S = \\{s_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array} \\right) \\right\\}\\] Podemos comprobar fácilmente que es un sistema ortogonal, hallando los productos escalares de los vectores de \\(S\\) dos a dos: \\[\\langle s_1, s_2 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) \\rangle = 1\\cdot(-1)+1\\cdot2+1\\cdot(-1)+0\\cdot0 = 0\\]\\[\\langle s_1, s_3 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) \\rangle = 1\\cdot1+1\\cdot0+1\\cdot(-1)+0\\cdot2 = 0\\]\\[\\langle s_1, s_4 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-1)+1\\cdot0+1\\cdot1+0\\cdot1 = 0\\]\\[\\langle s_2, s_3 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) \\rangle = (-1)\\cdot1+2\\cdot0+(-1)\\cdot(-1)+0\\cdot2 = 0\\]\\[\\langle s_2, s_4 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = (-1)\\cdot(-1)+2\\cdot0+(-1)\\cdot1+0\\cdot1 = 0\\]\\[\\langle s_3, s_4 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-1)+0\\cdot0+(-1)\\cdot1+2\\cdot1 = 0\\] Tenemos, por tanto, un sistema de 4 vectores ortogonales en \\(\\mathbb{R}^{4}\\), y, como hemos comentado antes, son linealmente independientes, luego forman una base del espacio vectorial. 5.3 Qué es el complemento ortogonal de un subespacio Consideremos un espacio vectorial \\(V\\) con producto escalar \\(\\langle \\cdot,\\cdot \\rangle\\), y sea \\(U\\) un subespacio de \\(V\\). Llamamos complemento ortogonal de \\(U\\) al conjunto de vectores que son ortogonales a todos los de \\(U\\): \\[U^{\\perp} = \\{v\\in V: \\langle v, u \\rangle = 0\\,\\,\\text{para todo }u\\in U\\}\\] Tenemos las siguientes propiedades del complemento ortogonal de un subespacio: \\(U^{\\perp}\\) es un subespacio vectorial de \\(V\\). \\(U\\cap U^{\\perp} = \\{0\\}\\). Si \\(V\\) es de dimensión finita, \\(\\mathrm{dim}(U) + \\mathrm{dim}(U^{\\perp}) = \\mathrm{dim}(V)\\). Como consecuencia, este resultado nos dice que \\(V\\) se puede descomponer como la suma directa de \\(U\\) y de su complemento ortogonal: \\(U\\oplus U^{\\perp} = V\\). ¿Cómo calculamos el complemento ortogonal de un subespacio? Hemos de hallar los vectores que son ortogonales a un subespacio \\(U\\). Como hemos visto antes, un vector \\(v\\) es ortogonal a \\(U\\) si, y solo si, es ortogonal a todos los vectores de una base de \\(U\\). Así pues, partimos de que tenemos una base \\(\\mathcal{B} = \\{u_1,\\ldots,u_m\\}\\) de \\(U\\) y tomemos un vector genérico \\(v\\in V\\). Para que \\(v\\in U^{\\perp}\\), debe cumplirse que \\(\\langle v, u_i\\rangle = 0\\) para todo \\(i=1,\\ldots,m\\). Cada una de las restricciones \\(\\langle v, u_i\\rangle = 0\\) nos proporciona una ecuación lineal que debe verificar un vector para poder pertenecer a \\(U^{\\perp}\\). Tomamos, por tanto, todas las ecuaciones lineales que se deducen de las restricciones anteriores: ese sistema de ecuaciones nos representa las ecuaciones cartesianas del espacio \\(U^{\\perp}\\). En el caso concreto de un espacio \\(V=\\mathbb{R}^n\\) con el producto escalar euclídeo, podemos ver qué aspecto tienen las ecuaciones del tipo \\(\\langle v, u_i \\rangle = 0\\). Supongamos que los vectores \\(v\\) y \\(u_i\\) tienen por coordenadas, en la base canónica, las siguientes: \\[v = \\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots \\\\ x_n\\\\ \\end{array}\\right), \\quad u_i = \\left( \\begin{array}{c} \\alpha_{i,1}\\\\ \\alpha_{i,2}\\\\ \\vdots \\\\ \\alpha_{i,n}\\\\ \\end{array}\\right)\\] Entonces \\[ \\langle v, u_i \\rangle = 0 \\Leftrightarrow \\langle \\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots \\\\ x_n\\\\ \\end{array}\\right) , \\left( \\begin{array}{c} \\alpha_{i,1}\\\\ \\alpha_{i,2}\\\\ \\vdots \\\\ \\alpha_{i,n}\\\\ \\end{array}\\right) \\rangle = 0 \\Leftrightarrow \\alpha_{i,1}x_1+\\alpha_{i,2}x_2+\\ldots+\\alpha_{i,n}x_n = 0 \\] Si nos fijamos, nos queda una ecuación lineal cuyas incógnitas son las coordenadas canónicas de \\(v\\) y cuyos coeficientes son las coordenadas de \\(u_i\\). Si repetimos el proceso para todo \\(i=1,\\ldots,m\\), tenemos el siguiente sistema de ecuaciones: \\[Av = 0\\quad\\text{donde}\\quad A = \\left( \\begin{array}{cccc} \\alpha_{1,1} &amp; \\alpha_{1,2} &amp; \\ldots &amp; \\alpha_{1,n} \\\\ \\alpha_{2,1} &amp; \\alpha_{2,2} &amp; \\ldots &amp; \\alpha_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\alpha_{m,1} &amp; \\alpha_{m,2} &amp; \\ldots &amp; \\alpha_{m,n} \\\\ \\end{array} \\right)\\] Es decir, las ecuaciones cartesianas de \\(U^{\\perp}\\) son las que tienen por matriz de coeficientes a las coordenadas de los vectores de la base de \\(U\\) puestos por filas. De hecho, si llamamos \\(B\\) a la matriz que resulta de poner por columnas los vectores de \\(\\mathcal{B}\\), entonces \\(A = B^{\\mathrm{t}}\\). A partir de esas ecuaciones cartesianas, ya podríamos encontrar una base del subespacio \\(U^{\\perp}\\). Ejemplo Consideremos el espacio \\(U\\) generado por la base siguiente: \\[\\mathcal{B} = \\{u_i\\} = \\left\\{\\left( \\begin{array}{c@{}} -2\\\\ 0\\\\ -2\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -2\\\\ 2 \\end{array} \\right) \\right\\}\\] Para hallar su complemento ortogonal, suponemos un vector genérico \\[v = \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in \\mathbb{R}^{4}\\] y hacemos su producto escalar por todos los vectores de \\(\\mathcal{B}\\), imponiendo que valga 0: \\[\\langle v, u_1 \\rangle = 0\\Leftrightarrow\\langle \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) , \\left(\\begin{array}{c} -2\\\\ 0\\\\ -2\\\\ 2 \\end{array}\\right) = 0\\Leftrightarrow\\ \\begin{array}{ccr} -2x-2z+ 2t &amp; = &amp; 0\\\\ \\end{array}\\]\\[\\langle v, u_2 \\rangle = 0\\Leftrightarrow\\langle \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ -1\\\\ -2\\\\ 2 \\end{array}\\right) = 0\\Leftrightarrow\\ \\begin{array}{ccr} -x-y-2z+ 2t &amp; = &amp; 0\\\\ \\end{array}\\] Hemos llegado, por tanto, al siguiente sistema de ecuaciones que representan las ecuaciones cartesianas de \\(U^{\\perp}\\): \\[\\begin{array}{rrrrcr} -2x &amp; &amp; -2z &amp; + 2t &amp; = &amp; 0\\\\ -x &amp; -y &amp; -2z &amp; + 2t &amp; = &amp; 0\\\\ \\end{array}\\] Nos podemos dar cuenta de que la matriz del sistema es la que resulta de poner los vectores de la base \\(\\mathcal{B}\\) por filas. Como siempre, podemos resolver el sistema por Gauss-Jordan para deducir la forma paramétrica de la solución, y, por tanto, un sistema generador de \\(U^{\\perp}\\): \\[\\left( \\begin{array}{cccc@{}} -2 &amp; 0 &amp; -2 &amp; 2\\\\ -1 &amp; -1 &amp; -2 &amp; 2 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc@{}} 1 &amp; 0 &amp; 1 &amp; -1\\\\ 0 &amp; 1 &amp; 1 &amp; -1 \\end{array} \\right) \\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} -1\\\\ -1\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} 1\\\\ 1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De aquí que una base del complemento ortogonal \\(U^{\\perp}\\) sea: \\[\\mathcal{B}&#39; = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Podemos comprobar fácilmente que los vectores de \\(\\mathcal{B}&#39;\\) son ortogonales a los de \\(\\mathcal{B}\\). 5.4 Cómo calculamos la proyección de un vector sobre un subespacio Consideremos un espacio euclídeo \\(\\left(V, \\langle\\cdot,\\cdot\\rangle\\right)\\). Consideremos un subespacio \\(U\\), del que conocemos una base ortogonal \\(\\mathcal{B}_U = \\{u_1,\\ldots,u_m\\}\\). Definimos la proyección ortogonal de un vector \\(v\\) sobre el subespacio \\(U\\), como: \\[\\mathrm{proy}_U(v) = \\frac{\\langle v, u_1\\rangle}{\\langle u_1, u_1 \\rangle}u_1 + \\frac{\\langle v, u_2\\rangle}{\\langle u_2, u_2 \\rangle}u_2 + \\ldots + \\frac{\\langle v, u_m\\rangle}{\\langle u_m, u_m \\rangle}u_m\\] Es decir, la proyección \\(\\mathrm{proy}_U(v)\\) es un vector del subespacio \\(U\\), ya que es combinación lineal de los elementos de su base, y además esta combinación lineal tiene como coeficientes el producto escalar de \\(v\\) por cada elemento de la base, dividido por la norma de \\(u_i\\) al cuadrado (recordemos que \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\)). Evidentemente, si los elementos de la base de \\(U\\) tienen norma 1, el denominador desaparece. La proyección de \\(v\\) sobre el subespacio \\(U\\) tiene la propiedad siguiente: Si \\(u\\) es un vector del subespacio \\(U\\), y \\(v\\in V\\), entonces \\(d(u, v) \\ge d(\\mathrm{proy}_U(v), v)\\), y la única forma para que se dé la igualdad es que \\(u = \\mathrm{proy}_U(v)\\). Esto significa que la proyección ortogonal de \\(v\\) sobre \\(U\\) es el vector de \\(U\\) más cercano (según la distancia \\(d\\)) a \\(v\\). Como caso particular, podríamos definir la proyección ortogonal de un vector \\(v\\) sobre otro vector \\(u\\) como \\[\\mathrm{proy}_u(v) = \\frac{\\langle v, u \\rangle}{\\langle u, u\\rangle}u\\] (realmente es la proyección del vector \\(v\\) sobre el subespacio generado por el vector \\(u\\)). Podemos entonces concluir que, si la base ortogonal de \\(U\\) es \\(\\mathcal{B}_U = \\{u_1,\\ldots,u_m\\}\\), entonces \\[\\mathrm{proy}_U(v) = \\mathrm{proy}_{u_1}(v)+\\ldots+\\mathrm{proy}_{u_m}(v)\\] A partir del hecho de que la base de \\(U\\) es ortogonal, podemos deducir que: Si \\(U\\) es un subespacio de \\(V\\), \\(v\\in V\\), y una base ortogonal de \\(U\\) es \\(\\mathcal{B}_U = \\{u_1,\\ldots,u_m\\}\\), entonces el vector \\(v - \\mathrm{proy}_U(v)\\) es ortogonal a \\(U\\), es decir, \\(v - \\mathrm{proy}_U(v) \\in U^{\\perp}\\) está en el complemento ortogonal de \\(U\\). Esto, junto con que \\(\\mathrm{proy}_U(v)\\in U\\), nos justifican el siguiente resultado, llamado teorema de la descomposición ortogonal: En un espacio euclídeo \\((V, \\langle\\cdot,\\cdot\\rangle)\\), dado un subespacio \\(U\\), y \\(v\\in V\\), existe una única descomposición de \\(v\\) como suma de dos vectores ortogonales, \\(v = v_1 + v_2\\), tales que \\(v_1\\in U\\) y \\(v_2\\in U^{\\perp}\\). Concretamente, \\(v_1 = \\mathrm{proy}_U(v)\\) y \\(v_2 = v - v_1 = \\mathrm{proy}_{U^{\\perp}}(v)\\). Podemos siempre descomponer un vector de \\(V\\) como suma de dos vectores, uno de \\(U\\) y otro de \\(U^{\\perp}\\), que coinciden con las proyecciones sobre ambos subespacios. Ejemplo Consideremos el subespacio \\(U\\) cuya base es \\[\\mathcal{B} = \\{u_i\\} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array} \\right) \\right\\}\\] Notemos que \\(\\mathcal{B}\\) es una base ortogonal. Consideremos el vector \\[v = \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right) \\] Buscamos la descomposición ortogonal de \\(v\\) con respecto al subespacio \\(U\\). Sabemos que podemos escribir \\(v = v_1 + v_2\\), donde \\(v_1 = \\mathrm{proy}_U(v)\\) y \\(v_2 = v - v_1 \\in U^{\\perp}\\). \\[v_1 = \\mathrm{proy}_U(v) = \\mathrm{proy}_{u_1}(v)+\\mathrm{proy}_{u_2}(v) = \\frac{\\langle v, u_1\\rangle}{\\langle u_1, u_1 \\rangle} u_1+\\frac{\\langle v, u_2\\rangle}{\\langle u_2, u_2 \\rangle} u_2\\] Si calculamos esas cantidades \\[\\langle v, u_1 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) \\rangle = (-1)\\cdot(-1)+1\\cdot1+(-2)\\cdot(-1)+0\\cdot(-1) = 4\\]\\[\\langle v, u_2 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array}\\right) \\rangle = (-1)\\cdot(-3)+1\\cdot(-5)+(-2)\\cdot1+0\\cdot(-3) = -4\\]\\[\\langle u_1, u_1 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) \\rangle = (-1)^2+1^2+(-1)^2+(-1)^2 = 4\\]\\[\\langle u_2, u_2 \\rangle = \\langle \\left(\\begin{array}{c} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array}\\right) , \\left(\\begin{array}{c} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array}\\right) \\rangle = (-3)^2+(-5)^2+1^2+(-3)^2 = 44\\] podemos sustituirlas en la expresión de más arriba y llegamos a \\[v_1 = 1\\cdot\\left(\\begin{array}{c} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) +(- \\frac{1}{11})\\cdot\\left(\\begin{array}{c} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array}\\right) = \\left( \\begin{array}{c@{}} - \\frac{8}{11}\\\\ \\frac{16}{11}\\\\ - \\frac{12}{11}\\\\ - \\frac{8}{11} \\end{array} \\right) \\] Por tanto, \\[v_2 = \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right) - \\left( \\begin{array}{c@{}} - \\frac{8}{11}\\\\ \\frac{16}{11}\\\\ - \\frac{12}{11}\\\\ - \\frac{8}{11} \\end{array} \\right) = \\left( \\begin{array}{c@{}} - \\frac{3}{11}\\\\ - \\frac{5}{11}\\\\ - \\frac{10}{11}\\\\ \\frac{8}{11} \\end{array} \\right) \\] Por lo comentado anteriormente, sabemos que \\(v_2 = \\mathrm{proy}_{U^{\\perp}}(v)\\in U^{\\perp}\\), lo cual se podría comprobar haciendo el producto escalar \\(\\langle v_2, u_i \\rangle\\) y viendo que su valor es 0 para todo \\(i\\). 5.5 Qué es y qué utilidad tiene una base ortonormal Un vector \\(v\\in V\\) se dice unitario si \\(\\|v\\| = 1\\). Es fácil crear vectores unitarios: dado \\(u\\in V\\), si definimos \\(v = \\frac{1}{\\|u\\|}u\\), este vector tiene norma 1. A este proceso se le llama normalizar. Un sistema de vectores \\(\\{v_1,\\ldots,v_k\\}\\) se llama sistema ortonormal si es un sistema ortogonal y cada \\(v_i\\) es un vector unitario (\\(i=1,\\ldots, k\\)). En relación a lo comentado de las propiedades de los sistemas ortogonales, en nuestro caso podemos decir que un sistema ortonormal \\(\\{v_1,\\ldots,v_n\\}\\), donde \\(\\mathrm{dim}(V) = n\\), es una base ortonormal. El interés de una base ortonormal es que es sencillo calcular las coordenadas de cualquier vector con respecto a dicha base: Sea \\(\\mathcal{B} = \\{v_1,\\ldots,v_n\\}\\) una base ortonormal del espacio vectorial \\(V\\). Entonces, para cada \\(v\\in V\\), tenemos: \\[v = \\langle v, v_1 \\rangle v_1 + \\ldots \\langle v, v_n \\rangle v_n\\] luego las coordenadas de \\(v\\) en la base \\(\\mathcal{B}\\) las podemos calcular como: \\[[v]_{\\mathcal{B}} = \\left( \\begin{array}{c} \\langle v, v_1 \\rangle \\\\ \\langle v, v_2 \\rangle \\\\ \\vdots \\\\ \\langle v, v_n \\rangle \\\\ \\end{array} \\right)_{\\mathcal{B}}\\] Ejemplo Consideremos ahora el sistema de vectores ortogonales del ejemplo anterior: \\[S = \\{s_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array} \\right) \\right\\}\\] A partir de \\(S\\), formemos un sistema ortonormal \\(S&#39;\\), donde cada vector de \\(S&#39;\\) es un vector de \\(S\\), que se ha normalizado. \\[S&#39; = \\left\\{\\left(\\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3} \\end{array}\\right), \\left(\\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3} \\end{array}\\right)\\right\\}\\] \\(S&#39;\\) forma una base ortonormal de \\(\\mathbb{R}^{4}\\). Si consideramos el vector \\[v = \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right) \\] podemos hallar sus coordenadas en la base \\(S&#39;\\), ya que cada coordenada de \\(v\\) será el producto escalar de \\(v\\) por el correspondiente vector de \\(S&#39;\\): \\[\\langle v, s&#39;_1 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array}\\right) \\rangle = \\frac{-2\\cdot\\sqrt{3}}{3}\\]\\[\\langle v, s&#39;_2 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0 \\end{array}\\right) \\rangle = \\frac{5\\cdot\\sqrt{6}}{6}\\]\\[\\langle v, s&#39;_3 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3} \\end{array}\\right) \\rangle = \\frac{\\sqrt{6}}{6}\\]\\[\\langle v, s&#39;_4 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3} \\end{array}\\right) \\rangle = \\frac{-\\sqrt{3}}{3}\\] Entonces, podremos escribir: \\[v = \\left(\\begin{array}{c} \\frac{-2\\cdot\\sqrt{3}}{3}\\\\ \\frac{5\\cdot\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{6}\\\\ \\frac{-\\sqrt{3}}{3} \\end{array}\\right)_{S&#39;}\\] 5.6 Cómo construimos una base ortonormal En las secciones anteriores hemos usado bases ortogonales y ortonormales, pero cabe preguntarnos si la existencia de tales bases está asegurada. La respuesta nos la da el siguiente resultado: Sea \\(V\\) un espacio vectorial euclídeo de dimensión finita. Entonces \\(V\\) tiene una base ortonormal. La demostración de este resultado teórico realmente nos presenta un método constructivo que permite hallar una base ortonormal del espacio \\(V\\) a partir de una base cualquiera. Método de Ortonormalización de Gram-Schmidt Ejemplo 5.7 Qué es una matriz ortogonal y una aplicación ortogonal Definición de matriz ortogonal Definición de aplicación ortogonal Ejemplo 5.8 Qué es la diagonalización ortogonal Definición de aplicación ortogonal y de matriz ortogonal Teorema de relación entre matriz ortogonal y base ortonormal Definición de matriz diagonalizable ortogonalmente Teorema diagonalización ortogonal Ejemplo 5.9 Qué matrices son diagonalizables ortogonalmente Teoremas que relacionan matrices simétricas con diagonabilidad ortogonal Procedimiento diagonalización ortogonal matriz simétrica Ejemplo "]
]
