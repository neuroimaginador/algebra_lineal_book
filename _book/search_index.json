[
["espacios-euclídeos.html", "5 Espacios Euclídeos 5.1 Qué son las normas y distancias 5.2 Qué significa el concepto de ortogonalidad 5.3 Qué es y qué utilidad tiene una base ortonormal 5.4 Cómo construimos una base ortonormal 5.5 Qué es el complemento ortogonal de un subespacio 5.6 Cómo calculamos la proyección de un vector sobre un subespacio 5.7 Qué es una matriz ortogonal y una aplicación ortogonal 5.8 Qué es la diagonalización ortogonal 5.9 Qué matrices son diagonalizables ortogonalmente", " 5 Espacios Euclídeos En este capítulo, damos un paso para acercarnos a los conceptos geométricos clásicos de distancias y ángulos, abstrayéndolos en el contexto de los espacios vectoriales. Para ello, precisaremos de la noción de producto escalar: Una aplicación \\(g:V\\times V\\to \\mathbb{R}\\), donde \\(V\\) es un espacio vectorial sobre \\(\\mathbb{R}\\), es un producto escalar si verifica: Es simétrica: \\(g(u, v) = g(v, u)\\) para todo \\(u,v\\in V\\). Es bilineal: \\(g(\\alpha u + \\beta v, w) = \\alpha g(u, w) + \\beta g(v, w)\\) para todos los \\(\\alpha,\\beta\\in\\mathbb{R}\\), \\(u,v,w\\in V\\). Por simetría, se tiene lo mismo para la linealidad en la segunda componente. Es definida positiva: \\(g(v, v) &gt; 0\\) para todo \\(v\\in V\\setminus\\{0\\}\\), \\(g(v, v) = 0\\) si, y sólo si \\(v = 0\\). En resumen, se suele definir un producto escalar como una forma bilineal, simétrica y definida positiva. Notación: El producto escalar de dos vectores \\(u\\) y \\(v\\) suele recibir diversas notaciones, aunque las más usadas son \\(u\\cdot v\\) y \\(\\langle u, v \\rangle\\). A un espacio vectorial \\(V\\) donde podemos definir un producto vectorial lo llamamos espacio euclídeo. Ejemplo En \\(\\mathbb{R}^n\\), disponemos de un producto escalar usual, de forma que si \\(u, v\\in\\mathbb{R}^n\\), entonces \\[\\langle u,v \\rangle = u_1v_1+u_2v_2+\\ldots+u_nv_n\\] siendo \\(u_i, v_i\\), \\(i=1,\\ldots,n\\) las coordenadas de \\(u\\) y de \\(v\\), respectivamente, en una base dada (la canónica como caso particular). A este producto vectorial es al que nos referiremos cuando usemos la notación \\(u\\cdot v\\). Podemos generalizar este producto escalar, definiendo unos pesos \\(\\alpha_i\\in\\mathbb{R}^{+}\\) para cada componente: \\[\\langle u,v \\rangle = \\alpha_1u_1v_1+\\alpha_2u_2v_2+\\ldots+\\alpha_nu_nv_n\\] Podemos construir productos escalares de la siguiente forma: Tomemos una matriz \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) simétrica y definida positiva. Entonces la aplicación \\(\\langle\\cdot,\\cdot\\rangle:\\mathbb{R}^n\\times\\mathbb{R}^n\\to\\mathbb{R}\\) definida por \\[\\langle u, v \\rangle = u^{\\text{t}}\\ A\\ v\\] es un producto escalar. Los dos ejemplos anteriores se correspondían con tomar como \\(A\\) la matriz identidad o una matriz diagonal con elementos positivos \\(\\alpha_i\\in\\mathbb{R}^{+}\\) en la diagonal. ¿Qué preguntas vamos a responder en este capítulo? ¿Qué son las normas y distancias? ¿Qué significa el concepto de ortogonalidad? ¿Qué es y qué utilidad tiene una base ortonormal? ¿Cómo construimos una base ortonormal? ¿Qué es el complemento ortogonal de un subespacio? ¿Cómo calculamos la proyección de un vector sobre un subespacio? ¿Qué es una matriz ortogonal y una aplicación ortogonal? ¿Qué es la diagonalización ortogonal? Qué matrices son diagonalizables ortogonalmente 5.1 Qué son las normas y distancias Una norma vectorial sobre un espacio \\(V\\) (sobre un cuerpo \\(\\mathbb{K}\\)) es un operador que sirve para medir la longitud de un vector (es decir, su distancia al origen), y se define formalmente como una aplicación \\(\\|\\cdot\\|:V\\to\\mathbb{R}\\) que verifica: Para todo \\(v\\in V\\), \\(\\|v\\| &gt; 0\\), con \\(\\|v\\| = 0\\) si, y solo si, \\(v = 0\\). Para todo \\(c\\in\\mathbb{K}\\), \\(v\\in V\\), se tiene \\(\\|cv\\| = |c|\\cdot \\|v\\|\\). Para todo \\(u,v\\in V\\), \\(\\|u+v\\|\\le\\|u\\|+\\|v\\|\\). Esta propiedad se llama desigualdad triangular (en un triángulo, la suma de la longitud de dos de sus lados – \\(\\|u\\|\\) y \\(\\|v\\|\\) – es siempre menor que la longitud de su otro lado – \\(\\|u+v\\|\\) – ). Cualquier operador \\(\\|\\cdot\\|\\) que verifique lo anterior es una norma vectorial. Sin embargo, podemos construir normas a partir de un producto escalar: Si \\(\\langle\\cdot,\\cdot\\rangle\\) es un producto escalar en el espacio \\(V\\), entonces la aplicación \\(\\|\\cdot\\|:V\\to\\mathbb{R}\\) dada por \\(\\|v\\| = \\sqrt{\\langle v, v\\rangle}\\) es una norma derivada del producto escalar sobre \\(V\\). Al definir la norma vectorial a partir de un producto escalar, además, tenemos la siguiente propiedad que será de gran interés en la siguiente sección: (Desigualdad de Cauchy-Schwarz) Si \\(\\|\\cdot\\|\\) es una norma derivada del producto escalar \\(\\langle\\cdot,\\cdot\\rangle\\), entonces \\[|\\langle u, v \\rangle| \\le \\|u\\|\\cdot\\|v\\|\\] para todo \\(u,v\\in V\\). Un concepto implícitamente relacionado con el de norma vectorial es el de distancia. Una distancia (también llamada métrica) es un operador \\(d:V\\times V\\to R\\) que verifica las siguientes propiedades: \\(d(u, v) = 0\\) si, y solo si, \\(u = v\\). Es simétrica: \\(d(u, v) = d(v, u)\\). Cumple la desigualdad triangular: \\(d(u, v) \\le d(u, w) + d(w, v)\\). Además, de aquí se deduce que \\(d(u, v) \\ge 0\\) para todo \\(u,v\\in V\\). Podemos deducir una distancia a partir de una norma, para así completar las relaciones existentes entre los conceptos aquí explicados: Si \\(\\|\\cdot\\|\\) es una norma sobre un espacio vectorial \\(V\\), entonces la aplicación \\(d:V\\times V\\to \\mathbb{R}\\) definida por \\[d(u, v) = \\|u-v\\|\\] para todo \\(u,v\\in V\\) es una distancia. Nota: Existen muchas más normas de las que se deducen a partir de un producto escalar, al igual que más distancias. Ejemplo Podemos deducir la expresión de la norma (a veces llamada módulo) de un vector \\(v\\in\\mathbb{R}^n\\). Si \\(v=\\left(\\begin{array}{c}v_1\\\\ v_2\\\\\\vdots\\\\ v_n\\end{array}\\right)\\), entonces \\[\\|v\\| = \\sqrt{v\\cdot v} = \\sqrt{v_1^2+v_2^2+\\ldots+v_n^2}\\] Y así, además, deducimos la expresión para la distancia euclídea usual en \\(\\mathbb{R}^n\\): \\[d(u,v) = \\|u - v\\| = \\sqrt{(u_1-v_1)^2+\\ldots+(u_n-v_n)^2}\\] Como ejemplo, podemos calcular la norma del vector \\[v = \\left(\\begin{array}{c} -2\\\\ 1\\\\ 1\\\\ 1 \\end{array}\\right) \\]\\[\\|v\\| = \\sqrt{(-2)^2+1^2+1^2+1^2} = \\sqrt{7}\\] 5.2 Qué significa el concepto de ortogonalidad Si partimos de la desigualdad de Cauchy-Schwarz que hemos visto antes, podemos llegar a que \\[-1 \\le \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v|} \\le 1\\] para todo \\(u,v\\in V\\). Por otro lado, la función coseno restringida a \\([0, \\pi]\\), es decir, \\(\\cos:[0,\\pi]\\to[-1,1]\\), es biyectiva. Eso quiere decir que existe un único \\(\\theta\\in[0, \\pi]\\) tal que \\(\\cos\\theta = \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v|}\\). Llamamos entonces ángulo entre los vectores \\(u,v\\in V\\) al único \\(\\theta\\in[0,\\pi]\\) tal que \\(\\cos\\theta = \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v|}\\). Diremos que dos vectores \\(u\\) y \\(v\\) de \\(V\\) son ortogonales (o perpendiculares) si el ángulo que forman es \\(\\frac{\\pi}{2}\\), es decir, si \\(\\langle u, v \\rangle = 0\\). La noción de ortogonalidad en el espacio euclídeo se corresponde con la perpendicularidad de vectores que todos tenemos de forma intuitiva. Lo podemos extender también a ortogonalidad con respecto a un espacio vectorial: Un vector \\(v\\in V\\) es ortogonal al subespacio \\(U\\) de \\(V\\) si, y sólo si, es ortogonal a todos y cada uno de los vectores de \\(U\\), es decir, \\(\\langle v, u \\rangle = 0\\) para todo \\(u\\in U\\). Sea \\(U\\) un subespacio de \\(V\\), y sea \\(\\mathcal{B} = \\{u_1,\\ldots,u_k\\}\\) una base de \\(U\\). Un vector \\(v\\in V\\) es ortogonal a \\(U\\) si, y solo si, es ortogonal a cada \\(u_i\\): \\(\\langle v, u_i \\rangle = 0\\) para todo \\(i=1,\\ldots,k\\). Un sistema ortogonal es un conjunto de vectores \\(\\{v_1,\\ldots,v_m\\}\\subset V\\) donde los \\(v_i\\) son ortogonales dos a dos, es decir, \\(\\langle v_i, v_j \\rangle = 0\\) para todo \\(i\\ne j\\). Algunas propiedades y caracterizaciones de los vectores y sistemas ortogonales Dos vectores \\(u\\) y \\(w\\) son ortogonales si, y solo si, \\(\\|u+w\\|^2 = \\|u\\|^2 + \\|v\\|^2\\) (generalización del Teorema de Pitágoras). Si un sistema de vectores es ortogonal, entonces es linealmente independiente. Por tanto, en un espacio \\(V\\) de dimensión finita \\(n\\), cualquier conjunto de \\(n\\) vectores ortogonales entre sí es una base. Este último resultado es interesante, pues nos constata que cualquier sistema ortogonal con tantos vectores como la dimensión del espacio vectorial es una base. Ejemplo Consideremos el espacio \\(U\\) dado por: \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} -x+ 4y+ z &amp; = &amp; 0\\\\ y+ t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] y los vectores \\[v = \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) \\quad\\quad w = \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ -1 \\end{array}\\right) \\] Calculemos el ángulo entre \\(v\\) y \\(w\\): \\[\\theta = \\arccos \\frac{v\\cdot w}{\\|v\\|\\cdot\\|w\\|}\\]\\[v\\cdot w = 1\\cdot(-1)+(-2)\\cdot2+(-1)\\cdot(-1)+2\\cdot(-1) = -6\\]\\[\\|v\\| = \\sqrt{1^2+(-2)^2+(-1)^2+2^2} = \\sqrt{10}\\]\\[\\|w\\| = \\sqrt{(-1)^2+2^2+(-1)^2+(-1)^2} = \\sqrt{7}\\] Luego \\[\\theta = \\arccos \\frac{-6}{\\sqrt{10}\\sqrt{7}} = \\arccos \\frac{-3\\cdot\\sqrt{70}}{35}\\] Vamos a comprobar ahora que \\(v\\) es ortogonal al subespacio \\(U\\). Para ello, a partir de las ecuaciones cartesianas de \\(U\\), vamos a hallar una base suya, resolviendo el sistema por Gauss-Jordan y pasando por las ecuaciones paramétricas: \\[\\left( \\begin{array}{cccc@{}} -1 &amp; 4 &amp; 1 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc@{}} 1 &amp; 0 &amp; -1 &amp; 4\\\\ 0 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De aquí que la base de \\(U\\) sea: \\[\\mathcal{B}_U = \\left\\{u_i\\right\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Como hemos visto antes, para comprobar que \\(v\\) es ortogonal a \\(U\\), nos basta con comprobar que lo es a cada vector de \\(\\mathcal{B}_U\\), hallando su producto escalar: \\[\\langle v, u_1 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) \\rangle = 1\\cdot1+(-2)\\cdot0+(-1)\\cdot1+2\\cdot0 = 0\\]\\[\\langle v, u_2 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-4)+(-2)\\cdot(-1)+(-1)\\cdot0+2\\cdot1 = 0\\] Consideremos ahora el siguiente sistema de vectores: \\[S = \\{s_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array} \\right) \\right\\}\\] Podemos comprobar fácilmente que es un sistema ortogonal, hallando los productos escalares de los vectores de \\(S\\) dos a dos: \\[\\langle s_1, s_2 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) \\rangle = 1\\cdot(-1)+1\\cdot2+1\\cdot(-1)+0\\cdot0 = 0\\]\\[\\langle s_1, s_3 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) \\rangle = 1\\cdot1+1\\cdot0+1\\cdot(-1)+0\\cdot2 = 0\\]\\[\\langle s_1, s_4 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-1)+1\\cdot0+1\\cdot1+0\\cdot1 = 0\\]\\[\\langle s_2, s_3 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) \\rangle = (-1)\\cdot1+2\\cdot0+(-1)\\cdot(-1)+0\\cdot2 = 0\\]\\[\\langle s_2, s_4 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = (-1)\\cdot(-1)+2\\cdot0+(-1)\\cdot1+0\\cdot1 = 0\\]\\[\\langle s_3, s_4 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-1)+0\\cdot0+(-1)\\cdot1+2\\cdot1 = 0\\] Tenemos, por tanto, un sistema de 4 vectores ortogonales en \\(\\mathbb{R}^{4}\\), y, como hemos comentado antes, son linealmente independientes, luego forman una base del espacio vectorial. 5.3 Qué es y qué utilidad tiene una base ortonormal Un vector \\(v\\in V\\) se dice unitario si \\(\\|v\\| = 1\\). Es fácil crear vectores unitarios: dado \\(u\\in V\\), si definimos \\(v = \\frac{1}{\\|u\\|}u\\), este vector tiene norma 1. A este proceso se le llama normalizar. Un sistema de vectores \\(\\{v_1,\\ldots,v_k\\}\\) se llama sistema ortonormal si es un sistema ortogonal y cada \\(v_i\\) es un vector unitario (\\(i=1,\\ldots, k\\)). En relación a lo comentado de las propiedades de los sistemas ortogonales, en nuestro caso podemos decir que un sistema ortonormal \\(\\{v_1,\\ldots,v_n\\}\\), donde \\(\\mathrm{dim}(V) = n\\), es una base ortonormal. El interés de una base ortonormal es que es sencillo calcular las coordenadas de cualquier vector con respecto a dicha base: Sea \\(\\mathcal{B} = \\{v_1,\\ldots,v_n\\}\\) una base ortonormal del espacio vectorial \\(V\\). Entonces, para cada \\(v\\in V\\), tenemos: \\[v = \\langle v, v_1 \\rangle v_1 + \\ldots \\langle v, v_n \\rangle v_n\\] luego las coordenadas de \\(v\\) en la base \\(\\mathcal{B}\\) las podemos calcular como: \\[[v]_{\\mathcal{B}} = \\left( \\begin{array}{c} \\langle v, v_1 \\rangle \\\\ \\langle v, v_2 \\rangle \\\\ \\vdots \\\\ \\langle v, v_n \\rangle \\\\ \\end{array} \\right)_{\\mathcal{B}}\\] Ejemplo Consideremos ahora el sistema de vectores ortogonales del ejemplo anterior: \\[S = \\{s_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array} \\right) \\right\\}\\] A partir de \\(S\\), formemos un sistema ortonormal \\(S&#39;\\), donde cada vector de \\(S&#39;\\) es un vector de \\(S\\), que se ha normalizado. \\[S&#39; = \\left\\{\\left(\\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3} \\end{array}\\right), \\left(\\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3} \\end{array}\\right)\\right\\}\\] \\(S&#39;\\) forma una base ortonormal de \\(\\mathbb{R}^{4}\\). Si consideramos el vector \\[v = \\left( \\begin{array}{c@{}} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array} \\right) \\] podemos hallar sus coordenadas en la base \\(S&#39;\\), ya que cada coordenada de \\(v\\) será el producto escalar de \\(v\\) por el correspondiente vector de \\(S&#39;\\): \\[\\langle v, s&#39;_1 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array}\\right) \\rangle = \\frac{-2\\cdot\\sqrt{3}}{3}\\]\\[\\langle v, s&#39;_2 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0 \\end{array}\\right) \\rangle = \\frac{-2\\cdot\\sqrt{6}}{3}\\]\\[\\langle v, s&#39;_3 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3} \\end{array}\\right) \\rangle = \\sqrt{6}\\]\\[\\langle v, s&#39;_4 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right), \\left(\\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3} \\end{array}\\right) \\rangle = 0\\] Entonces, podremos escribir: \\[v = \\left(\\begin{array}{c} \\frac{-2\\cdot\\sqrt{3}}{3}\\\\ \\frac{-2\\cdot\\sqrt{6}}{3}\\\\ \\sqrt{6}\\\\ 0 \\end{array}\\right)_{S&#39;}\\] 5.4 Cómo construimos una base ortonormal Teorema de existencia Gram-Schmidt Ejemplo 5.5 Qué es el complemento ortogonal de un subespacio Definición de complemento ortogonal Método de cálculo Ejemplo 5.6 Cómo calculamos la proyección de un vector sobre un subespacio Definición de proyección ortogonal sobre otro vector Propiedades Definición de proyección sobre un subespacio Teorema de descomposición ortogonal Ejemplo 5.7 Qué es una matriz ortogonal y una aplicación ortogonal 5.8 Qué es la diagonalización ortogonal Definición de aplicación ortogonal y de matriz ortogonal Teorema de relación entre matriz ortogonal y base ortonormal Definición de matriz diagonalizable ortogonalmente Teorema diagonalización ortogonal Ejemplo 5.9 Qué matrices son diagonalizables ortogonalmente Teoremas que relacionan matrices simétricas con diagonabilidad ortogonal Procedimiento diagonalización ortogonal matriz simétrica Ejemplo "]
]
