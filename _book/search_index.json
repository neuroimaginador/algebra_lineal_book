[
["index.html", "Preguntas y respuestas de Álgebra Lineal 1 Introducción", " Preguntas y respuestas de Álgebra Lineal Domingo López 2020-05-23 1 Introducción En este libro vamos a recopilar algunas de las preguntas y dudas más frecuentes en los temas de Álgebra Lineal, con ejemplos resueltos, y acompañados de la teoría necesaria para comprender los resultados. No pretende ser un libro de teoría exhaustivo, con demostraciones de teoremas, sino que pretende complementar a ese tipo de manuales. Cada capítulo incluye preguntas y sus respuestas acerca de aquellos procedimientos más usuales relacionados con el tema correspondiente. La idea de uso de este libro es que se pueda navegar libremente, yendo a aquellas preguntas sobre las que se tenga dudas, ya que en su respuesta se incluirán enlaces a todos los contenidos y procedimientos necesarios para poder resolver dicha cuestión. Además, incluimos un último capítulo con problemas resueltos, siguiendo las indicaciones dadas en los capítulos correspondientes. "],
["ev.html", "2 Espacios Vectoriales 2.1 Qué es un subespacio vectorial 2.2 Qué es un sistema de vectores linealmente independiente 2.3 Qué es un sistema generador y el subespacio generado por un conjunto de vectores 2.4 Cómo extraemos una base a partir de un sistema generador de un (sub)espacio vectorial 2.5 Cómo encontrar la base (y la dimensión) para un subespacio vectorial 2.6 Cómo convertir entre ecuaciones cartesianas y paramétricas de un subespacio vectorial 2.7 Cómo hallar la intersección de dos subespacios vectoriales 2.8 ¿Es la unión de subespacios un subespacio? 2.9 Cómo hallar la suma de dos subespacios vectoriales 2.10 Qué dice el teorema de la dimensión 2.11 Cómo calcular el subespacio suplementario de uno dado 2.12 Cómo calcular las coordenadas de un vector con respecto a una base dada 2.13 Cómo calcular la matriz de cambio de base entre dos bases de un mismo espacio vectorial", " 2 Espacios Vectoriales Supongamos un cuerpo \\(\\mathbb{K}\\) (generalmente \\(\\mathbb{K} = \\mathbb{R}\\)). Consideremos un conjunto \\(V\\) dotado de dos operaciones: Una operación interna, la suma, de forma que si \\(u,v\\in V\\) entonces su suma es \\(u+v\\in V\\). Una operación externa, producto por un escalar de \\(\\mathbb{K}\\): si \\(c\\in \\mathbb{K}\\) y \\(v\\in V\\), su producto es \\(c \\cdot v \\in V\\). Si: \\(V\\) con la operación \\(+\\) es grupo abeliano. El producto por un escalar verifica las propiedades distributiva (\\((c+d)\\cdot v = c\\cdot v + d\\cdot v\\) y \\(c\\cdot (u+v) = c\\cdot u + c\\cdot v\\), para \\(c,d\\in\\mathbb{K},\\,u,v\\in V\\)), pseudoasociativa (\\(c\\cdot(d\\cdot v) = (c\\cdot d)\\cdot v\\) para \\(c,d\\in\\mathbb{K},\\,v\\in V\\)) y existencia de neutro (\\(1\\in\\mathbb{K}\\), tal que \\(1\\cdot v = v\\) para todo \\(v\\in V\\)). Entonces a \\((V, +, \\cdot)\\) se de denomina espacio vectorial, y a los elementos de \\(V\\), vectores. En este documento, nos referiremos normalmente a resultados sobre espacios vectoriales denominados de dimensión finita. Este concepto lo hablaremos más adelante. Ejemplo \\(V=\\mathbb{R}^{4}\\) es un espacio vectorial y \\(\\left( \\begin{array}{c@{}} 2\\\\ 1\\\\ -2\\\\ -1 \\end{array} \\right)\\) es un vector de \\(V\\). ¿Qué preguntas vamos a responder en este capítulo? ¿Qué es un subespacio vectorial? ¿Qué es un sistema de vectores linealmente independiente? ¿Qué es un sistema generador y el subespacio generado por un conjunto de vectores? ¿Cómo extraemos una base a partir de un sistema generador de un (sub)espacio vectorial? ¿Cómo encontrar la base (y la dimensión) para un subespacio vectorial? ¿Cómo convertir entre ecuaciones cartesianas y paramétricas de un subespacio vectorial? ¿Cómo hallar la intersección de dos subespacios vectoriales? ¿Es la unión de subespacios un subespacio? ¿Cómo hallar la suma de dos subespacios vectoriales? ¿Qué dice el teorema de la dimensión? ¿Cómo calcular el subespacio suplementario de uno dado? ¿Cómo calcular las coordenadas de un vector con respecto a una base dada? ¿Cómo calcular la matriz de cambio de base entre dos bases de un mismo espacio vectorial? 2.1 Qué es un subespacio vectorial Un subconjunto de \\(U\\subset V\\) es un subespacio vectorial de \\(V\\) si la restricción de las operaciones suma y producto por un escalar a ese conjunto son operaciones cerradas. Eso equivale a decir que la suma de dos vectores cualesquiera de \\(U\\) debe estar en \\(U\\), y si multiplicamos por un escalar un vector de \\(U\\), el producto vuelve a estar en \\(U\\). Ejemplo \\(U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}: x + y - z = 0\\right\\}\\) es un subespacio de \\(V=\\mathbb{R}^{4}\\). 2.2 Qué es un sistema de vectores linealmente independiente Supongamos un conjunto de vectores \\(\\{v_1,\\ldots,v_n\\}\\). Este conjunto es linealmente independiente si ninguno de los vectores se puede poner como combinación lineal de los demás vectores. Esto es equivalente a que si tenemos una combinación lineal igualada a 0, de la forma \\[\\alpha_1 v_1 + \\ldots + \\alpha_n v_n = 0\\] entonces el sistema de los \\(v_i\\) será independiente si necesariamente todos los \\(\\alpha_i\\) valen 0. Evidentemente, cualquier sistema de vectores que incluya al vector 0 no es linealmente independiente. Ejemplo Consideremos el sistema de vectores \\(\\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 1\\\\ -2\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 4\\\\ 3\\\\ -8\\\\ -3 \\end{array} \\right) \\right\\}\\). ¿Cómo podemos saber si es linealmente independiente? Comenzamos por ver que no está el vector 0. Estudiemos entonces qué pasa si escribimos una combinación lineal de los tres vectores igualada a cero: \\[\\alpha_{1}\\left(\\begin{array}{c} 2\\\\ 1\\\\ -2\\\\ -1 \\end{array}\\right) {}+\\alpha_{2}\\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\alpha_{3}\\left(\\begin{array}{c} 4\\\\ 3\\\\ -8\\\\ -3 \\end{array}\\right) {} = \\left(\\begin{array}{c} 0\\\\ 0\\\\ 0\\\\ 0 \\end{array}\\right) \\] Esto lo podemos reescribir como un sistema de ecuaciones lineales donde las incógnitas son las \\(\\alpha_i\\). \\[\\left\\{\\begin{array}{rrrcr} 2\\alpha_{1} &amp; + \\alpha_{2} &amp; + 4\\alpha_{3} &amp; = &amp; 0\\\\ \\alpha_{1} &amp; &amp; + 3\\alpha_{3} &amp; = &amp; 0\\\\ -2\\alpha_{1} &amp; + \\alpha_{2} &amp; -8\\alpha_{3} &amp; = &amp; 0\\\\ -\\alpha_{1} &amp; &amp; -3\\alpha_{3} &amp; = &amp; 0\\\\ \\end{array}\\right.\\] Fijaos en que la matriz de coeficientes está formada por los vectores puestos por columnas. Pueden ocurrir dos casos: Si el sistema es compatible determinado, entonces su única solución es que todos los \\(\\alpha_i = 0\\). Esto, por el comentario anterior, nos dice que el sistema será linealmente independiente. Si el sistema es compatible indeterminado, entonces admite más soluciones aparte de la trivial. Por tanto, hay una combinación lineal, donde no todos los \\(\\alpha_i\\) son nulos, y que da igual al vector 0. Eso nos indica que el sistema no es linealmente independiente. Lo más sencillo en este caso es usar el método de Gauss para encontrar la forma escalonada de la matriz de coeficientes. Del estudio de sistemas homogéneos, sabemos el siguiente resultado, que se aplica tras tener la forma escalonada: Si el número de ecuaciones (filas) es menor estrictamente que el número de incógnitas (columnas), entonces el sistema es compatible indeterminado. En otro caso (mismo número de ecuaciones que de incógnitas), será compatible determinado. Usamos entonces Gauss: \\[\\left( \\begin{array}{ccc|c} 2 &amp; 1 &amp; 4 &amp; 0\\\\ 1 &amp; 0 &amp; 3 &amp; 0\\\\ -2 &amp; 1 &amp; -8 &amp; 0\\\\ -1 &amp; 0 &amp; -3 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{ccc|c} 2 &amp; 1 &amp; 4 &amp; 0\\\\ 0 &amp; - \\frac{1}{2} &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\] En este caso, tenemos 2 filas no nulas y 3 incógnitas. Por lo anterior, el sistema es compatible indeterminado, tiene infinitas soluciones, y eso implica que el sistema no es linealmente independiente. Si hubiéramos llegado a 3 ecuaciones no nulas, entonces estaríamos en la situación de un sistema compatible determinado, luego la única solución es la nula. Por lo anterior, el sistema sería linealmente independiente. 2.3 Qué es un sistema generador y el subespacio generado por un conjunto de vectores Consideramos un sistema de vectores \\(\\mathcal{S} = \\{v_1,\\ldots,v_n\\}\\). El subespacio generado por \\(\\mathcal{S}\\) es el conjunto de vectores que se calcula como combinaciones lineales de los de \\(\\mathcal{S}\\). Es decir, si yo voy dando valores a unos escalares \\(\\alpha_1,\\ldots,\\alpha_n\\), entonces el vector \\(\\alpha_1v_1+\\ldots+\\alpha_nv_n\\) es combinación lineal de los de \\(\\mathcal{S}\\), luego pertenece al subespacio generado por éste. Como conclusión, el subespacio generado por \\(\\mathcal{S}\\) es el menor subespacio de \\(V\\) que contiene a los elementos de \\(\\mathcal{S}\\). Denotamos como \\(\\mathcal{L}(\\mathcal{S})\\) al subsepacio generado por \\(\\mathcal{S}\\). Ejemplo Consideremos el sistema de vectores \\(\\mathcal{S} = \\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 1\\\\ -2\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) \\right\\}\\). Entonces \\[\\mathcal{L}(\\mathcal{S}) = \\left\\{\\alpha_{1}\\left(\\begin{array}{c} 2\\\\ 1\\\\ -2\\\\ -1 \\end{array}\\right) {}+\\alpha_{2}\\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {}:\\alpha_i\\in\\mathbb{R}\\right\\}\\] Dando valores a \\(\\alpha_1\\) y \\(\\alpha_2\\) tendremos todos los vectores generados en \\(\\mathcal{L}(\\mathcal{S})\\). Por ejemplo, para \\[\\alpha_1 = -1,\\quad\\alpha_2 = 2\\quad\\Rightarrow\\quad v = -1\\left(\\begin{array}{c} 2\\\\ 1\\\\ -2\\\\ -1 \\end{array}\\right) {}+2\\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {} = \\left(\\begin{array}{c} 0\\\\ -1\\\\ 4\\\\ 1 \\end{array}\\right) \\in \\mathcal{L}(\\mathcal{S})\\] 2.4 Cómo extraemos una base a partir de un sistema generador de un (sub)espacio vectorial Para cualquier espacio vectorial, o subespacio, una base es un sistema de vectores que lo genera (mediante combinaciones lineales, como hemos visto antes) y que es linealmente independiente. La base canónica \\(\\mathcal{C}\\) en un espacio \\(\\mathbb{R}^n\\) es la dada por los vectores \\(e_i\\) (\\(i = 1,\\ldots,n\\)), donde \\(e_i\\) es el vector que tiene todas sus componentes a 0 menos la \\(i\\)-ésima que vale 1. Si tenemos un sistema generador de un (sub)espacio vectorial, siempre podremos encontrar una base, gracias al siguiente resultado: Todo sistema generador de un subespacio contiene un sistema linealmente independiente, que también genera el mismo subespacio. El concepto de dimensión está intrínsecamente relacionado con el de base: es el número de vectores en cualquier base del espacio. Hay que tener en cuenta que todas las bases que se puedan encontrar en un mismo espacio vectorial tienen el mismo número de elementos. Aparte, hay dos propiedades más que relacionan las bases y las propiedades de ser sistema generador o linealmente independiente. Si \\(n\\) es la dimensión de un espacio vectorial: Todo sistema de vectores con más de \\(n\\) elementos no puede ser linealmente independiente. Todo sistema generador de ese espacio debe tener, al menos, \\(n\\) elementos. Dado un sistema de vectores \\(S\\) que se sepa que es sistema generador de un espacio o de un subespacio, la mejor estrategia es utilizar transformaciones elementales sobre \\(S\\) para eliminar aquellos vectores que sean combinaciones lineales de los demás. Ejemplo Consideremos el sistema de vectores \\[\\mathcal{S} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ -2\\\\ -1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -5\\\\ -1\\\\ -3\\\\ 0 \\end{array} \\right) \\right\\}\\] Vamos a encontrar un sistema de vectores que sea base de \\(\\mathcal{L}(S)\\). Para ellos, vamos a eliminar de \\(S\\) aquellos vectores que sean combinaciones lineales de los demás. Esa tarea es justo la que realiza el método de Gauss con las filas de una matriz, al ponerla en forma escalonada. Si aplicamos Gauss a una matriz, las filas que acaban siendo de ceros son las que originalmente eran combinación lineal de las demás. Por tanto, nos aprovechamos de esta propiedad para eliminar vectores linealmente dependientes. Vamos a construir la matriz \\(M\\) resultado de poner los vectores de \\(S\\) por filas (primer vector en primera fila, segundo en segunda fila, etc.). \\[M = \\left( \\begin{array}{cccc@{}} -1 &amp; 2 &amp; -1 &amp; -1\\\\ -2 &amp; -1 &amp; -1 &amp; -1\\\\ -2 &amp; -2 &amp; -1 &amp; 2\\\\ -5 &amp; -1 &amp; -3 &amp; 0 \\end{array} \\right) \\] Si aplicamos Gauss para encontrar la forma escalonada (omitimos los detalles intermedios): \\[\\left( \\begin{array}{cccc@{}} -1 &amp; 2 &amp; -1 &amp; -1\\\\ -2 &amp; -1 &amp; -1 &amp; -1\\\\ -2 &amp; -2 &amp; -1 &amp; 2\\\\ -5 &amp; -1 &amp; -3 &amp; 0 \\end{array} \\right) \\sim \\left( \\begin{array}{cccc@{}} -1 &amp; 2 &amp; -1 &amp; -1\\\\ 0 &amp; -5 &amp; 1 &amp; 1\\\\ 0 &amp; 0 &amp; - \\frac{1}{5} &amp; \\frac{14}{5}\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\] Como se puede ver, tenemos que la última fila es de ceros. Por tanto, el vector original del que procede esa fila es linealmente dependiente del resto de vectores de \\(S\\). Si hubiéramos tenido que intercambiar filas en algún momento, habría que llevar cuenta de cada intercambio para saber a qué vector original se corresponden aquellas filas que valen 0. Por tanto, la base del subespacio generado por \\(S\\), que consiste en únicamente los vectores linealmente independientes de \\(S\\), es: \\[\\mathcal{B}_{\\mathcal{L}(\\mathcal{S})} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ -2\\\\ -1\\\\ 2 \\end{array} \\right) \\right\\}\\] 2.5 Cómo encontrar la base (y la dimensión) para un subespacio vectorial En muchas ocasiones, necesitaremos encontrar una base para un subespacio vectorial, puesto que con las bases tenemos mucha información adicional sobre un subespacio. Además, cada subespacio queda completamente caracterizado una vez dada su base. Sin embargo, existen otras formas en las que podemos definir un subespacio vectorial, y necesitamos conocer cómo extraer una base de dicho subespacio, para tener una forma estándar de describir los subespacios. 2.5.1 Partiendo de las ecuaciones paramétricas La primera forma escrita en que podemos definir un subespacio vectorial es mediante sus ecuaciones paramétricas, que nos indican cómo determinar las diversas componentes de cada vector (\\(x,y,\\ldots\\)) a partir de unos parámetros libres \\(\\alpha_1,\\ldots,\\alpha_n\\). Realmente esto es equivalente a escribir un vector genérico \\(v\\) como combinación lineal de otros vectores. Esos otros vectores que forman la combinación lineal son, por tanto, un sistema generador del subespacio que queremos definir. Usando lo explicado en la sección correspondiente, obtendríamos la base deseada. Ejemplo Consideremos el subespacio \\(U\\) dado en su forma paramétrica: \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in V:\\quad\\begin{array}{c@{}cc} x &amp; = &amp; -\\alpha -2\\beta -2\\gamma -5\\delta\\\\ y &amp; = &amp; 2\\alpha -\\beta -2\\gamma -\\delta\\\\ z &amp; = &amp; -\\alpha -\\beta -\\gamma -3\\delta\\\\ t &amp; = &amp; -\\alpha -\\beta + 2\\gamma \\end{array} ,\\alpha,\\beta,\\gamma,\\delta\\in\\mathbb{R}\\right\\}\\] En este caso, lo que debemos hacer es expresar esas ecuaciones paramétricas en forma de combinación lineal de vectores. Para ello, consideramos los coeficientes que acompañan a cada uno de los parámetros \\(\\alpha,\\beta,\\gamma,\\delta\\). Si lo hacemos, nos queda (ya poniéndolo en formato vectorial) que un vector \\(v\\) pertenece a \\(U\\) si se puede expresar como una combinación lineal de la forma: \\[v = \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ -1 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} -2\\\\ -1\\\\ -1\\\\ -1 \\end{array}\\right) {}+\\gamma\\left(\\begin{array}{c} -2\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) {}+\\delta\\left(\\begin{array}{c} -5\\\\ -1\\\\ -3\\\\ 0 \\end{array}\\right) {}\\] Es decir, \\(\\mathcal{S} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ -2\\\\ -1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -5\\\\ -1\\\\ -3\\\\ 0 \\end{array} \\right) \\right\\}\\) es un sistema generador del subespacio \\(U\\). Este proceso no nos asegura que sean vectores linealmente independientes, así que no podemos decir, de momento, que sea una base de \\(U\\). A partir de aquí, basta aplicar lo descrito en la sección sobre cómo extraer una base a partir de un sistema generador. 2.5.2 Partiendo de las ecuaciones cartesianas Un subespacio vectorial puede venir definido mediante unas expresiones cartesianas que relacionan las componentes \\(x,y,\\ldots\\) entre sí. Esas expresiones cartesianas realmente expresan un sistema de ecuaciones homogéneo cuyo conjunto solución es el propio subespacio que buscamos. Por tanto, la forma de proceder es tomar las ecuaciones cartesianas, formar el sistema, y resolverlo. Para ello, haciendo Gauss, pasamos a forma escalonada o escalonada reducida y desde esa forma, calculamos las expresiones paramétricas del conjunto solución y aplicamos lo explicado en la sección anterior. Ejemplo Sea el subespacio \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} 3x-y+ t &amp; = &amp; 0\\\\ 3x+ y-t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] El sistema homogéneo dado por su ecuaciones cartesianas es: \\[\\left\\{\\begin{array}{rrrrcr} 3x &amp; -y &amp; &amp; + t &amp; = &amp; 0\\\\ 3x &amp; + y &amp; &amp; -t &amp; = &amp; 0\\\\ \\end{array}\\right.\\] Este sistema, resuelto por Gauss, nos da las siguientes expresiones paramétricas: \\[\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 0\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} 0\\\\ 1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] A partir de aquí, ya sabemos que estos dos vectores son un sistema generador de \\(U\\), luego podemos encontrar una base para \\(U\\) si seguimos las explicaciones en la sección correspondiente. Podemos fijarnos que, en este ejemplo, ya los dos vectores son linealmente independientes, así que ya conforman una base, pero no es la situación general y conviene hacer, para estar seguros, lo mencionado en el párrafo anterior. Luego, además, podemos saber que la dimensión de \\(U\\) es \\(\\mathrm{dim}(U) = 2\\). 2.6 Cómo convertir entre ecuaciones cartesianas y paramétricas de un subespacio vectorial En la sección anterior, ya hemos visto que para pasar de ecuaciones cartesianas a paramétricas, basta con usar el método de Gauss-Jordan para resolver el sistema de ecuaciones homogéneo. Para pasar de ecuaciones paramétricas de un subespacio a las ecuaciones cartesianas, se suele utilizar el método de eliminación de parámetros. Si tenemos \\(m\\) parámetros \\(\\alpha_i\\) (\\(i = 1,\\ldots,m\\)) y el espacio es de dimensión \\(n &gt; m\\), entonces tendremos, a lo sumo, \\(n - m\\) ecuaciones cartesianas. Buscamos un sistema de ecuaciones lineales homogéneo (las ecuaciones cartesianas) cuya solución, en forma paramétrica sea las ya dadas. Para ello, comenzamos con la expresión paramétrica con una ecuación por cada coordenada cartesiana del subespacio (una expresión de \\(x\\) en función de los \\(\\alpha_i\\), una para la \\(y\\), etc.). Se toma uno de los parámetros, y se despeja de alguna de las ecuaciones en las que aparece. Se sutituye en las demás ecuaciones y desechamos la ecuación de la que hemos despejado el parámetro. En esta situación, tenemos una ecuación menos y un parámetro menos. Con el nuevo sistema de ecuaciones, repetimos el proceso anterior: seleccionar un parámetro, despejarlo de una ecuación y sustituirlo en las demás, eliminando la ecuación de la que se ha despejado. Repetimos todo el proceso hasta que no queden parámetros que eliminar. Ejemplo Consideramos el subespacio \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in V:\\quad\\begin{array}{c@{}cc} x &amp; = &amp; -\\alpha -2\\beta -2\\gamma -5\\delta\\\\ y &amp; = &amp; 2\\alpha -\\beta -2\\gamma -\\delta\\\\ z &amp; = &amp; -\\alpha -\\beta -\\gamma -3\\delta\\\\ t &amp; = &amp; -\\alpha -\\beta + 2\\gamma \\end{array} ,\\alpha,\\beta,\\gamma,\\delta\\in\\mathbb{R}\\right\\}\\] Vamos a aplicar el método de eliminación de los parámetros: Despejamos \\(\\alpha\\) de la ecuación 1, y sustituimos en las demás, lo cual nos da: \\[\\begin{array}{c@{}cc} 2x + y &amp; = &amp; -5\\beta -6\\gamma -11\\delta\\\\ -x + z &amp; = &amp; \\beta + \\gamma + 2\\delta\\\\ -x + t &amp; = &amp; \\beta + 4\\gamma + 5\\delta \\end{array} \\] Despejamos \\(\\beta\\) de la ecuación 1, y sustituimos en las demás, lo cual nos da: \\[\\begin{array}{c@{}cc} -3/5x + 1/5y + z &amp; = &amp; -1/5\\gamma -1/5\\delta\\\\ -3/5x + 1/5y + t &amp; = &amp; 14/5\\gamma + 14/5\\delta \\end{array} \\] Por último, despejando \\(\\gamma\\) de la ecuación 1, nos queda:\\[\\begin{array}{rrrrcr} -9x &amp; + 3y &amp; + 14z &amp; + t &amp; = &amp; 0\\\\ \\end{array}\\] que es la expresión en cartesianas del espacio \\(U\\). 2.7 Cómo hallar la intersección de dos subespacios vectoriales Si tenemos dos subespacios, \\(U\\) y \\(W\\) de \\(V\\), podemos calcular su intersección, que es el conjunto de vectores que pertenecen a ambos subespacios a la vez. La intersección de dos subespacios es un nuevo subespacio, aunque puede suceder que la intersección sea únicamente el vector 0. Conviene tener ambos subespacios expresados en forma de ecuaciones cartesianas. Si no estuviera así (porque solo se dé su base o, lo que es equivalente, las ecuaciones paramétricas), basta ver la sección sobre cómo convertir de ecuaciones paramétricas a cartesianas. Si un vector \\(v\\) pertenece a la intersección, entonces verifica tanto las ecuaciones paramétricas de \\(U\\) como las de \\(W\\). Por tanto, podemos construir un sistema de ecuaciones con todas las ecuaciones de \\(U\\) y de \\(W\\), y la intersección \\(U\\cap W\\) será el conjunto solución de este sistema homogéneo ampliado. A partir de ese sistema, ya hemos explicado cómo encontrar la base del subespacio intersección. Ejemplo Consideremos los subespacios siguientes: \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} 3x-y+ t &amp; = &amp; 0\\\\ 3x+ y-t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\]\\[W = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} 3x+ z+ 3t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] El subespacio \\(U\\) ya lo hemos estudiado antes. Para calcular la intersección \\(U\\cap W\\), construimos un sistema de ecuaciones con todas las ecuaciones de \\(U\\) y las de \\(W\\). La intersección es el conjunto solución del sistema homogéneo resultante: \\[\\left\\{\\begin{array}{rrrrcr} 3x &amp; -y &amp; &amp; + t &amp; = &amp; 0\\\\ 3x &amp; + y &amp; &amp; -t &amp; = &amp; 0\\\\ 3x &amp; &amp; + z &amp; + 3t &amp; = &amp; 0\\\\ \\end{array}\\right.\\] Aplicamos lo que hemos visto antes para estudiar este sistema. Este sistema, resuelto por Gauss, nos da las siguientes expresiones paramétricas: \\[\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 0\\\\ 1\\\\ -3\\\\ 1 \\end{array}\\right) {}\\] Por tanto, una base de \\(U\\cap W\\) será: \\[\\mathcal{B}_{U\\cap W} = \\left\\{\\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ -3\\\\ 1 \\end{array} \\right) \\right\\}\\] Y, así, \\(\\mathrm{dim}(U\\cap W) = 1\\). 2.8 ¿Es la unión de subespacios un subespacio? En general, no. La única forma en que la unión de dos subespacios sea también un subespacio es que sean iguales o que, al menos, uno de ellos sea el espacio \\(\\{0\\}\\) o el total, \\(V\\). 2.9 Cómo hallar la suma de dos subespacios vectoriales Dados dos subespacios \\(U\\) y \\(W\\) de \\(V\\), se define su suma \\(U+W\\) como el conjunto de vectores que son combinación lineal de un vector de \\(U\\) y otro de \\(W\\). Si, además, \\(U\\cap W = \\{0\\}\\) y \\(U+W = V\\), entonces se llama suma ortogonal y se denota por \\(U\\oplus W\\). Se dice entonces que \\(U\\) es el subespacio suplementario de \\(W\\) y viceversa. El resultado teórico esencial es el siguiente: Si \\(\\mathcal{B}_U\\) y \\(\\mathcal{B}_W\\) son bases de \\(U\\) y \\(W\\) respectivamente, entonces \\(\\mathcal{G} = \\mathcal{B}_U \\cup \\mathcal{B}_W\\) es un sistema generador de \\(U+W\\). Por tanto, teniendo bases de \\(U\\) y de \\(W\\), lo primero será unirlas para obtener un sistema generador de la suma. Después, usaremos la técnica para conseguir una base de un subespacio a partir de un sistema generador. Ejemplo Consideramos los mismos espacios \\(U\\) y \\(W\\) del ejemplo de la sección sobre intersección. \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} 3x-y+ t &amp; = &amp; 0\\\\ 3x+ y-t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\]\\[W = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} 3x+ z+ 3t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] Podemos aplicar la técnica para conseguir una base a partir de las ecuaciones cartesianas, y llegamos a las siguientes bases: \\[\\mathcal{B}_{U} = \\left\\{\\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) \\right\\}\\] \\[\\mathcal{B}_{W} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{1}{3}\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Ahora, tomando \\[\\mathcal{G} = \\mathcal{B}_{U} \\cup \\mathcal{B}_{W} = \\left\\{\\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} - \\frac{1}{3}\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] que es un sistema generador de la suma, podemos construir una base de \\(U+W\\) eliminando los vectores de \\(\\mathcal{G}\\) que sean combinaciones lineales de los demás, lo cual requiere usar el método de Gauss para obtener la forma escalonada reducida de ese sistema de vectores. De esta forma, encontramos una base de \\(U+W\\): \\[\\mathcal{B}_{U + W} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{1}{3}\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) \\right\\}\\] Y, así, tenemos que \\(\\mathrm{dim}(U+W) = 4\\). 2.10 Qué dice el teorema de la dimensión Hay un resultado teórico que relaciona las dimensiones de dos subespacios \\(U\\) y \\(W\\) con las de su intersección y su suma: Fórmula de Grassmann: \\(\\mathrm{dim}(U+W) = \\mathrm{dim}(U) + \\mathrm{dim}(W) - \\mathrm{dim}(U\\cap W)\\) Ejemplo Si consideramos los espacios \\(U\\) y \\(W\\) que hemos usado como ejemplos en las secciones anteriores, podemos comprobar fácilmente el teorema de la dimensión: \\[\\begin{array}{ccccccc} \\mathrm{dim}(U+W) &amp; = &amp; \\mathrm{dim}(U) &amp; + &amp; \\mathrm{dim}(W) &amp; - &amp; \\mathrm{dim}(U\\cap V) \\\\ 4 &amp; = &amp; 2 &amp; + &amp; 3 &amp; - &amp; 1 \\\\ \\end{array}\\] 2.11 Cómo calcular el subespacio suplementario de uno dado Como hemos comentado antes, el subespacio suplementario de un subespacio \\(U\\) es aquel \\(W\\) que verifica: \\(U\\cap W = \\{0\\}\\) \\(U \\oplus W = V\\) Supongamos que tenemos una base \\(\\mathcal{B}_U\\) de \\(U\\). Para determinar \\(W\\), basta con que demos una forma de construir una base suya. El primer paso será encontrar una forma sencilla para la base de \\(U\\). Realmente, queremos encontrar una base de \\(U\\) a partir de la que ya tenemos, donde los vectores formen un sistema escalonado reducido. El método más indicado es usar Gauss-Jordan sobre la matriz que forman los vectores de \\(\\mathcal{B}_U\\) por filas. Una vez que tenemos la forma escalonada reducida, nos fijamos en qué posiciones se encuentran los pivotes en esa matriz, es decir, las posiciones del primer elemento distinto de cero de cada fila. Tomamos entonces, para formar la base de \\(W\\), aquellos vectores de la base canónica que no tengan un 1 en esas posiciones. Los vectores así determinados nos dan la base de \\(W\\), el subespacio suplementario, pues nos aseguramos las dos propiedades mencionadas más arriba. Ejemplo Consideremos el subespacio \\(U\\) cuya base es: \\[\\mathcal{B}_U = \\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 0\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Si no tuviéramos su base, sino que \\(U\\) viene dado de otra forma, ya sabemos cómo encontrar la base. Ponemos los vectores de la base de \\(U\\) en filas y aplicamos el método de Gauss (o Gauss-Jordan) para encontrar la forma escalonada de la matriz: \\[\\left( \\begin{array}{cccc@{}} 2 &amp; 0 &amp; 1 &amp; -1\\\\ 1 &amp; -1 &amp; 0 &amp; 1 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc@{}} 2 &amp; 0 &amp; 1 &amp; -1\\\\ 0 &amp; -1 &amp; - \\frac{1}{2} &amp; \\frac{3}{2} \\end{array} \\right) \\] Al inspeccionar la matriz escalonada, podemos ver que sus pivotes (primer elemento no nulo de cada fila) están en las columnas \\(\\{1, 2\\}\\). Consideramos la base canónica \\[\\mathcal{C} = \\{e_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Nos vamos a quedar con aquellos vectores de \\(\\mathcal{C}\\) que no tengan un 1 en las posiciones de los pivotes (\\(\\{1, 2\\}\\)). De esta forma, lo que tenemos es una base de \\(W\\), el subespacio suplementario de \\(U\\): \\[\\mathcal{B}_W = \\left\\{\\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] 2.12 Cómo calcular las coordenadas de un vector con respecto a una base dada Un concepto importante es el de coordenadas de un vector, asociadas a una base \\(\\mathcal{B}\\) del espacio vectorial \\(V\\). Supongamos una base \\(\\mathcal{B} = \\{v_1,\\ldots,v_n\\}\\) de \\(V\\). Las coordenadas de un vector \\(v\\) son unos escalares \\(\\alpha_1,\\ldots,\\alpha_n\\) tales que podemos expresar \\(v\\) como una combinación lineal de los vectores de la base con esos coeficientes: \\[v = \\alpha_1 v_1 + \\cdots + \\alpha_n v_n\\] Generalmente, como venimos haciendo desde el principio, estamos denotando e identificando a un vector con sus coordenadas, de forma que escribimos: \\[v = \\left( \\begin{array}{c} \\alpha_1\\\\ \\vdots\\\\ \\alpha_n \\end{array} \\right)_{\\mathcal{B}}\\] Cuando la base \\(\\mathcal{B}\\) es la canónica, no es necesario indicar el nombre de la base con respecto a la cual tomamos las coordenadas, a menos que lleve a confusión. Dado un vector con sus coordenadas en la base canónica, ¿cómo puedo saber sus coordenadas en otra base \\(\\mathcal{B}\\)? Tenemos dos vías, una que exploramos aquí y otra que veremos en la sección siguiente. El determinar las coordenadas de un vector en una base distinta de la canónica es realmente equivalente a la resolución de un sistema de ecuaciones lineales, donde la matriz de coeficientes es la formada por los vectores de la nueva base por columnas. Lo vemos directamente con un ejemplo. Ejemplo Consideremos el vector \\(v\\) dado por sus coordenadas en la base canónica \\(\\left(\\begin{array}{c} -5\\\\ 1\\\\ 3\\\\ -2 \\end{array}\\right)\\). Consideremos también la base: \\[\\mathcal{B}_{1} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ -1\\\\ -1 \\end{array} \\right) \\right\\}\\] Unos escalares \\(a, b, c, d\\) serán las coordenadas de \\(v\\) en la base \\(\\mathcal{B}_1\\) si se verifica: \\[\\left(\\begin{array}{c} -5\\\\ 1\\\\ 3\\\\ -2 \\end{array}\\right) = a\\left(\\begin{array}{c} -1\\\\ 1\\\\ 0\\\\ -1 \\end{array}\\right) {}+b\\left(\\begin{array}{c} 0\\\\ -1\\\\ 1\\\\ -1 \\end{array}\\right) {}+c\\left(\\begin{array}{c} 0\\\\ 0\\\\ 1\\\\ -1 \\end{array}\\right) {}+d\\left(\\begin{array}{c} 1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) {}\\] que se puede reescribir como el siguiente sistema de ecuaciones lineales (donde la matriz de coeficientes resulta de poner los vectores de la base \\(\\mathcal{B}_1\\) por columnas): \\[\\left\\{\\begin{array}{rrrrcr} -a &amp; &amp; &amp; + d &amp; = &amp; -5\\\\ a &amp; -b &amp; &amp; + d &amp; = &amp; 1\\\\ &amp; b &amp; + c &amp; -d &amp; = &amp; 3\\\\ -a &amp; -b &amp; -c &amp; -d &amp; = &amp; -2\\\\ \\end{array}\\right.\\] Este sistema se puede resolver usando métodos Gaussianos, y nos da: \\[ v = \\left(\\begin{array}{c} a\\\\ b\\\\ c\\\\ d \\end{array}\\right) _{\\mathcal{B}_1} = \\left(\\begin{array}{c} 3\\\\ 0\\\\ 1\\\\ -2 \\end{array}\\right) _{\\mathcal{B}_1}\\] 2.13 Cómo calcular la matriz de cambio de base entre dos bases de un mismo espacio vectorial Supongamos dos bases \\(\\mathcal{B}_1\\) y \\(\\mathcal{B}_2\\) de \\(V\\), y que tenemos un vector \\(v\\), expresado en coordenadas de \\(\\mathcal{B}_1\\). Nos planteamos una forma sencilla de calcular las coordenadas del mismo vector \\(v\\) respecto a la otra base \\(\\mathcal{B}_2\\). El principal resultado teórico en el que nos basamos (y del que alguna pincelada hemos dado en la sección anterior) es el que sigue: La transformación de las coordenadas para un mismo vector \\(v\\) entre dos bases \\(\\mathcal{B}_1\\) y \\(\\mathcal{B}_2\\) viene dada por la siguiente expresión: \\[[v]_{\\mathcal{B}_2} = A [v]_{\\mathcal{B}_1}\\] donde \\(A\\) es una matriz cuadrada formada por las coordenadas de cada vector de \\(\\mathcal{B}_1\\) en la base \\(\\mathcal{B}_2\\) y \\([v]_{\\mathcal{B}_i}\\) indica las coordenadas de \\(v\\) en la base \\(\\mathcal{B}_i\\). Notación: A la matriz de cambio entre la base \\(\\mathcal{B}_1\\) y \\(\\mathcal{B}_2\\), se suele denotar por \\(P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}\\). Para calcular \\(P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}\\), para cada vector de \\(\\mathcal{B}_1\\) vamos a encontrar sus coordenadas en la base \\(\\mathcal{B}_2\\). Siguiendo las explicaciones acerca de cómo calcular coordenadas de un vector respecto de una base, queremos expresar cada \\(v_i\\) como combinación lineal de los \\(v_i&#39;\\): \\[v_i = a_{1,i}v_1&#39;+\\ldots+a_{n,i}v_n&#39;\\quad\\,\\,\\ i = 1,\\ldots,n\\] En este caso, los valores \\(a_{1,i},\\ldots,a_{1,n}\\) son las coordenadas de \\(v_i\\) en la base \\(\\mathcal{B}_2\\), y podemos escribir, para todo \\(i=1,\\ldots,n\\): \\[[v_i]_{\\mathcal{B}_2} = \\left(\\begin{array}{c} a_{1,i}\\\\ \\vdots\\\\ a_{n,i} \\end{array}\\right)\\] Según el resultado anterior, la matriz de cambio de base \\(A=P_{\\mathcal{B}_1 \\to \\mathcal{B}_2}\\) tendrá por columnas estas coordenadas previamente calculadas: \\[A = ([v_1]_{\\mathcal{B}_2}|\\ldots|[v_n]_{\\mathcal{B}_2})\\] Llamemos \\(B_1 = (v_1|v_2|\\ldots|v_n)\\) y \\(B_2 = (v_1&#39;|v_2&#39;|\\ldots|v_n&#39;)\\) a las dos matrices que resultan de poner los vectores de las dos bases que tenemos, en forma de columna. Entonces, resulta que la combinación lineal de más arriba que relaciona un \\(v_i\\) con los vectores de \\(\\mathcal{B}_2\\) mediante sus coordenadas se puede reescribir como: \\[v_i = B_2\\ [v_i]_{\\mathcal{B}_2}\\] Si vamos concatenando los \\(v_i\\) en la izquierda, hasta formar la matriz \\(B_1\\), en la derecha debemos ir haciendo lo mismo con sus coordenadas en la base \\(\\mathcal{B}_2\\), y entonces tenemos: \\[(v_1|v_2|\\ldots|v_n) = B_2\\ ([v_1]_{\\mathcal{B}_2}|[v_2]_{\\mathcal{B}_2}\\ldots|[v_n]_{\\mathcal{B}_2})\\] es decir, \\[B_1 = B_2\\ P_{\\mathcal{B}_1\\to\\mathcal{B}_2}\\] Como nos interesa una forma fácil de calcular \\(P_{\\mathcal{B}_1\\to\\mathcal{B}_2}\\), podemos despejarla de la expresión anterior, luego tenemos \\(P_{\\mathcal{B}_1\\to\\mathcal{B}_2} = B_2^{-1}\\ B_1\\). El método práctico para calcular esta matriz de cambio de base, por tanto, será usar Gauss-Jordan, comenzando por la matriz \\((B_2|B_1)\\) hasta dejarla en la forma \\((I_n|A)\\). En ese momento, en la derecha, tendremos \\(A=P_{\\mathcal{B}_1\\to\\mathcal{B}_2}\\). Una vez que tenemos determinada la matriz \\(A = P_{\\mathcal{B}_1\\to\\mathcal{B}_2}\\) que nos proporciona el cambio de base entre \\(\\mathcal{B}_1\\) y \\(\\mathcal{B}_2\\), el cambio inverso, es decir, entre \\(\\mathcal{B}_2\\) y \\(\\mathcal{B}_1\\), tiene como matriz asociada \\(A^{-1}\\). Recordemos que la inversa de una matriz \\(A\\) se puede calcular mediante el método de Gauss-Jordan, comenzando por \\((A|I_n)\\) y reduciéndola a la forma \\((I_n|A^{-1})\\). Caso particular: Cambio de base a y desde la base canónica Supongamos que una de las bases, por ejemplo \\(\\mathcal{B}_2 = \\mathcal{C}\\) es la base canónica. La matriz que forman sus vectores por columna en este caso sería \\(B_2 = I_n\\), la identidad. Aplicando lo desarrollado, tendríamos que la matriz de cambio de base de \\(\\mathcal{B}_1\\) a la canónica es: \\(P_{\\mathcal{B}_1\\to\\mathcal{C}} = P_{\\mathcal{B}_1\\to\\mathcal{B}_2} = B_2^{-1}\\ B_1 = I_n \\ B_1 = B_1\\). Y entonces \\(P_{\\mathcal{C}\\to\\mathcal{B}_1} = P_{\\mathcal{B}_2\\to\\mathcal{B}_1} = B_1^{-1}\\ B_2 = B_1^{-1}\\ I_n = B_1^{-1}\\). Ejemplo Consideremos dos bases: \\[\\mathcal{B}_{1} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ -1\\\\ -1 \\end{array} \\right) \\right\\}\\]\\[\\mathcal{B}_{2} = \\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 2\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 2\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 2\\\\ 2\\\\ 2\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 2 \\end{array} \\right) \\right\\}\\] Vamos a hallar la matriz del cambio de base entre ambas bases, en ambos sentidos. Comenzamos por hallar la matriz \\(A\\) de cambio de base de \\(\\mathcal{B}_1\\) a \\(\\mathcal{B}_2\\). Construimos las matrices que tienen los vectores de \\(\\mathcal{B}_1\\) y \\(\\mathcal{B}_2\\) por columnas: \\[B_1 = \\left( \\begin{array}{cccc@{}} -1 &amp; 0 &amp; 0 &amp; 1\\\\ 1 &amp; -1 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 1 &amp; -1\\\\ -1 &amp; -1 &amp; -1 &amp; -1 \\end{array} \\right) \\]\\[B_2 = \\left( \\begin{array}{cccc@{}} 2 &amp; 0 &amp; 2 &amp; 1\\\\ 2 &amp; 2 &amp; 2 &amp; 1\\\\ 0 &amp; 0 &amp; 2 &amp; 0\\\\ 0 &amp; 0 &amp; 2 &amp; 2 \\end{array} \\right) \\] Como se ha mencionado antes, para hallar \\(A\\), podemos hacer Gauss-Jordan sobre \\((B_2|B_1)\\) hasta llegar a \\((I_n|A)\\): \\[\\left( \\begin{array}{cccc|cccc} 2 &amp; 0 &amp; 2 &amp; 1 &amp; -1 &amp; 0 &amp; 0 &amp; 1\\\\ 2 &amp; 2 &amp; 2 &amp; 1 &amp; 1 &amp; -1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; -1\\\\ 0 &amp; 0 &amp; 2 &amp; 2 &amp; -1 &amp; -1 &amp; -1 &amp; -1 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|cccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; - \\frac{1}{4} &amp; 0 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; - \\frac{1}{2} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; - \\frac{1}{2}\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; - \\frac{1}{2} &amp; -1 &amp; -1 &amp; 0 \\end{array} \\right) \\] Luego \\[P_{\\mathcal{B}_1\\to\\mathcal{B}_2} = \\left( \\begin{array}{cccc@{}} - \\frac{1}{4} &amp; 0 &amp; 0 &amp; 1\\\\ 1 &amp; - \\frac{1}{2} &amp; 0 &amp; 0\\\\ 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; - \\frac{1}{2}\\\\ - \\frac{1}{2} &amp; -1 &amp; -1 &amp; 0 \\end{array} \\right) \\] Para el camino contrario, pasar de coordenadas en \\(\\mathcal{B}_2\\) a coordenadas en \\(\\mathcal{B}_1\\), hacemos la inversa de la matriz anterior (aunque podríamos haber aplicado el mismo razonamiento inicial): \\[\\left( \\begin{array}{cccc|cccc} - \\frac{1}{4} &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; - \\frac{1}{2} &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; - \\frac{1}{2} &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ - \\frac{1}{2} &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|cccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; - \\frac{4}{3} &amp; 0 &amp; - \\frac{8}{3} &amp; - \\frac{4}{3}\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; - \\frac{8}{3} &amp; -2 &amp; - \\frac{16}{3} &amp; - \\frac{8}{3}\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; \\frac{10}{3} &amp; 2 &amp; \\frac{20}{3} &amp; \\frac{7}{3}\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\frac{2}{3} &amp; 0 &amp; - \\frac{2}{3} &amp; - \\frac{1}{3} \\end{array} \\right) \\] De esta forma \\[P_{\\mathcal{B}_2\\to\\mathcal{B}_1} = \\left( \\begin{array}{cccc@{}} - \\frac{4}{3} &amp; 0 &amp; - \\frac{8}{3} &amp; - \\frac{4}{3}\\\\ - \\frac{8}{3} &amp; -2 &amp; - \\frac{16}{3} &amp; - \\frac{8}{3}\\\\ \\frac{10}{3} &amp; 2 &amp; \\frac{20}{3} &amp; \\frac{7}{3}\\\\ \\frac{2}{3} &amp; 0 &amp; - \\frac{2}{3} &amp; - \\frac{1}{3} \\end{array} \\right) \\] Para completar el ejemplo, vemos el cambio de base entre \\(\\mathcal{B}_1\\) y \\(\\mathcal{C}\\): \\[P_{\\mathcal{B}_1\\to\\mathcal{C}} = B_1 = \\left( \\begin{array}{cccc@{}} -1 &amp; 0 &amp; 0 &amp; 1\\\\ 1 &amp; -1 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 1 &amp; -1\\\\ -1 &amp; -1 &amp; -1 &amp; -1 \\end{array} \\right) \\] Y entre \\(\\mathcal{C}\\) y \\(\\mathcal{B}_1\\) (calculando la inversa de \\(B_1\\) usando Gauss-Jordan): \\[P_{\\mathcal{C}\\to\\mathcal{B}_1} = B_1^{-1} = \\left( \\begin{array}{cccc@{}} - \\frac{2}{3} &amp; 0 &amp; - \\frac{1}{3} &amp; - \\frac{1}{3}\\\\ - \\frac{1}{3} &amp; -1 &amp; - \\frac{2}{3} &amp; - \\frac{2}{3}\\\\ \\frac{2}{3} &amp; 1 &amp; \\frac{4}{3} &amp; \\frac{1}{3}\\\\ \\frac{1}{3} &amp; 0 &amp; - \\frac{1}{3} &amp; - \\frac{1}{3} \\end{array} \\right) \\] "],
["apli.html", "3 Aplicaciones lineales 3.1 Qué propiedades básicas tienen las aplicaciones lineales 3.2 Cuál es la matriz asociada a una aplicación lineal en unas bases dadas 3.3 Cómo influye un cambio de base en la matriz asociada a una aplicación lineal 3.4 Qué es y cómo determinar el núcleo de una aplicación lineal 3.5 Cómo podemos identificar una aplicación lineal inyectiva 3.6 Cómo calcular el subespacio imagen de una aplicación lineal 3.7 Cómo identificar si una aplicación lineal es sobreyectiva 3.8 Qué es un isomorfismo 3.9 Cómo determinar la imagen de un subespacio vectorial 3.10 A qué se llama rango y nulidad de la aplicación lineal 3.11 Qué dice el teorema de la dimensión para núcleo e imagen", " 3 Aplicaciones lineales Consideremos dos espacios vectoriales \\(V\\) y \\(W\\) sobre el mismo cuerpo \\(\\mathbb{K}\\), y una aplicación \\(f:V\\to W\\) entre ellos. Diremos que \\(f\\) es una aplicación lineal si verifica las siguientes dos condiciones: \\(f(v_1 + v_2) = f(v_1) + f(v_2)\\) para todo \\(v_1, v_2\\in V\\). \\(f(c\\cdot v) = c\\cdot f(v)\\) para todo \\(c\\in\\mathbb{K},\\,v\\in V\\). Una aplicación lineal cuyo dominio y codominio sean iguales (\\(V = W\\)) se llama endomorfismo. Ejemplo Existen muchos ejemplos de aplicaciones lineales: La aplicación 0, la identidad… son aplicaciones lineales de forma trivial. Las rotaciones de vectores en \\(\\mathbb{R}^n\\) son aplicaciones lineales. Sobre el espacio vectorial \\(\\mathcal{C}([a, b])\\) de funciones continuas y derivables en un intervalo \\([a, b]\\), tanto el operador derivada \\(f\\to f&#39;\\) como el operador integral \\(f \\to \\int_a^b f\\), son aplicaciones lineales. ¿Qué preguntas vamos a responder en este capítulo? ¿Qué propiedades básicas tienen las aplicaciones lineales? ¿Cuál es la matriz asociada a una aplicación lineal en unas bases dadas? ¿Cómo influye un cambio de base en la matriz asociada a una aplicación lineal? ¿Qué es y cómo determinar el núcleo de una aplicación lineal? ¿Cómo podemos identificar una aplicación lineal inyectiva? ¿Cómo calcular el subespacio imagen de una aplicación lineal? ¿Cómo identificar si una aplicación lineal es sobreyectiva? ¿Qué es un isomorfismo? ¿Cómo determinar la imagen de un subespacio vectorial? ¿A qué se llama rango y nulidad de la aplicación lineal? ¿Qué dice el teorema de la dimensión para núcleo e imagen? 3.1 Qué propiedades básicas tienen las aplicaciones lineales Vamos a revisar aquí algunas de las propiedades básicas de las aplicaciones lineales entre espacios vectoriales. Consideremos una aplicación lineal \\(f:V\\to W\\). \\(f(0_V) = 0_W\\) \\(f(-v) = -f(v),\\quad v\\in V\\) \\(f(v_1 - v_2) = f(v_1) - f(v_2),\\quad v_1, v_2\\in V\\) \\(f(\\alpha_1 v_1 + \\ldots \\alpha_n v_n) = \\alpha_1 f(v_1) + \\ldots + \\alpha_n f(v_n),\\quad \\alpha_i\\in\\mathbb{K},v_i\\in V\\) Además: Si \\(U\\) es un subespacio de \\(V\\), entonces \\(f(U)\\) es un subespacio de \\(W\\). Análogamente, si \\(S\\) es subespacio de \\(W\\), su preimagen \\(f^{-1}(S)\\) es también un subespacio de \\(V\\). Si \\(\\mathcal{G} = \\{u_1,\\ldots,u_m\\}\\) es un sistema generador de un subespacio \\(U\\) de \\(V\\), entonces \\(f(\\mathcal{G}) = \\{f(u_1),\\ldots,f(u_m)\\}\\) es un sistema generador de \\(f(U)\\). Y, por último, Si \\(f:V \\to W\\) y \\(g:W \\to U\\) son dos aplicaciones lineales, entonces su composición \\(g \\circ f :V \\to U\\) también es una aplicación lineal. 3.2 Cuál es la matriz asociada a una aplicación lineal en unas bases dadas Si tenemos una matriz \\(A\\in \\mathcal{M}_{mn}(\\mathbb{R})\\), podemos definir una aplicación lineal \\(f_A\\) entre \\(\\mathbb{R}^n\\) y \\(\\mathbb{R}^m\\) de la forma: \\[f_A(v) = A\\cdot v,\\quad v\\in\\mathbb{R}^n\\] En este resultado, podemos cambiar el cuerpo \\(\\mathbb{R}\\) de los números reales por cualquier otro cuerpo. Este resultado nos habla de una forma de construir aplicaciones lineales, mediante el producto por una matriz que tenga las dimensiones adecuadas. Un resultado más interesante es el siguiente, que nos dice que TODA aplicación lineal viene definida por el producto con una matriz: Sea \\(f:V\\to W\\) una aplicación lineal. Fijemos dos bases \\(\\mathcal{B}_V\\) y \\(\\mathcal{B}_W\\) en dichos espacios, respectivamente. Entonces existe una matriz \\(A\\in\\mathcal{M}_{mn}(\\mathbb{R})\\) tal que si \\(w = f(v)\\) entonces \\([w]_{\\mathcal{B}_W} = A\\cdot [v]_{\\mathcal{B}_V}\\) para todo \\(v\\in V\\). Esto significa que, fijando las bases, podemos encontrar una matriz \\(A\\) de forma que para calcular la imagen de un vector \\(v\\in V\\), basta con tomar sus coordenadas en la base \\(\\mathcal{B}_V\\), que representamos por \\([v]_{\\mathcal{B}_V}\\), y multiplicarlas por \\(A\\), para obtener las coordenadas de \\(f(v)\\) en la base correspondiente del codominio \\(W\\), las cuales se denotan por \\([f(v)]_{\\mathcal{B}_W}\\). A la matriz \\(A\\) que nos proporciona este resultado la llamamos matriz asociada a \\(f\\) en las bases \\(\\mathcal{B}_V\\) y \\(\\mathcal{B}_W\\). Hay que notar que la matriz \\(A\\) está determinada siempre y cuando fijemos las dos bases antes dichas. Pero, ¿cómo determinamos la matriz \\(A\\)? Necesitamos fijar las dos bases de antemano: \\[\\mathcal{B}_V = \\{v_1,\\ldots,v_n\\}\\] \\[\\mathcal{B}_W = \\{w_1,\\ldots,w_m\\}\\] El procedimiento general para construir la matriz \\(A\\) es el que sigue. Para determinar la columna \\(j\\)-ésima de \\(A\\): Tomamos el vector \\(j\\)-ésimo de la base de \\(V\\): \\(v_j\\). Calculamos su imagen \\(y_j = f(v_j)\\). Calculamos las coordenadas de \\(y_j\\) en la base \\(\\mathcal{B}_W\\). \\[[y_j]_{\\mathcal{B}_W} = \\left( \\begin{array}{c} a_{1, j}\\\\ a_{2, j}\\\\ \\vdots\\\\ a_{m, j} \\end{array} \\right) \\Leftrightarrow y_j = a_{1,j} w_1 + a_{2,j}w_2 + \\ldots + a_{m, j} w_m\\] Esas coordenadas son la columna \\(j\\) de la matriz \\(A\\). Vamos a pasar todo esto a notación matricial, para encontrar una forma cómoda de resolver este problema. Llamemos \\(B_2\\) a la matriz formada, por columnas, por las coordenadas de los vectores de \\(\\mathcal{B}_W\\) en la base canónica: \\(B_2 = \\left(w_1|w_2|\\ldots|w_m\\right)\\). Llamamos \\(Y = \\left(y_1|y_2|\\ldots|y_n\\right) = \\left(f(v_1)|f(v_2)|\\ldots|f(v_n)\\right)\\). Entonces, la expresión anterior se puede escribir matricialmente como \\(Y = B_2\\cdot A\\), siendo \\(A\\) la matriz asociada a la aplicación lineal. Por tanto, podemos despejar \\(A = B_2^{-1} Y\\). Por tanto, para calcular \\(A\\) de forma directa, podemos realizar Gauss-Jordan partiendo de \\((B_2 | Y )\\) hasta lograr \\((I_m | A)\\). Caso particular: Base canónica en \\(V\\) y \\(W\\) En este caso, la matriz asociada a la aplicación lineal es \\(A = Y = \\left(f(v_1)|f(v_2)|\\ldots|f(v_n)\\right)\\), puesto que los vectores de la base canónica puestos por columnas forman la matriz identidad. De hecho, en la situación de la base canónica, lo más rápido es fijarse en los coeficientes de cada una de las variables \\(x,y,z,\\ldots\\) o \\(x_1,x_2,\\ldots\\), según sea el caso, y ponerlos por columnas. Así, para formar la matriz \\(A\\) asociada a la aplicación lineal, tomaríamos como primera columna los coeficientes de la \\(x\\), en la segunda, los coeficientes de la \\(y\\), y así sucesivamente con el resto de variables. Ejemplo Consideremos la aplicación \\(f:V=\\mathbb{R}^{3}\\to W=\\mathbb{R}^{3}\\) dada por: \\[f \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left( \\begin{array}{c} x+ y+ z\\\\ x-2y\\\\ 2x-y+ z\\\\ \\end{array} \\right)\\] Como hemos visto, para encontrar su matriz asociada en las bases canónicas de \\(v\\) y \\(W\\), basta con mirar los coeficientes de \\(x,y,\\ldots\\), y ponerlos por columnas: \\[A = \\left(\\begin{array}{ccc} 1 &amp; 1 &amp; 1\\\\ 1 &amp; -2 &amp; 0\\\\ 2 &amp; -1 &amp; 1 \\end{array}\\right) \\] Esto significa que si tenemos un vector \\(v\\in V\\) expresado en coordenadas en la base canónica, entonces \\(f(v) = A\\ v\\), también expresado en las coordenadas canónicas de \\(W\\). Consideremos ahora dos bases distintas de la canónica tanto en \\(V\\) como \\(W\\): \\[\\mathcal{B}_{1} = \\{v_i\\} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ -1\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1 \\end{array} \\right) \\right\\}\\]\\[\\mathcal{B}_{2} = \\{w_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 2\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Vamos a calcular la matriz \\(A&#39;\\) asociada a \\(f\\) en estas dos bases. Debemos calcular: \\[Y= (f(v_1)|\\ldots|f(v_n))\\] \\[B_2 = (w_1|\\ldots|w_m)\\] Aplicando \\(f\\) sucesivamente a los vectores de \\(\\mathcal{B}_1\\), tenemos que \\[Y = \\left( \\begin{array}{ccc@{}} -1 &amp; 0 &amp; -3\\\\ -3 &amp; 2 &amp; 1\\\\ -4 &amp; 2 &amp; -2 \\end{array} \\right) \\] y \\[B_2 = \\left( \\begin{array}{ccc@{}} 1 &amp; 1 &amp; 1\\\\ 0 &amp; 2 &amp; 0\\\\ 2 &amp; 0 &amp; 1 \\end{array} \\right) \\] Luego podemos obtener \\(A&#39;\\) como \\(B_2^{-1} Y\\), que resolvemos por Gauss-Jordan: \\[\\left( \\begin{array}{ccc|ccc} 1 &amp; 1 &amp; 1 &amp; -1 &amp; 0 &amp; -3\\\\ 0 &amp; 2 &amp; 0 &amp; -3 &amp; 2 &amp; 1\\\\ 2 &amp; 0 &amp; 1 &amp; -4 &amp; 2 &amp; -2 \\end{array} \\right) \\sim\\left( \\begin{array}{ccc|ccc} 1 &amp; 0 &amp; 0 &amp; - \\frac{9}{2} &amp; 3 &amp; \\frac{3}{2}\\\\ 0 &amp; 1 &amp; 0 &amp; - \\frac{3}{2} &amp; 1 &amp; \\frac{1}{2}\\\\ 0 &amp; 0 &amp; 1 &amp; 5 &amp; -4 &amp; -5 \\end{array} \\right) \\] Y de aquí que la nueva matriz, asociada a \\(f\\) en las nuevas bases, sea: \\[A&#39; = \\left( \\begin{array}{ccc@{}} - \\frac{9}{2} &amp; 3 &amp; \\frac{3}{2}\\\\ - \\frac{3}{2} &amp; 1 &amp; \\frac{1}{2}\\\\ 5 &amp; -4 &amp; -5 \\end{array} \\right) \\] 3.3 Cómo influye un cambio de base en la matriz asociada a una aplicación lineal Supongamos que tenemos una aplicación lineal \\(f:V\\to W\\) y que tenemos prefijadas unas bases \\(\\mathcal{B}_V\\) y \\(\\mathcal{B}_W\\) y, por tanto, tenemos determinada la matriz \\(A\\), asociada a \\(f\\) en esas bases. Consideremos unas nuevas bases \\(\\mathcal{B}&#39;_V\\) y \\(\\mathcal{B}&#39;_W\\) y nos preguntamos cuál será la matriz \\(A&#39;\\) asociada a \\(f\\) en dichas bases. Para saber hallar \\(A&#39;\\), nos basamos en dos resultados teóricos. El primero de ellos nos relaciona los cambios de base con las aplicaciones lineales: Un cambio de base entre dos bases \\(\\mathcal{B}_V\\) y \\(\\mathcal{B}&#39;_V\\) en un espacio vectorial \\(V\\) se corresponde con la aplicación lineal identidad \\(\\mathrm{id}_V:V \\to V\\), donde en el dominio hemos considerado la base \\(\\mathcal{B}_V\\) y en el codominio la base \\(\\mathcal{B}&#39;_V\\). Además, la matriz asociada a \\(\\mathrm{id}_V\\) en esta situación es justo la del cambio de base \\(P_{\\mathcal{B}_V \\to \\mathcal{B}&#39;_V}\\). Es decir, podemos considerar que un cambio de base no es más que la aplicación de la función identidad en el espacio vectorial, donde en dominio y codominio hemos considerado bases distintas. El otro resultado teórico nos relaciona la composición de aplicaciones lineales (que es de nuevo una aplicación lineal) con sus matrices asociadas: Si \\(f: V \\to W\\) y \\(g:W \\to U\\) son dos aplicaciones lineales, con matrices asociadas \\(A_f\\) y \\(A_g\\), respectivamente, habiendo fijado bases en \\(V\\), \\(W\\) y \\(U\\), entonces la aplicación lineal composición de \\(f\\) y \\(g\\), \\(g\\circ f:V\\to U\\), tiene como matriz asociada a \\(A_{g \\circ f} = A_g \\cdot A_f\\). Luego si componemos dos aplicaciones lineales, habiendo fijado bases en cada caso, la matriz asociada a la composición es el producto de las matrices individuales. Con estos dos resultados, ya estamos en condiciones de estudiar cómo influye un cambio de base en la matriz de la aplicación lineal. Veamos el siguiente esquema, donde denotamos \\(V_{\\mathcal{B}}\\) que estamos considerando la base \\(\\mathcal{B}\\) en el espacio \\(V\\): \\[ \\begin{array}{ccccccc} V_{\\mathcal{B}&#39;_V} &amp; \\xrightarrow{\\mathrm{id}_V} &amp; V_{\\mathcal{B}_V} &amp; \\xrightarrow{f} &amp; W_{\\mathcal{B}_W} &amp; \\xrightarrow{\\mathrm{id}_W} &amp; W_{\\mathcal{B}&#39;_W} \\\\ &amp; P_{\\mathcal{B}&#39;_V \\to \\mathcal{B}_V} &amp; &amp; A &amp; &amp; P_{\\mathcal{B}_W \\to \\mathcal{B}&#39;_W} &amp; \\\\ \\end{array} \\] Llamemos, por comodidad, \\(P = P_{\\mathcal{B}&#39;_V \\to \\mathcal{B}_V}\\) y \\(Q = P_{\\mathcal{B}_W \\to \\mathcal{B}&#39;_W}\\). Recordemos que tanto \\(P\\) como \\(Q\\) se pueden calcular de forma sencilla. De esta forma, vemos que si fijamos las nuevas bases, entonces la aplicación \\(f\\) resultante equivale a la \\(f\\) anterior, a la que precedemos y sucedemos con la aplicación identidad dentro de \\(V\\) y \\(W\\), respectivamente, puesto que \\(f = \\mathrm{id}_W\\circ f \\circ \\mathrm{id}_V\\). Teniendo en cuenta los resultados teóricos anteriores, esto quiere decir que \\(A&#39; = Q\\ A\\ P\\). Como consecuencia, tenemos además que Todas las matrices asociadas a una misma aplicación lineal \\(f\\) en distintas bases son equivalentes. Ejemplo Continuemos con el ejemplo de la sección anterior. Sabemos que la aplicación \\(f\\) allí definida tenía por matriz asociada: \\[A = \\left( \\begin{array}{ccc@{}} - \\frac{9}{2} &amp; 3 &amp; \\frac{3}{2}\\\\ - \\frac{3}{2} &amp; 1 &amp; \\frac{1}{2}\\\\ 5 &amp; -4 &amp; -5 \\end{array} \\right) \\] Consideremos ahora dos bases distintas tanto en \\(V\\) como \\(W\\): \\[\\mathcal{B}&#39;_{1} =\\left\\{\\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ -1\\\\ 1 \\end{array} \\right) \\right\\}\\]\\[\\mathcal{B}&#39;_{2} =\\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 2\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Vamos a aplicar lo anterior para encontrar la matriz \\(A&#39;\\) asociada a \\(f\\) en las nuevas bases. Para ello, sabemos que \\(A&#39; = Q\\ A\\ P\\), donde \\(P\\) es la matriz de cambio de base de \\(\\mathcal{B}&#39;_1\\) a \\(\\mathcal{B}_1\\) y \\(Q\\) es el cambio de \\(\\mathcal{B}_2\\) a \\(\\mathcal{B}&#39;_2\\). Si llamamos \\(B_1\\), \\(B&#39;_1\\), \\(B_2\\) y \\(B&#39;_2\\) a las matrices que resultan de poner por columnas los vectores de \\(\\mathcal{B}_1\\), \\(\\mathcal{B}&#39;_1\\), \\(\\mathcal{B}_2\\) y \\(\\mathcal{B}&#39;_2\\), respectivamente, podemos calcular \\(P\\) y \\(Q\\) de la siguiente manera, según ya hemos visto: \\[P = B_1^{-1}\\ B&#39;_1,\\quad\\quad Q = {B&#39;_2}^{-1}\\ B_2\\] Hacemos Gauss-Jordan partiendo de \\((B_1 | B&#39;_1)\\) hasta llegar a \\((I_n | P)\\), y partiendo de \\((B&#39;_2 | B_2)\\) hasta \\((I_m | Q)\\), obteniendo: \\[P = \\left( \\begin{array}{ccc@{}} -1 &amp; -1 &amp; 0\\\\ 0 &amp; -1 &amp; 1\\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right) ,\\quad\\quad\\ Q = \\left( \\begin{array}{ccc@{}} 0 &amp; 1 &amp; 0\\\\ 1 &amp; -1 &amp; 1\\\\ 0 &amp; 0 &amp; -1 \\end{array} \\right) \\] De ahí: \\[A&#39; = Q\\ A\\ P = \\left( \\begin{array}{ccc@{}} 0 &amp; 1 &amp; 0\\\\ 1 &amp; -1 &amp; 1\\\\ 0 &amp; 0 &amp; -1 \\end{array} \\right) \\left( \\begin{array}{ccc@{}} - \\frac{9}{2} &amp; 3 &amp; \\frac{3}{2}\\\\ - \\frac{3}{2} &amp; 1 &amp; \\frac{1}{2}\\\\ 5 &amp; -4 &amp; -5 \\end{array} \\right) \\left( \\begin{array}{ccc@{}} -1 &amp; -1 &amp; 0\\\\ 0 &amp; -1 &amp; 1\\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right) = \\left( \\begin{array}{ccc@{}} \\frac{3}{2} &amp; 1 &amp; 1\\\\ -2 &amp; -4 &amp; -2\\\\ 5 &amp; 6 &amp; 4 \\end{array} \\right) \\] 3.4 Qué es y cómo determinar el núcleo de una aplicación lineal Llamamos núcleo de una aplicación lineal \\(f:V \\to W\\), y lo denotamos por \\(\\mathrm{Ker}\\ f\\), al conjunto de aquellos vectores \\(v\\in V\\) tales que \\(f(v) = 0_W\\): \\[\\mathrm{Ker}\\ f = \\{v\\in V:f(v) = 0_W\\}\\] El primer resultado teórico interesante es: \\(\\mathrm{Ker}\\ f \\ne\\varnothing\\), puesto que, al menos, \\(0_V\\in\\mathrm{Ker}\\ f\\), como hemos visto antes. Podemos decir todavía más: Dada una aplicación lineal \\(f:V\\to W\\), su núcleo \\(\\mathrm{Ker}\\ f\\) es un subespacio vectorial de \\(V\\). Por tanto, para operar con el núcleo, podemos hacerlo como hemos visto en el capítulo acerca de espacios vectoriales: podemos calcular un sistema generador, determinar una base y su dimensión. ¿Qué relación tiene el núcleo de \\(f\\) con su matriz asociada? Supongamos que fijamos las bases en \\(V\\) y \\(W\\) y que la matriz asociada a \\(f\\) en esas bases es \\(A\\). Entonces, si \\(v\\in V\\) está en el núcleo, será que \\(f(v) = 0_W\\), luego las coordenadas de \\(f(v)\\) en la base de \\(W\\) son todas cero: \\([f(v)]_{\\mathcal{B}_W} = 0\\). Pero como conocemos la matriz asociada a \\(f\\), entonces sabemos que \\([f(v)]_{\\mathcal{B}_W} = A\\ [v]_{\\mathcal{B}_V}\\). En resumen, \\(v\\in\\mathrm{Ker}\\ f\\) si, y sólo si \\(A\\ [v]_{\\mathcal{B}_V} = 0\\), es decir, si y sólo si las coordenadas de \\(v\\) son solución del sistema de ecuaciones lineales homogéneo \\(Ax = 0\\). \\(\\mathrm{Ker}\\ f\\) coincide con el subespacio de las soluciones del sistema \\(Ax = 0\\). Esto también implica que las ecuaciones cartesianas de \\(\\mathrm{Ker}\\ f\\) son \\(Ax = 0\\). Por tanto, podemos aplicar lo que ya hemos visto para calcular una base del núcleo de \\(f\\). Ejemplo Seguimos con el ejemplo de la aplicación \\(f:V=\\mathbb{R}^{3}\\to W=\\mathbb{R}^{3}\\) dada por: \\[f \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left( \\begin{array}{c} x+ y+ z\\\\ x-2y\\\\ 2x-y+ z\\\\ \\end{array} \\right)\\] Como hemos visto, su matriz asociada es: \\[A = \\left(\\begin{array}{ccc} 1 &amp; 1 &amp; 1\\\\ 1 &amp; -2 &amp; 0\\\\ 2 &amp; -1 &amp; 1 \\end{array}\\right) \\] Usamos esa matriz \\(A\\) para construir el sistema homogéneo: \\[\\begin{array}{rrrcr} x &amp; + y &amp; + z &amp; = &amp; 0\\\\ x &amp; -2y &amp; &amp; = &amp; 0\\\\ 2x &amp; -y &amp; + z &amp; = &amp; 0\\\\ \\end{array}\\] que se corresponde con las ecuaciones cartesianas de \\(\\mathrm{Ker}\\ f\\) (podíamos haber hecho directamente \\(f(v) = 0\\) usando la definición de \\(f\\)). Usaremos lo que hemos visto ya para convertir esas ecuaciones a forma paramétrica (usando eliminación Gaussiana), y así conseguir un sistema generador y una base: \\[\\begin{array}{rrrcr} x &amp; + y &amp; + z &amp; = &amp; 0\\\\ x &amp; -2y &amp; &amp; = &amp; 0\\\\ 2x &amp; -y &amp; + z &amp; = &amp; 0\\\\ \\end{array}\\Leftrightarrow\\ \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} - \\frac{2}{3}\\\\ - \\frac{1}{3}\\\\ 1 \\end{array}\\right) {}\\] Luego \\[\\mathcal{B}_{\\mathrm{Ker}\\ f} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{2}{3}\\\\ - \\frac{1}{3}\\\\ 1 \\end{array} \\right) \\right\\}\\] 3.5 Cómo podemos identificar una aplicación lineal inyectiva Recordemos que una aplicación \\(f:X\\to Y\\) es inyectiva si y sólo si \\(f(x_1)\\ne f(x_2)\\) siempre que \\(x_1\\ne x_2\\), con \\(x_1, x_2\\in X\\). Hay una forma de caracterizar las aplicaciones lineales inyectivas: La aplicación lineal \\(f:V\\to W\\) es inyectiva si, y sólo si, \\(\\mathrm{Ker}\\ f = \\{0\\}\\), es decir, si y sólo si \\(\\mathrm{dim}(\\mathrm{Ker}\\ f) = 0\\). Un resultado interesante nos presenta una relación entre la inyectividad de \\(f\\) y las dimensiones de los espacios \\(V\\) y \\(W\\): Si la aplicación lineal \\(f:V\\to W\\) es inyectiva, entonces se verifica que \\(\\mathrm{dim}(V)\\le\\mathrm{dim}(W)\\). El recíproco no es cierto, no siempre que tengamos \\(\\mathrm{dim}(V)\\le\\mathrm{dim}(W)\\) la aplicación puede ser inyectiva. Por ejemplo, la aplicación cero \\(0:\\mathbb{R}^2\\to\\mathbb{R}^3\\), que asocia a todo vector el vector 0 del codominio, no puede ser inyectiva, a pesar de que la dimensión del dominio es menor que la del codominio. Podemos decir todavía algún resultado teórico que nos proporciona más información acerca de cómo identificar aplicaciones lineales inyectivas: Una aplicación \\(f:V\\to W\\) es inyectiva si, y sólo si, la imagen de todo sistema linealmente independiente es también un sistema linealmente independiente. Si \\(V\\) es de dimensión finita, entonces la aplicación lineal \\(f:V\\to W\\) es inyectiva si, y sólo si, \\(\\mathrm{dim}(V) = \\mathrm{dim}(\\mathrm{Im}\\ f)\\) Ejemplo En el ejemplo de la sección anterior, tenemos que \\(\\mathrm{dim}(\\mathrm{Ker}\\ f) = 1\\), luego \\(f\\) no es inyectiva, ya que en ese caso debería ser \\(\\mathrm{dim}(\\mathrm{Ker}\\ f) = 0\\). Consideremos otra aplicación \\(g:\\mathbb{R}^{3}\\to \\mathbb{R}^{3}\\) dada por: \\[g \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left( \\begin{array}{ccc@{}} 2 &amp; -1 &amp; 0\\\\ 1 &amp; 3 &amp; 0\\\\ 2 &amp; 0 &amp; 4 \\end{array} \\right) \\ \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left(\\begin{array}{c} 2x-y\\\\ x+ 3y\\\\ 2x+ 4z\\\\ \\end{array}\\right)\\] Podemos estudiar su núcleo para comprobar si es inyectiva: \\[\\mathrm{Ker}\\ g =\\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) \\in\\mathbb{R}^{3}:\\begin{array}{ccr} 2x-y &amp; = &amp; 0\\\\ x+ 3y &amp; = &amp; 0\\\\ 2x+ 4z &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] Resolvemos el sistema homogéneo para llegar a que \\[\\left\\{\\begin{array}{rrrcr} 2x &amp; -y &amp; &amp; = &amp; 0\\\\ x &amp; + 3y &amp; &amp; = &amp; 0\\\\ 2x &amp; &amp; + 4z &amp; = &amp; 0\\\\ \\end{array}\\right.\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left(\\begin{array}{c} 0\\\\ 0\\\\ 0 \\end{array}\\right) \\] Luego \\(\\mathrm{Ker}\\ g=\\{0\\}\\), y así \\(g\\) es inyectiva. 3.6 Cómo calcular el subespacio imagen de una aplicación lineal Llamamos imagen de la aplicación lineal \\(f:V\\to W\\) a: \\[\\mathrm{Im}\\ f = \\{w\\in W: \\text{existe }v\\in V \\text{ con }f(v) = w\\} = f(V)\\] Análogamente a lo que pasaba con el núcleo de una aplicación lineal, tenemos: Si \\(f:V\\to W\\) es una aplicación lineal, entonces \\(\\mathrm{Im}\\ f\\) es un subespacio vectorial de \\(W\\). Por tanto, podemos estudiarlo con todas las herramientas que conocemos. ¿Cómo calculamos \\(\\mathrm{Im}\\ f\\)? Recordemos que si \\(\\mathcal{G}\\) es un sistema generador de un subespacio \\(U\\) de \\(V\\), entonces \\(f(\\mathcal{G})\\) es un sistema generador de \\(f(U)\\). En concreto, tomando una base \\(\\mathcal{B}\\) de \\(V\\), su imagen \\(f(\\mathcal{B})\\) es un sistema generador de \\(\\mathrm{Im}\\ f\\). Basta, por tanto, aplicar \\(f\\) a cada vector de \\(\\mathcal{B}\\) y con eso construimos un sistema generador del que podemos eliminar los vectores linealmente dependientes para obtener una base de \\(\\mathrm{Im}\\ f\\). En consecuencia: Sea \\(f\\) una aplicación lineal entre dos espacios \\(V\\) y \\(W\\), y sea \\(A\\) la matriz asociada a \\(f\\), una vez que hemos fijado dos bases en \\(V\\) y \\(W\\). Entonces \\(\\mathrm{Im}\\ f\\) es el subespacio de \\(W\\) generado por los vectores columna que forman la matriz \\(A\\). Ejemplo Vamos a estudiar la imagen de las dos aplicaciones \\(f\\) y \\(g\\) del ejemplo anterior. Para \\(f\\), tenemos que \\(\\mathrm{Im}\\ f\\) está generado por los vectores columnas de la matriz \\(A\\), luego un sistema generador de \\(\\mathrm{Im}\\ f\\) será: \\[\\mathcal{G}_{\\mathrm{Im}\\ f} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ -2\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] A partir de él, eliminando los vectores que sean linealmente dependientes, encontramos una base de \\(\\mathrm{Im}\\ f\\): \\[\\mathcal{B}_{\\mathrm{Im}\\ f} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ -2\\\\ -1 \\end{array} \\right) \\right\\}\\] Luego \\(\\mathrm{dim}(\\mathrm{Im}\\ f) = 2\\). Vayamos ahora con la aplicación \\(g\\). Con el mismo razonamiento, sabemos que \\[\\mathcal{G}_{\\mathrm{Im}\\ g} = \\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 3\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 4 \\end{array} \\right) \\right\\}\\] es un sistema generador de \\(\\mathrm{Im}\\ g\\). Igual que antes, podemos encontrar una base de \\(\\mathrm{Im}\\ g\\): \\[\\mathcal{B}_{\\mathrm{Im}\\ g} = \\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 3\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 4 \\end{array} \\right) \\right\\}\\] Entonces \\(\\mathrm{dim}(\\mathrm{Im}\\ g) = 3\\). Esto nos dice que \\(\\mathrm{Im}\\ g = W\\). 3.7 Cómo identificar si una aplicación lineal es sobreyectiva Recordemos que una aplicación \\(f:X\\to Y\\) se dice sobreyectiva si, para cada \\(y\\in Y\\), existe un \\(x\\in X\\) tal que \\(f(x) = y\\), es decir, si \\(f(X) = Y\\). En cuanto a aplicaciones lineales, tenemos el siguiente resultado, consecuencia de lo anterior: La aplicación lineal \\(f:V\\to W\\) es sobreyectiva si \\(f(V) = \\mathrm{Im}\\ f = W\\), y esto es equivalente a decir que \\(\\mathrm{dim}(\\mathrm{Im}\\ f) = \\mathrm{dim}(W)\\). Una propiedad adicional, con relación a las dimensiones de \\(V\\) y de \\(W\\): Si la aplicación lineal \\(f:V\\to W\\) es sobreyectiva, entonces \\(\\mathrm{dim}(V)\\ge\\mathrm{dim}(W)\\). Ejemplo En el ejemplo anterior, estudiamos las imágenes de las aplicaciones \\(f\\) y \\(g\\) de los ejemplos. Si aplicamos el resultado teórico de antes, llegamos a que: \\(f\\) no es sobreyectiva, pues \\(\\mathrm{dim}(\\mathrm{Im}\\ f) = 2 \\ne 3 = \\mathrm{dim}(W)\\). \\(g\\) sí es sobreyectiva, pues \\(\\mathrm{dim}(\\mathrm{Im}\\ g) = 3 = \\mathrm{dim}(W)\\). 3.8 Qué es un isomorfismo Un isomorfismo es una aplicación lineal inyectiva y sobreyectiva. Un automorfismo es un isomorfismo donde dominio y codominio son el mismo espacio \\(V\\). Algunas propiedades básicas de los isomorfismos La composición de isomorfismos es un isomorfismo. La aplicación inversa de un isomorfismo es también un isomorfismo Cuando, además, estamos hablando de espacios de dimensión finita, tenemos los siguientes resultados: Una aplicación lineal \\(f:V\\to W\\) es un isomorfismo si, y sólo si, \\(\\mathrm{dim}(V) = \\mathrm{dim}(W)\\). Una aplicación \\(f:V\\to V\\) es un automorfismo si, y sólo si, o bien es inyectiva o bien es sobreyectiva. Este último resultado nos dice que en un endomorfismo, basta con ver si la aplicación es inyectiva o sobreyectiva para tener que es un isomorfismo. Ejemplo Si miramos las aplicaciones \\(f\\) y \\(g\\) de las secciones anteriores, entonces podemos confirmar que \\(g\\) es un automorfismo, mientras que \\(f\\) no lo es, ya que no es ni inyectiva ni sobreyectiva. 3.9 Cómo determinar la imagen de un subespacio vectorial Este problema es realmente un caso particular de cómo calcular la imagen de una aplicación lineal. Tanto en esa sección como en la de propiedades, vimos que si \\(U\\) es un subespacio vectorial generado por los vectores de \\(\\mathcal{G}=\\{u_1,\\ldots,u_k\\}\\), entonces \\(f(\\mathcal{G}) = \\{f(u_1),\\ldots,f(u_k)\\}\\) es un sistema generador de \\(f(U)\\). Por tanto, si deseamos calcular la imagen de un subespacio \\(U\\), los pasos a seguir serán: A partir de sus ecuaciones (si se dan), encontrar un sistema generador o una base de \\(U\\). Calcular la imagen, mediante la aplicación \\(f\\), de los vectores de la base de \\(U\\). Por lo anterior, esto es un sistema generador de \\(f(U)\\). Eliminar los vectores linealmente dependientes de ese sistema generador, para obtener una base de \\(f(U)\\). Ejemplo Consideremos el subespacio \\(U\\) dado por \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) \\in\\mathbb{R}^{3}:\\begin{array}{ccr} -x- \\frac{1}{2}y+ z &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] Queremos calcular \\(f(U)\\) y \\(g(U)\\), siendo \\(f\\) y \\(g\\) las de los ejemplos anteriores. El primer paso es calcular una base de \\(U\\). Para ello, pasamos las ecuaciones cartesianas de \\(U\\) a paramétricas, que nos dan un sistema generador, resolviendo el sistema de ecuaciones por Gauss. A partir del sistema generador, ya podríamos proceder, o podemos calcular una base de \\(U\\). En nuestro caso, \\[\\mathcal{B}_U = \\left\\{\\left( \\begin{array}{c@{}} -2\\\\ 2\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Comenzamos por calcular \\(f(U)\\). Debemos calcular la imagen de los vectores de la base de \\(U\\) con la aplicación \\(f\\): \\[f(\\mathcal{B}_U) = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -6\\\\ -7 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 2\\\\ 1\\\\ 3 \\end{array} \\right) \\right\\}\\] Si eliminamos los vectores linealmente dependientes de \\(f(\\mathcal{B}_U)\\), nos queda una base de \\(f(U)\\): \\[\\mathcal{B}_{f(U)} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -6\\\\ -7 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 2\\\\ 1\\\\ 3 \\end{array} \\right) \\right\\}\\] Repetimos el proceso con la aplicación \\(g\\). Calculamos: \\[g(\\mathcal{B}_U) = \\left\\{\\left( \\begin{array}{c@{}} -6\\\\ 4\\\\ -8 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 2\\\\ 1\\\\ 6 \\end{array} \\right) \\right\\}\\] Como sabemos que \\(g\\) es un isomorfismo, sabemos que la imagen de un sistema linealmente independiente es también un sistema linealmente independiente. Luego \\(\\mathcal{B}_{g(U)} = g(\\mathcal{B}_U)\\). Si no recordáramos este resultado, deberíamos intentar eliminar los vectores linealmente dependientes en \\(g(\\mathcal{B}_U)\\) para así obtener la base. 3.10 A qué se llama rango y nulidad de la aplicación lineal Se llama rango de una aplicación lineal \\(f:V\\to W\\) a la dimensión de la imagen de \\(f\\): \\(\\mathrm{rg}(f) = \\mathrm{dim}(\\mathrm{Im}\\ f)\\). Se llama nulidad de una aplicación lineal \\(f:V\\to W\\) a la dimensión del núcleo de \\(f\\): \\(\\mathrm{nul}(f) = \\mathrm{dim}(\\mathrm{Ker}\\ f)\\). De forma análoga, se pueden definir rango y nulidad para matrices, puesto que tenemos una importante relación entre matrices y aplicaciones lineales. Dada una matriz \\(A\\), su rango \\(\\mathrm{rg}(A)\\) es la dimensión del subespacio generado por los vectores que forman sus columnas, y su nulidad \\(\\mathrm{nul}(A)\\) es la dimensión del subespacio vectorial de las soluciones del sistema de ecuaciones lineales \\(A\\ v = 0\\). Un resultado importante, debido a que todas las matrices asociadas a una misma aplicación lineal son equivalentes, es que todas tienen el mismo rango y la misma nulidad, y coinciden con el ranog y nulidad de \\(f\\). Por tanto, para calcular rango y nulidad de \\(f\\), bastará con estudiar el rango y nulidad de cualquiera de sus expresiones matriciales en unas bases fijadas de \\(V\\) y \\(W\\). Ejemplo Vamos a calcular el rango y la nulidad de las aplicaciones \\(f\\) y \\(g\\) que estamos estudiando en los ejemplos anteriores. El caso fácil es el de \\(g\\), que era un isomorfismo. Luego: \\(\\mathrm{nul}(g) = \\mathrm{dim}(\\mathrm{Ker}\\ g) = \\mathrm{dim}(\\{0\\}) = 0\\), por ser \\(g\\) inyectiva. \\(\\mathrm{rg}(g) = \\mathrm{dim}(\\mathrm{Im}\\ g) = \\mathrm{dim}(W) = 3\\), por ser \\(g\\) sobreyectiva. Y lo mismo se aplica para sus matrices asociadas. En cuanto a \\(f\\), recopilando la información que tenemos: Su nulidad es la dimensión de su núcleo: \\(\\mathrm{nul}(f) = \\mathrm{dim}(\\mathrm{Ker}\\ f) = 1\\). Su rango es la dimensión de su imagen: \\(\\mathrm{rg}(f) = \\mathrm{dim}(\\mathrm{Im}\\ f) = 2\\) 3.11 Qué dice el teorema de la dimensión para núcleo e imagen El resultado teórico que nos relaciona las dimensiones de núcleo e imagen de una aplicación lineal \\(f:V\\to W\\), es decir, su nulidad y su rango, es el siguiente: Si \\(f:V\\to W\\) es una aplicación lineal, entonces \\[\\mathrm{dim}(V) = \\mathrm{dim}(\\mathrm{Ker}\\ f) + \\mathrm{dim}(\\mathrm{Im}\\ f) = \\mathrm{nul}(f) + \\mathrm{rg}(f)\\] Este resultado, reescrito en términos matriciales, queda como: Si \\(A\\) es una matriz con \\(m\\) filas y \\(n\\) columnas, entonces \\[n = \\mathrm{nul}(A) + \\mathrm{rg}(A)\\] Ejemplo Vamos a comprobar el teorema de la dimensión para las aplicaciones lineales \\(f\\) y \\(g\\) de los ejemplos anteriores. Para \\(f\\): \\[\\begin{array}{ccccc} \\mathrm{dim}(\\mathbb{R}^{3}) &amp; = &amp; \\mathrm{dim}(\\mathrm{Ker}\\ f) &amp; + &amp; \\mathrm{dim}(\\mathrm{Im}\\ f) \\\\ 3 &amp; = &amp; 1 &amp; + &amp; 2 \\\\ \\end{array}\\] Para \\(g\\): \\[\\begin{array}{ccccc} \\mathrm{dim}(\\mathbb{R}^{3}) &amp; = &amp; \\mathrm{dim}(\\mathrm{Ker}\\ g) &amp; + &amp; \\mathrm{dim}(\\mathrm{Im}\\ g) \\\\ 3 &amp; = &amp; 0 &amp; + &amp; 3 \\\\ \\end{array}\\] "],
["diagonalización.html", "4 Diagonalización 4.1 Qué son los autovalores de un endomorfismo y de una matriz 4.2 Cómo calculamos los autovectores asociados a un autovalor 4.3 Cómo sabemos si un endomorfismo o una matriz es diagonalizable 4.4 Si una matriz es diagonalizable, cómo podemos hallar sus potencias 4.5 Para qué podemos utilizar el Teorema de Cayley-Hamilton", " 4 Diagonalización En este capítulo nos vamos a centrar en el estudio de endomorfismos, es decir, de aplicaciones lineales \\(f:V\\to V\\) donde el dominio y el codominio son el mismo espacio vectorial sobre un cuerpo \\(\\mathbb{K}\\) (generalmente \\(\\mathbb{K} = \\mathbb{R}\\) o \\(\\mathbb{K} = \\mathbb{C}\\)). Una de las propiedades que hemos estudiado es que si tenemos una base \\(\\mathcal{B}\\) fija en \\(V\\), entonces podemos encontrar una matriz \\(A\\), asociada a \\(f\\), tal que podemos expresar las coordenadas de \\(f(v)\\) en dicha base como \\(A\\ v\\), sea cual sea \\(v\\in V\\). ¿Qué ocurre si fijamos otra base distinta \\(\\mathcal{B}&#39;\\) en \\(V\\)? En ese caso, la matriz asociada al endomorfismo \\(f\\) será \\(A&#39; = P^{-1}\\ A\\ P\\), donde \\(P\\) es la matriz de cambio de base de \\(\\mathcal{B&#39;}\\) a \\(\\mathcal{B}\\). Esta relación especial entre \\(A\\) y \\(A&#39;\\) tiene nombre: son matrices semejantes. Dos matrices \\(A,A&#39;\\in\\mathcal{M}_n(\\mathbb{K})\\) se dicen semejantes si existe \\(P\\in\\mathcal{M}_n(\\mathbb{K})\\) regular tal que \\(A&#39;=P^{-1}\\ A\\ P\\). Esto tiene implicaciones como que los cálculos que podamos necesitar hacer con un endomorfismo, como calcular su núcleo y su imagen, o los que veremos en este tema, lo podemos hacer con cualquiera de sus matrices asociadas, respecto a diferentes bases, ya que son todas semejantes. Sin embargo, para ciertas operaciones, podremos necesitar tener una matriz asociada a \\(f\\) lo más simple posible. ¿Qué entendemos por una matriz simple? Una matriz diagonal. La diagonalización es el proceso por el cual pretendemos encontrar una base \\(\\mathcal{B}\\) de \\(V\\) tal que la matriz asociada a un endomorfismo \\(f\\) sea diagonal. No todos los endomorfismos serán diagonalizables, así que veremos cuándo podremos asegurar que sí lo son, y el mecanismo para encontrar la forma diagonal de la matriz asociada al endomorfismo. Nota: A menudo, abusaremos de la notación y usaremos indistintamente \\(v\\) para designar a un vector o a sus coordenadas con respecto a la base canónica. Ejemplo Consideremos el siguiente endomorfismo \\(f:\\mathbb{R}^{3}\\to\\mathbb{R}^{3}\\), dado por \\[f\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left(\\begin{array}{c} 9x+ 6y+ 6z\\\\ -2x+ y-2z\\\\ -10x-10y-7z\\\\ \\end{array}\\right)\\] Es claro que su matriz asociada en la base canónica es: \\[A = \\left( \\begin{array}{ccc@{}} 9 &amp; 6 &amp; 6\\\\ -2 &amp; 1 &amp; -2\\\\ -10 &amp; -10 &amp; -7 \\end{array} \\right) \\] Si consideramos la siguiente base: \\[\\mathcal{B} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{3}{5}\\\\ \\frac{1}{5}\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] entonces la matriz de cambio de \\(\\mathcal{B}\\) a la canónica \\(\\mathcal{C}\\) es: \\[P = \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) \\] y la matriz asociada a \\(f\\) en la nueva base es: \\[\\begin{array}{rcl}A&#39; &amp; = &amp; \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) ^{-1}\\ \\left( \\begin{array}{ccc@{}} 9 &amp; 6 &amp; 6\\\\ -2 &amp; 1 &amp; -2\\\\ -10 &amp; -10 &amp; -7 \\end{array} \\right) \\ \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) = \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} -3 &amp; 0 &amp; 0\\\\ 0 &amp; 3 &amp; 0\\\\ 0 &amp; 0 &amp; 3 \\end{array} \\right) \\end{array}\\] En este capítulo, veremos el método para hallar esa descomposición, la base y la matriz diagonal. ¿Qué preguntas vamos a responder en este capítulo? ¿Qué son los autovalores de un endomorfismo y de una matriz? ¿Cómo calculamos los autovectores asociados a un autovalor? ¿Cómo sabemos si un endomorfismo o una matriz es diagonalizable? ¿Si una matriz es diagonalizable, cómo podemos hallar sus potencias? ¿Para qué podemos utilizar el Teorema de Cayley-Hamilton? 4.1 Qué son los autovalores de un endomorfismo y de una matriz Consideremos un endomorfismo \\(f:V\\to V\\). Diremos que un valor \\(\\lambda\\in\\mathbb{K}\\) es un autovalor (o valor propio) de \\(f\\) si existe un vector no nulo \\(v\\in V\\) tal que \\(f(v) = \\lambda\\ v\\). A ese \\(v\\) se le denomina autovector (o vector propio) asociado a \\(\\lambda\\). Nota: Eliminamos la posibilidad de que \\(v\\) sea nulo en la definición, porque sabemos que \\(f(0) = 0\\) en cualquier aplicación lineal, así que no nos proporcionaría información alguna. Intuitivamente, un autovector \\(v\\) es un vector no nulo que, al aplicarle \\(f\\), su imagen \\(f(v)\\) es proporcional a él mismo, y la constante de proporcionalidad es el autovalor \\(\\lambda\\). Si consideramos una matriz \\(A\\) asociada a \\(f\\), la condición \\(f(v) = \\lambda\\ v\\) se puede reescribir como \\(A\\ v = \\lambda\\ v\\). Podemos generalizar entonces esta definición a matrices cuadradas: Un escalar \\(\\lambda\\) se llama autovalor o valor propio de una matriz \\(A\\in\\mathcal{M}_n(\\mathbb{K})\\) si existe \\(v\\in\\mathbb{K}^{n}\\) no nulo, llamado autovector o vector propio asociado a \\(\\lambda\\), tal que \\(A\\ v = \\lambda\\ v\\). Nos falta comprobar si hay alguna incoherencia en las definiciones: si distintas matrices asociadas a un mismo endomorfismo tuvieran distintos autovalores, eso sería un problema de la definición. Sin embargo, el siguiente resultado nos asegura que esto no puede ser así: Si dos matrices \\(A\\) y \\(A&#39;\\) son semejantes, entonces tienen los mismos autovalores. Podemos decir entonces que: Si \\(\\lambda\\) es un autovalor de \\(f\\), entonces lo es de cualquiera de sus matrices asociadas en diferentes bases, y esto es debido a su relación de semejanza. Esto significa que si queremos calcular los autovalores de un endomorfismo \\(f\\), basta con calcularlos para cualquiera de sus matrices asociadas, en particular para su matriz asociada en la base canónica, la más sencilla de encontrar. ¿Cómo podemos encontrar los autovalores de un endomorfismo o de su matriz asociada? Para encontrar los autovalores de un endomorfismo, siempre partiremos de una matriz asociada al mismo. Por tanto, explicaremos cómo calcular los autovalores de una matriz cuadrada, y eso nos bastará para encontrar los del endomorfismo, por lo comentado anteriormente. La condición para ser \\(\\lambda\\in\\mathbb{K}\\) un autovalor es que exista \\(v\\in V\\) tal que \\(A\\ v = \\lambda\\ v\\). Esto es equivalente a \\(A\\ v - \\lambda v = 0\\), es decir, \\(A\\ v - \\lambda\\ I\\ v = 0\\), siendo \\(I\\) la matriz identidad del mismo tamaño que \\(v\\). En forma compacta, podemos decir que \\(\\lambda\\) es autovalor de \\(A\\) si existe \\(v\\) tal que \\((A-\\lambda\\ I)\\ v = 0\\). Es decir, \\(v\\) debe ser solución del siguiente sistema homogéneo: \\((A-\\lambda\\ I)\\ x = 0\\). Recordemos que un sistema homogéneo tiene siempre solución. Estamos interesados en soluciones distintas del vector 0, puesto que hemos impuesto en la definición de autovector que sea un vector no nulo. ¿Qué condición tenemos para saber que un sistema homogéneo es compatible indeterminado? Pues que la matriz de coeficientes del sistema sea singular, es decir, tenga determinante igual a 0. En este caso, la matriz de coeficientes del sistema es \\(A-\\lambda\\ I\\), así que tendremos soluciones distinta de la trivial si \\(\\mathrm{det}(A-\\lambda\\ I) = 0\\). Hay que notar que la expresión \\(\\mathrm{det}(A-\\lambda\\ I)\\) depende de \\(\\lambda\\), así que realmente estamos buscando los valores de \\(\\lambda\\) que hacen que \\(\\mathrm{det}(A-\\lambda\\ I) = 0\\). Llamamos polinomio característico de la matriz \\(A\\) a \\(p(\\lambda) = \\mathrm{det}(A-\\lambda\\ I)\\), y ecuación característica a \\(\\mathrm{det}(A-\\lambda\\ I) = 0\\). Por tanto, los autovalores de una matriz \\(A\\) son soluciones a la ecuación característica, es decir, las raíces del polinomio característico. Ejemplo Vamos a calcular los autovalores del endomorfismo \\(f:\\mathbb{R}^{3}\\to\\mathbb{R}^{3}\\), dado por \\[f\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left(\\begin{array}{c} 9x+ 6y+ 6z\\\\ -2x+ y-2z\\\\ -10x-10y-7z\\\\ \\end{array}\\right)\\] del ejemplo anterior. Sabemos que su matriz asociada en la base canónica es: \\[A = \\left( \\begin{array}{ccc@{}} 9 &amp; 6 &amp; 6\\\\ -2 &amp; 1 &amp; -2\\\\ -10 &amp; -10 &amp; -7 \\end{array} \\right) \\] Definimos entonces el polinomio característico como: \\[\\begin{array}{rcl}p(\\lambda) &amp; = &amp; \\mathrm{det}(A - \\lambda\\ I) = \\\\ &amp; = &amp; \\mathrm{det}\\left(\\left( \\begin{array}{ccc@{}} 9 &amp; 6 &amp; 6\\\\ -2 &amp; 1 &amp; -2\\\\ -10 &amp; -10 &amp; -7 \\end{array} \\right) - \\lambda \\left( \\begin{array}{ccc@{}} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right) \\right) = \\\\ &amp; = &amp; \\mathrm{det}\\left( \\begin{array}{ccc@{}} 9-\\lambda &amp; 6 &amp; 6\\\\ -2 &amp; 1-\\lambda &amp; -2\\\\ -10 &amp; -10 &amp; -7-\\lambda \\end{array} \\right) \\end{array}\\] Si desarrollamos el determinante, nos queda como polinomio característico: \\[p(\\lambda) = \\lambda^{3}-3\\lambda^{2}-9\\lambda+27\\] Igualándolo a 0, tenemos la ecuación característica \\(\\lambda^{3}-3\\lambda^{2}-9\\lambda+27 = 0\\), que podemos resolver (generalmente se podrá factorizar de forma simple o usando Ruffini) y nos quedan las siguientes soluciones: \\(\\lambda = -3, 3\\). Esta última solución es raíz doble, ya que: \\[p(\\lambda) = (\\lambda +3)\\cdot (\\lambda -3)^{2}\\] Por tanto, \\(\\lambda = -3, 3\\) son autovalores de la matriz \\(A\\) y del endomorfismo \\(f\\). 4.2 Cómo calculamos los autovectores asociados a un autovalor Acabamos de ver que los autovectores asociados a un autovalor \\(\\lambda\\) son soluciones no triviales del sistema homogéneo \\((A-\\lambda\\ I)x = 0\\). También sabemos que las soluciones de un sistema homogéneo forman un subespacio vectorial. Por tanto, el conjunto de autovectores asociado a un autovalor \\(\\lambda\\) es un subespacio vectorial que llamamos subespacio asociado al autovalor \\(\\lambda\\). Generalmente, denotaremos \\(U_{\\lambda}\\) al subespacio asociado al autovalor \\(\\lambda\\). En ocasiones, se denomina subespacio invariante por \\(f\\) a \\(U_{\\lambda}\\), puesto que \\(f(U_{\\lambda}) \\subseteq U_{\\lambda}\\). Como con cualquier subespacio vectorial, para tener perfectamente determinado a \\(U_{\\lambda}\\) nos basta con dar una base suya que, evidentemente, estará formada por autovectores asociados a \\(\\lambda\\). En este caso, podemos partir de las ecuaciones cartesianas de \\(U_{\\lambda}\\), que son el sistema \\((A-\\lambda\\ I)\\ x = 0\\), y encontrar una base suya, como ya hemos visto en el capítulo de espacios vectoriales. Ejemplo Retomamos el endomorfismo \\(f\\) de los ejemplos anteriores. Calculemos los subespacios asociados a los distintos autovalores: Para el autovalor \\(\\lambda = -3\\): \\[\\begin{array}{rcl}(A + 3 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = 0 &amp; \\Leftrightarrow &amp; \\left( \\begin{array}{ccc@{}} 12 &amp; 6 &amp; 6\\\\ -2 &amp; 4 &amp; -2\\\\ -10 &amp; -10 &amp; -4 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = 0 \\Leftrightarrow\\quad\\\\&amp; \\Leftrightarrow &amp;\\left\\{\\begin{array}{rrrcr} 12x &amp; + 6y &amp; + 6z &amp; = &amp; 0\\\\ -2x &amp; + 4y &amp; -2z &amp; = &amp; 0\\\\ -10x &amp; -10y &amp; -4z &amp; = &amp; 0\\\\ \\end{array}\\right. \\\\\\end{array}\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de su conjunto solución, que es \\(U_{ -3 }\\): \\[\\left( \\begin{array}{ccc|c} 12 &amp; 6 &amp; 6 &amp; 0\\\\ -2 &amp; 4 &amp; -2 &amp; 0\\\\ -10 &amp; -10 &amp; -4 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{ccc|c} 12 &amp; 0 &amp; \\frac{36}{5} &amp; 0\\\\ 0 &amp; 5 &amp; -1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\]\\[\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} - \\frac{3}{5}\\\\ \\frac{1}{5}\\\\ 1 \\end{array}\\right) {}\\] De aquí que una base de \\(U_{ -3 }\\) sea: \\[\\mathcal{B}_{U_{-3}} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{3}{5}\\\\ \\frac{1}{5}\\\\ 1 \\end{array} \\right) \\right\\}\\]- Para el autovalor \\(\\lambda = 3\\): \\[\\begin{array}{rcl}(A -3 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = 0 &amp; \\Leftrightarrow &amp; \\left( \\begin{array}{ccc@{}} 6 &amp; 6 &amp; 6\\\\ -2 &amp; -2 &amp; -2\\\\ -10 &amp; -10 &amp; -10 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = 0 \\Leftrightarrow\\quad\\\\&amp; \\Leftrightarrow &amp;\\left\\{\\begin{array}{rrrcr} 6x &amp; + 6y &amp; + 6z &amp; = &amp; 0\\\\ -2x &amp; -2y &amp; -2z &amp; = &amp; 0\\\\ -10x &amp; -10y &amp; -10z &amp; = &amp; 0\\\\ \\end{array}\\right. \\\\\\end{array}\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de su conjunto solución, que es \\(U_{ 3 }\\): \\[\\left( \\begin{array}{ccc|c} 6 &amp; 6 &amp; 6 &amp; 0\\\\ -2 &amp; -2 &amp; -2 &amp; 0\\\\ -10 &amp; -10 &amp; -10 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{ccc|c} 6 &amp; 6 &amp; 6 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\]\\[\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} -1\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} -1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De aquí que una base de \\(U_{ 3 }\\) sea: \\[\\mathcal{B}_{U_{3}} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Los vectores de las bases encontradas, así como cualquiera de sus múltiplos, son autovectores asociados al correpondiente autovalor \\(\\lambda\\). 4.3 Cómo sabemos si un endomorfismo o una matriz es diagonalizable Una vez que sabemos cómo hallar los autovalores y los autovectores de una matriz y de un endomorfismo, vamos a plantearnos cómo utilizarlos para saber si podemos encontrar una base \\(\\mathcal{B}\\) del espacio \\(V\\) tal que la matriz asociada al endomorfismo \\(f\\) es diagonal. Es decir, nos planteamos el estudio de si \\(f\\) es diagonalizable. El principal resultado teórico que necesitamos es el siguiente: Un endomorfismo \\(f:V\\to V\\) es diagonalizable si y sólo si existe una base \\(\\mathcal{B}\\) de \\(V\\) formada únicamente por autovectores de \\(f\\). En ese caso, la matriz asociada \\(D\\) en esa base es diagonal y tiene en su diagonal principal los autovalores de \\(f\\). Además, la matriz de cambio de base \\(P_{\\mathcal{B}\\to\\mathcal{C}}\\) (formada por los vectores de \\(\\mathcal{B}\\) por columnas) verifica: \\[P_{\\mathcal{B}\\to\\mathcal{C}}^{-1}\\ A\\ P_{\\mathcal{B}\\to\\mathcal{C}} = D\\] o, equivalentemente, \\[A = P_{\\mathcal{C}\\to\\mathcal{B}}^{-1}\\ D\\ P_{\\mathcal{C}\\to\\mathcal{B}}\\] Este resultado proporciona mucha información: nos dice que para que sea diagonalizable un endomorfismo (o una matriz), necesitamos una base de \\(V\\) formada por únicamente autovectores, y que la matriz diagonal asociada tendrá a los autovalores en la diagonal principal. Nos preguntamos entonces bajo qué condiciones podemos asegurar que tenemos una base de \\(V\\) formada enteramente por autovectores. El siguiente resultado es de gran ayuda: Autovectores asociados a distintos autovalores son linealmente independientes. Supongamos que llamamos \\(\\lambda_1,\\ldots,\\lambda_k\\) a los autovalores de un endomorfismo o de una matriz, y \\(U_{\\lambda_1},\\ldots,U_{\\lambda_k}\\) a los subespacios asociados a los autovalores. Supongamos que todos los \\(\\lambda_i\\) son distintos unos de otros. El resultado anterior me asegura que si \\(i\\ne j\\), entonces los vectores de las bases de \\(U_{\\lambda_i}\\) y de \\(U_{\\lambda_j}\\) son linealmente independientes. Llamamos \\(\\mathcal{B} = \\mathcal{B}_{U_{\\lambda_1}}\\cup \\ldots\\cup\\mathcal{B}_{U_{\\lambda_k}}\\), es un sistema linealmente independiente por lo que acabamos de comentar. Si, además, \\(\\mathcal{B}\\) tuviera \\(n\\) elementos, puesto que \\(\\mathrm{dim}(V) = n\\), entonces \\(\\mathcal{B}\\) sería una base de \\(V\\) (recordemos que una base es un sistema linealmente independiente con tantos elementos como la dimensión del espacio vectorial). Por tanto, si unimos todas las bases de los subespacios asociados a los autovalores, y tenemos tantos elementos como la dimensión del espacio \\(V\\), entonces se verifica todo lo anterior: el endomorfismo (y sus matrices asociadas) son diagonalizables. Esto es así porque entonces se verificaría: \\[V = U_{\\lambda_1} + \\ldots + U_{\\lambda_k}\\] La primera consecuencia de esto es: Si una matriz o un endomorfismo tienen exactamente \\(n\\) autovalores distintos, entonces es diagonalizable. Esto es así porque cada uno de los \\(n\\) autovalores tendría un subespacio asociado cuya base es linealmente independiente de la base de los demás subespacios. Esto solo podría suceder si cada una de las \\(n\\) bases \\(\\mathcal{B}_{U_{\\lambda_i}}\\), (\\(i = 1,\\ldots,n\\)) solo tiene un elemento, es decir, si \\(\\mathrm{dim}(U_{\\lambda_i}) = 1\\) para todo \\(i=1,\\ldots,n\\). Si no fuera así, acabaríamos con más de \\(n\\) vectores linealmente independientes en \\(V\\), lo cual es imposible. Entonces \\(\\mathcal{B}\\), definida como la unión de las bases mencionadas, tiene exactamente \\(n\\) elementos. Esto nos demuestra que la matriz o el endomorfismo con \\(n\\) autovalores distintos es diagonalizable. Pero ¿qué pasa cuando no tenemos \\(n\\) autovalores distintos? Es de gran importancia conocer, para todo \\(i = 1,\\ldots,k\\), la dimensión del subespacio asociado al autovalor \\(\\lambda_i\\). Llamaremos multiplicidad geométrica del autovalor \\(\\lambda\\) a \\(\\mathrm{m_g}(\\lambda) = \\mathrm{dim}(U_{\\lambda_i})\\). Se tiene el siguiente resultado: Para todo autovalor \\(\\lambda\\) de un endomorfismo o de una matriz, se tiene que \\[1\\le\\mathrm{m_g}(\\lambda)\\le\\mathrm{m_a}(\\lambda)\\] donde \\(\\mathrm{m_g}(\\lambda)\\) es su multiplicidad geométrica y \\(\\mathrm{m_a}(\\lambda)\\) es la multiplicidad algebraica de \\(\\lambda\\), es decir, su multiplicidad como raíz del polinomio característico. Es decir, la multiplicidad geométrica siempre es mayor o igual que 1 (indicando que el subespacio \\(U_{\\lambda}\\) no es el trivial \\(\\{0\\}\\)), y menor que la multiplicidad algebraica. Un corolario de todos los resultados anteriores es: Si, para todo autovalor \\(\\lambda\\) de un endomorfismo o de una matriz cuadrada, se tiene que \\(\\mathrm{m_g}(\\lambda) = \\mathrm{m_a}(\\lambda)\\), entonces es diagonalizable. Y el recíproco es también cierto. Aquí tenemos un criterio sencillo para determinar si un endomorfismo o una matriz son diagonalizables: Si tenemos \\(n\\) autovalores distintos, entonces es diagonalizable. Si tenemos menos de \\(n\\) autovalores distintos, debemos comprobar que las multiplicidades geométricas de todos los autovalores coincidan con sus multiplicidades algebraicas. En ambos casos, la base de \\(V\\) definida por \\(\\mathcal{B} = \\mathcal{B_{U_{\\lambda_1}}}\\cup\\ldots\\cup\\mathcal{B_{U_{\\lambda_k}}}\\) es una base formada por autovectores y la matriz asociada al endomorfismo \\(f\\) en dicha base es diagonal con los autovalores en la diagonal principal (repetidos según su multiplicidad). Si \\(P\\) es la matriz de paso de \\(\\mathcal{B}\\) a la canónica \\(\\mathcal{C}\\), entonces la expresión matricial del endomorfismo \\(f\\) en dicha base \\(\\mathcal{B}\\) es \\(D = P^{-1}\\ A\\ P\\). Ejemplo Consideramos el endomorfismo \\(f\\) de los ejemplos anteriores. Si tenemos en cuenta lo que se ha determinado de sus autovalores y de los subespacios asociados, tenemos la siguiente tabla resumen: \\[ \\begin{array}{ccc@{}} \\text{Autovalor} &amp; \\text{Mult. Algebraica} &amp; \\text{Mult. Geométrica}\\\\ -3 &amp; 1 &amp; 1\\\\ 3 &amp; 2 &amp; 2 \\end{array} \\] Con esto, queda demostrado que el endomorfismo \\(f\\) sí es diagonalizable. Además, la base \\(\\mathcal{B}\\) de autovectores es: \\[\\mathcal{B} = \\mathcal{B}_{-3}\\cup\\ \\mathcal{B}_{3} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{3}{5}\\\\ \\frac{1}{5}\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] La matriz de cambio de base de \\(\\mathcal{B}\\) a \\(\\mathcal{C}\\) es la que tiene los elementos de \\(\\mathcal{B}\\) por columnas: \\[P = \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) \\] Y la matriz diagonal es la que tiene los autovalores en la diagonal, que se calcula como: \\[\\begin{array}{rcl}D &amp; = &amp; P^{-1}\\ A\\ P = \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) ^{-1}\\left( \\begin{array}{ccc@{}} 9 &amp; 6 &amp; 6\\\\ -2 &amp; 1 &amp; -2\\\\ -10 &amp; -10 &amp; -7 \\end{array} \\right) \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} -3 &amp; 0 &amp; 0\\\\ 0 &amp; 3 &amp; 0\\\\ 0 &amp; 0 &amp; 3 \\end{array} \\right) \\end{array}\\] 4.4 Si una matriz es diagonalizable, cómo podemos hallar sus potencias Consideremos una matriz cuadrada diagonalizable. En concreto, eso significa que existe otra matriz cuadrada \\(P\\) regular (cuyas columnas son los vectores de la base de autovectores) tal que \\[A = P^{-1}\\ D\\ P\\] donde \\(D\\) es la matriz diagonal con los autovalores de \\(A\\) en la diagonal principal. Supongamos que queremos hallar \\(A^k\\), con \\(k &gt; 1\\) entero. Si usamos la expresión anterior, nos queda: \\[A^k = A\\cdot A\\cdot\\ldots A = (P^{-1}\\ D\\ P) \\cdot (P^{-1}\\ D\\ P) \\cdot\\ldots (P^{-1}\\ D\\ P)\\] donde los productos se repiten \\(k\\) veces. Si agrupamos cada par \\(P\\cdot P^{-1}\\), que es igual a la identidad, entonces nos queda: \\[A^k = P^{-1}\\ D^k\\ P\\] Como \\(D\\) es una matriz diagonal con los autovalores, es fácil calcular su potencia \\(k\\)-ésima: \\[ \\left( \\begin{array}{cccc} \\lambda_1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; \\lambda_n \\\\ \\end{array}\\right)^k = \\left( \\begin{array}{cccc} \\lambda_1^k &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\lambda_2^k &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; \\lambda_n^k \\\\ \\end{array}\\right) \\] De esta forma, calcular la potencia de \\(A\\) se reduce a calcular la potencia \\(k\\)-ésima de la matriz \\(D\\). Esto es especialmente útil para calcular potencias altas de \\(A\\). Ejemplo Calculemos \\(A^{100}\\) donde \\(A\\) es la matriz del ejemplo anterior. Conocemos su descomposición: \\[\\begin{array}{rcl}A &amp; = &amp; P\\ D\\ P^{-1} = \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) \\left( \\begin{array}{ccc@{}} -3 &amp; 0 &amp; 0\\\\ 0 &amp; 3 &amp; 0\\\\ 0 &amp; 0 &amp; 3 \\end{array} \\right) \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) ^{-1} \\\\\\end{array}\\] Entonces, por lo comentado anteriormente, \\(A^{100} = P\\ D^{100}\\ P^{-1}\\), es decir: \\[\\begin{array}{rcl}A^{100} &amp; = &amp; P D^{100} P^{-1} = \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) \\left( \\begin{array}{ccc@{}} (-3)^{100} &amp; 0 &amp; 0\\\\ 0 &amp; 3^{100} &amp; 0\\\\ 0 &amp; 0 &amp; 3^{100} \\end{array} \\right) \\left( \\begin{array}{ccc@{}} - \\frac{3}{5} &amp; -1 &amp; -1\\\\ \\frac{1}{5} &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) ^{-1} \\\\\\end{array}\\] 4.5 Para qué podemos utilizar el Teorema de Cayley-Hamilton El Teorema de Cayley-Hamilton nos dice que: Toda matriz cuadrada \\(A\\) verifica su ecuación característica, es decir, si \\(p(\\lambda)\\) es el polinomio característico de \\(A\\), entonces \\(p(A) = 0\\). Supongamos que el polinomio característico de una matriz \\(A\\) es \\[p(\\lambda) = c_n\\lambda^n+\\ldots+c_1\\lambda+c_0\\] El teorema nos dice que \\(A\\) hace cero su polinomio característico: \\[p(A) = c_n A^n + \\ldots + c_1 A + c_0 I = 0\\] donde \\(A^k\\) representa el producto de \\(A\\) por sí misma \\(k\\) veces. ¿Qué utilidades tiene este teorema? Supongamos que el término independiente del polinomio característico, \\(c_0\\) es no nulo (esto equivale a que 0 no sea un autovalor, lo cual es también equivalente a que \\(A\\) sea regular). Si de la igualdad superior despejamos la identidad, \\(I\\), nos queda lo siguiente: \\[ \\begin{array}{rcl} I &amp; = &amp; -\\dfrac{1}{c_0}\\left(c_nA^n+\\ldots+c_1A\\right) = \\\\ &amp; = &amp; -\\dfrac{1}{c_0}\\left(c_nA^{n-1}+\\ldots+c_1\\ I\\right)\\ A \\end{array} \\] De aquí deducimos que \\[A^{-1} = -\\dfrac{1}{c_0}\\left(c_nA^{n-1}+\\ldots+c_1\\ I\\right)\\] Por tanto, el teorema de Cayley-Hamilton nos da una forma de calcular la inversa de una matriz cuadrada \\(A\\), únicamente a partir de sumas de potencias de la matriz. Esta misma estrategia nos proporciona un método para calcular potencias de la matriz \\(A\\), aparte del ya explicado, reduciendo el problema a potencias de \\(A\\) de grado menor que el tamaño de la propia matriz. Si, de la igualdad del teorema, despejamos el término \\(A^n\\), nos queda \\[A^n = -\\frac{1}{c_n}\\left(-c_{n-1}A^{n-1}-\\ldots-c_1 A- c_0 I\\right)\\] A partir de \\(A^n\\), siendo \\(n\\) el número de filas o columnas de la matriz \\(A\\), todas sus potencias se pueden poner como suma de potencias de \\(A\\) de grado menor estrictamente que \\(n\\): \\[ \\begin{array}{rcl} A^{n+1} &amp; = &amp; A\\cdot A^n = \\\\ &amp; = &amp; A \\cdot \\left(-\\frac{1}{c_n}\\left(-c_{n-1}A^{n-1}-\\ldots-c_1 A- c_0 I\\right)\\right) = \\\\ &amp; = &amp; -\\frac{1}{c_n}\\left(-c_{n-1}A^n-\\ldots-c_1 A^2- c_0 A\\right) = \\\\ &amp; = &amp; -\\frac{1}{c_n}\\left(-c_{n-1}\\left(-\\frac{1}{c_n}\\left(-c_{n-1}A^{n-1}-\\ldots-c_1 A- c_0 I\\right)\\right)-\\ldots-c_1 A^2- c_0 A\\right) = \\\\ &amp; = &amp; ...\\\\ \\end{array} \\] Siguiendo con este patrón, se puede poner cualquier potencia \\(A^m\\), con \\(m\\ge n\\), como suma de \\(A,A^2,\\ldots,A^{n-1}\\). Ejemplo Vamos a considerar la matriz \\[A = \\left( \\begin{array}{ccc@{}} 9 &amp; 6 &amp; 6\\\\ -2 &amp; 1 &amp; -2\\\\ -10 &amp; -10 &amp; -7 \\end{array} \\right) \\] asociada al endomorfismo \\(f\\) de los ejemplos anteriores. Vamos a calcular \\(A^{-1}\\), \\(A^4\\) y \\(A^5\\) usando el Teorema de Cayley-Hamilton. Conocemos su polinomio característico: \\[p(\\lambda) = \\lambda^{3}-3\\lambda^{2}-9\\lambda+27\\] El teorema nos proporciona la siguiente igualdad: \\[p(A) = A^{3}-3A^{2}-9A+27I = 0\\] Despejando la identidad, nos queda: \\[\\begin{array}{rcl}\\displaystyle I &amp; = &amp; \\frac{1}{27} \\left(A^{3}-3A^{2}-9A\\right) = \\\\ &amp; = &amp; \\left(- \\frac{1}{27}A^{2}+\\frac{1}{9}A+\\frac{1}{3}I\\right)\\ A\\end{array}\\] Luego \\[\\begin{array}{rcl}A^{-1} &amp; = &amp; - \\frac{1}{27}A^{2}+\\frac{1}{9}A+\\frac{1}{3}I =\\\\ &amp; = &amp; - \\frac{1}{27}\\left( \\begin{array}{ccc@{}} 9 &amp; 0 &amp; 0\\\\ 0 &amp; 9 &amp; 0\\\\ 0 &amp; 0 &amp; 9 \\end{array} \\right) +\\frac{1}{9}\\left( \\begin{array}{ccc@{}} 9 &amp; 6 &amp; 6\\\\ -2 &amp; 1 &amp; -2\\\\ -10 &amp; -10 &amp; -7 \\end{array} \\right) +\\frac{1}{3}\\left( \\begin{array}{ccc@{}} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right) =\\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} 1 &amp; \\frac{2}{3} &amp; \\frac{2}{3}\\\\ - \\frac{2}{9} &amp; \\frac{1}{9} &amp; - \\frac{2}{9}\\\\ - \\frac{10}{9} &amp; - \\frac{10}{9} &amp; - \\frac{7}{9} \\end{array} \\right) \\end{array}\\] Vamos ahora a calcular \\(A^4\\) y \\(A^5\\) como suma de potencias de \\(A\\) de grado menor que su tamaño, es decir, potencias de \\(A\\) de grado menor que \\(3\\). En todos los casos, tenemos que partir de despejar el término \\(A^{3}\\) de la igualdad que nos proporciona el Teorema de Cayley-Hamilton: \\[A^{3} = 3A^{2}+9A-27I\\] Para \\(A^4\\), tenemos: \\[A^4 = A\\cdot A^3 = A \\left(3A^{2}+9A-27I\\right) = 3A^{3}+9A^{2}-27A\\] Si queremos dejarlo como suma de potencias de \\(A\\) de grado menor que \\(3\\), debemos sustituir aquí \\(A^{3}\\) por la expresión que hemos despejado hace un momento. \\[\\begin{array}{rcl}A^4 &amp; = &amp; 3 \\left( 3A^{2}+9A-27I \\right)+9A^{2}-27A = \\\\ &amp; = &amp; 18A^{2}-81I = \\\\ &amp; = &amp; 18\\left( \\begin{array}{ccc@{}} 9 &amp; 0 &amp; 0\\\\ 0 &amp; 9 &amp; 0\\\\ 0 &amp; 0 &amp; 9 \\end{array} \\right) -81\\left( \\begin{array}{ccc@{}} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right) = \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} 81 &amp; 0 &amp; 0\\\\ 0 &amp; 81 &amp; 0\\\\ 0 &amp; 0 &amp; 81 \\end{array} \\right) \\end{array}\\] Evidentemente, podríamos haber hecho el cálculo bien directamente bien haciendo \\(A^4 = A^2\\cdot A^2\\), pero esa forma no está expresada como suma de potencias de grado menor que \\(n\\). Repetimos el proceso para calcular \\(A^5\\). \\[A^5 = A\\cdot A^4 = A \\left(18A^{2}-81I\\right) = 18A^{3}-81A\\] Sustituimos de nuevo \\(A^{3}\\) por su expresión: \\[\\begin{array}{rcl}A^5 &amp; = &amp; 18 \\left( 3A^{2}+9A-27I \\right)-81A = \\\\ &amp; = &amp; 54A^{2}+81A-486I = \\\\ &amp; = &amp; 54\\left( \\begin{array}{ccc@{}} 9 &amp; 0 &amp; 0\\\\ 0 &amp; 9 &amp; 0\\\\ 0 &amp; 0 &amp; 9 \\end{array} \\right) +81\\left( \\begin{array}{ccc@{}} 9 &amp; 6 &amp; 6\\\\ -2 &amp; 1 &amp; -2\\\\ -10 &amp; -10 &amp; -7 \\end{array} \\right) -486\\left( \\begin{array}{ccc@{}} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right) = \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} 729 &amp; 486 &amp; 486\\\\ -162 &amp; 81 &amp; -162\\\\ -810 &amp; -810 &amp; -567 \\end{array} \\right) \\end{array}\\] "],
["espeuc.html", "5 Espacios Euclídeos 5.1 Qué son las normas y distancias 5.2 Qué significa el concepto de ortogonalidad 5.3 Qué es el complemento ortogonal de un subespacio 5.4 Cómo calculamos la proyección de un vector sobre un subespacio 5.5 Qué es y qué utilidad tiene una base ortonormal 5.6 Cómo construimos una base ortonormal 5.7 Qué es una matriz ortogonal y una aplicación ortogonal 5.8 Qué es la diagonalización ortogonal 5.9 Qué matrices son diagonalizables ortogonalmente", " 5 Espacios Euclídeos En este capítulo, damos un paso para acercarnos a los conceptos geométricos clásicos de distancias y ángulos, abstrayéndolos en el contexto de los espacios vectoriales. Para ello, precisaremos de la noción de producto escalar: Una aplicación \\(g:V\\times V\\to \\mathbb{R}\\), donde \\(V\\) es un espacio vectorial sobre \\(\\mathbb{R}\\), es un producto escalar si verifica: Es simétrica: \\(g(u, v) = g(v, u)\\) para todo \\(u,v\\in V\\). Es bilineal: \\(g(\\alpha u + \\beta v, w) = \\alpha g(u, w) + \\beta g(v, w)\\) para todos los \\(\\alpha,\\beta\\in\\mathbb{R}\\), \\(u,v,w\\in V\\). Por simetría, se tiene lo mismo para la linealidad en la segunda componente. Es definida positiva: \\(g(v, v) &gt; 0\\) para todo \\(v\\in V\\setminus\\{0\\}\\), \\(g(v, v) = 0\\) si, y sólo si \\(v = 0\\). En resumen, se suele definir un producto escalar como una forma bilineal, simétrica y definida positiva. Notación: El producto escalar de dos vectores \\(u\\) y \\(v\\) suele recibir diversas notaciones, aunque las más usadas son \\(u\\cdot v\\) y \\(\\langle u, v \\rangle\\). A un espacio vectorial \\(V\\) donde podemos definir un producto vectorial lo llamamos espacio euclídeo, y lo solemos denotar mediante el par \\(\\left(V, \\langle\\cdot,\\cdot\\rangle\\right)\\). Ejemplo En \\(\\mathbb{R}^n\\), disponemos de un producto escalar usual, de forma que si \\(u, v\\in\\mathbb{R}^n\\), entonces \\[\\langle u,v \\rangle = u_1v_1+u_2v_2+\\ldots+u_nv_n\\] siendo \\(u_i, v_i\\), \\(i=1,\\ldots,n\\) las coordenadas de \\(u\\) y de \\(v\\), respectivamente, en una base dada (la canónica como caso particular). A este producto vectorial es al que nos referiremos cuando usemos la notación \\(u\\cdot v\\). Podemos generalizar este producto escalar, definiendo unos pesos \\(\\alpha_i\\in\\mathbb{R}^{+}\\) para cada componente: \\[\\langle u,v \\rangle = \\alpha_1u_1v_1+\\alpha_2u_2v_2+\\ldots+\\alpha_nu_nv_n\\] Podemos construir productos escalares de la siguiente forma: Tomemos una matriz \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) simétrica y definida positiva. Entonces la aplicación \\(\\langle\\cdot,\\cdot\\rangle:\\mathbb{R}^n\\times\\mathbb{R}^n\\to\\mathbb{R}\\) definida por \\[\\langle u, v \\rangle = u^{\\text{t}}\\ A\\ v\\] es un producto escalar. A esta matriz \\(A\\) se le denomina matriz de Gram, y todo producto escalar tiene asociada una. Los dos ejemplos anteriores se correspondían con tomar como \\(A\\) la matriz identidad o una matriz diagonal con elementos positivos \\(\\alpha_i\\in\\mathbb{R}^{+}\\) en la diagonal. ¿Qué preguntas vamos a responder en este capítulo? ¿Qué son las normas y distancias? ¿Qué significa el concepto de ortogonalidad? ¿Qué es el complemento ortogonal de un subespacio? ¿Cómo calculamos la proyección de un vector sobre un subespacio? ¿Qué es y qué utilidad tiene una base ortonormal? ¿Cómo construimos una base ortonormal? ¿Qué es una matriz ortogonal y una aplicación ortogonal? ¿Qué es la diagonalización ortogonal? Qué matrices son diagonalizables ortogonalmente 5.1 Qué son las normas y distancias Una norma vectorial sobre un espacio \\(V\\) (sobre un cuerpo \\(\\mathbb{K}\\)) es un operador que sirve para medir la longitud de un vector (es decir, su distancia al origen), y se define formalmente como una aplicación \\(\\|\\cdot\\|:V\\to\\mathbb{R}\\) que verifica: Para todo \\(v\\in V\\), \\(\\|v\\| &gt; 0\\), con \\(\\|v\\| = 0\\) si, y solo si, \\(v = 0\\). Para todo \\(c\\in\\mathbb{K}\\), \\(v\\in V\\), se tiene \\(\\|cv\\| = |c|\\cdot \\|v\\|\\). Para todo \\(u,v\\in V\\), \\(\\|u+v\\|\\le\\|u\\|+\\|v\\|\\). Esta propiedad se llama desigualdad triangular (en un triángulo, la suma de la longitud de dos de sus lados – \\(\\|u\\|\\) y \\(\\|v\\|\\) – es siempre menor que la longitud de su otro lado – \\(\\|u+v\\|\\) – ). Cualquier operador \\(\\|\\cdot\\|\\) que verifique lo anterior es una norma vectorial. Sin embargo, podemos construir normas a partir de un producto escalar: Si \\(\\langle\\cdot,\\cdot\\rangle\\) es un producto escalar en el espacio \\(V\\), entonces la aplicación \\(\\|\\cdot\\|:V\\to\\mathbb{R}\\) dada por \\(\\|v\\| = \\sqrt{\\langle v, v\\rangle}\\) es una norma derivada del producto escalar sobre \\(V\\). Al definir la norma vectorial a partir de un producto escalar, además, tenemos la siguiente propiedad que será de gran interés en la siguiente sección: (Desigualdad de Cauchy-Schwarz) Si \\(\\|\\cdot\\|\\) es una norma derivada del producto escalar \\(\\langle\\cdot,\\cdot\\rangle\\), entonces \\[|\\langle u, v \\rangle| \\le \\|u\\|\\cdot\\|v\\|\\] para todo \\(u,v\\in V\\). Un concepto implícitamente relacionado con el de norma vectorial es el de distancia. Una distancia (también llamada métrica) es un operador \\(d:V\\times V\\to R\\) que verifica las siguientes propiedades: \\(d(u, v) = 0\\) si, y solo si, \\(u = v\\). Es simétrica: \\(d(u, v) = d(v, u)\\). Cumple la desigualdad triangular: \\(d(u, v) \\le d(u, w) + d(w, v)\\). Además, de aquí se deduce que \\(d(u, v) \\ge 0\\) para todo \\(u,v\\in V\\). Podemos deducir una distancia a partir de una norma, para así completar las relaciones existentes entre los conceptos aquí explicados: Si \\(\\|\\cdot\\|\\) es una norma sobre un espacio vectorial \\(V\\), entonces la aplicación \\(d:V\\times V\\to \\mathbb{R}\\) definida por \\[d(u, v) = \\|u-v\\|\\] para todo \\(u,v\\in V\\) es una distancia. Nota: Existen muchas más normas de las que se deducen a partir de un producto escalar, al igual que más distancias. Ejemplo Podemos deducir la expresión de la norma (a veces llamada módulo) de un vector \\(v\\in\\mathbb{R}^n\\). Si \\(v=\\left(\\begin{array}{c}v_1\\\\ v_2\\\\\\vdots\\\\ v_n\\end{array}\\right)\\), entonces \\[\\|v\\| = \\sqrt{v\\cdot v} = \\sqrt{v_1^2+v_2^2+\\ldots+v_n^2}\\] Y así, además, deducimos la expresión para la distancia euclídea usual en \\(\\mathbb{R}^n\\): \\[d(u,v) = \\|u - v\\| = \\sqrt{(u_1-v_1)^2+\\ldots+(u_n-v_n)^2}\\] Como ejemplo, podemos calcular la norma del vector \\[v = \\left(\\begin{array}{c} -2\\\\ 1\\\\ 1\\\\ 1 \\end{array}\\right) \\]\\[\\|v\\| = \\sqrt{(-2)^2+1^2+1^2+1^2} = \\sqrt{7}\\] 5.2 Qué significa el concepto de ortogonalidad Si partimos de la desigualdad de Cauchy-Schwarz que hemos visto antes, podemos llegar a que \\[-1 \\le \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v\\|} \\le 1\\] para todo \\(u,v\\in V\\). Por otro lado, la función coseno restringida a \\([0, \\pi]\\), es decir, \\(\\cos:[0,\\pi]\\to[-1,1]\\), es biyectiva. Eso quiere decir que existe un único \\(\\theta\\in[0, \\pi]\\) tal que \\(\\cos\\theta = \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v\\|}\\). Llamamos entonces ángulo entre los vectores \\(u,v\\in V\\) al único \\(\\theta\\in[0,\\pi]\\) tal que \\(\\cos\\theta = \\frac{\\langle u, v\\rangle}{\\|u\\|\\cdot\\|v\\|}\\). Diremos que dos vectores \\(u\\) y \\(v\\) de \\(V\\) son ortogonales (o perpendiculares) si el ángulo que forman es \\(\\frac{\\pi}{2}\\), es decir, si \\(\\langle u, v \\rangle = 0\\). La noción de ortogonalidad en el espacio euclídeo se corresponde con la perpendicularidad de vectores que todos tenemos de forma intuitiva. Lo podemos extender también a ortogonalidad con respecto a un espacio vectorial: Un vector \\(v\\in V\\) es ortogonal al subespacio \\(U\\) de \\(V\\) si, y sólo si, es ortogonal a todos y cada uno de los vectores de \\(U\\), es decir, \\(\\langle v, u \\rangle = 0\\) para todo \\(u\\in U\\). Sea \\(U\\) un subespacio de \\(V\\), y sea \\(\\mathcal{B} = \\{u_1,\\ldots,u_k\\}\\) una base de \\(U\\). Un vector \\(v\\in V\\) es ortogonal a \\(U\\) si, y solo si, es ortogonal a cada \\(u_i\\): \\(\\langle v, u_i \\rangle = 0\\) para todo \\(i=1,\\ldots,k\\). Un sistema ortogonal es un conjunto de vectores \\(\\{v_1,\\ldots,v_m\\}\\subset V\\) donde los \\(v_i\\) son ortogonales dos a dos, es decir, \\(\\langle v_i, v_j \\rangle = 0\\) para todo \\(i\\ne j\\). Algunas propiedades y caracterizaciones de los vectores y sistemas ortogonales Dos vectores \\(u\\) y \\(w\\) son ortogonales si, y solo si, \\(\\|u+w\\|^2 = \\|u\\|^2 + \\|v\\|^2\\) (generalización del Teorema de Pitágoras). Si un sistema de vectores es ortogonal, entonces es linealmente independiente. Por tanto, en un espacio \\(V\\) de dimensión finita \\(n\\), cualquier conjunto de \\(n\\) vectores ortogonales entre sí es una base. Este último resultado es interesante, pues nos constata que cualquier sistema ortogonal con tantos vectores como la dimensión del espacio vectorial es una base. Ejemplo Consideremos el espacio \\(U\\) dado por: \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} -x+ 4y+ z &amp; = &amp; 0\\\\ y+ t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] y los vectores \\[v = \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) \\quad\\quad w = \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ -1 \\end{array}\\right) \\] Calculemos el ángulo entre \\(v\\) y \\(w\\): \\[\\theta = \\arccos \\frac{v\\cdot w}{\\|v\\|\\cdot\\|w\\|}\\]\\[v\\cdot w = 1\\cdot(-1)+(-2)\\cdot2+(-1)\\cdot(-1)+2\\cdot(-1) = -6\\]\\[\\|v\\| = \\sqrt{1^2+(-2)^2+(-1)^2+2^2} = \\sqrt{10}\\]\\[\\|w\\| = \\sqrt{(-1)^2+2^2+(-1)^2+(-1)^2} = \\sqrt{7}\\] Luego \\[\\theta = \\arccos \\frac{-6}{\\sqrt{10}\\sqrt{7}} = \\arccos \\frac{-3\\cdot\\sqrt{70}}{35}\\] Vamos a comprobar ahora que \\(v\\) es ortogonal al subespacio \\(U\\). Para ello, a partir de las ecuaciones cartesianas de \\(U\\), vamos a hallar una base suya, resolviendo el sistema por Gauss-Jordan y pasando por las ecuaciones paramétricas: \\[\\left( \\begin{array}{cccc@{}} -1 &amp; 4 &amp; 1 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc@{}} 1 &amp; 0 &amp; -1 &amp; 4\\\\ 0 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De aquí que la base de \\(U\\) sea: \\[\\mathcal{B}_U = \\left\\{u_i\\right\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Como hemos visto antes, para comprobar que \\(v\\) es ortogonal a \\(U\\), nos basta con comprobar que lo es a cada vector de \\(\\mathcal{B}_U\\), hallando su producto escalar: \\[\\langle v, u_1 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) \\rangle = 1\\cdot1+(-2)\\cdot0+(-1)\\cdot1+2\\cdot0 = 0\\]\\[\\langle v, u_2 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ -2\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} -4\\\\ -1\\\\ 0\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-4)+(-2)\\cdot(-1)+(-1)\\cdot0+2\\cdot1 = 0\\] Consideremos ahora el siguiente sistema de vectores: \\[S = \\{s_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array} \\right) \\right\\}\\] Podemos comprobar fácilmente que es un sistema ortogonal, hallando los productos escalares de los vectores de \\(S\\) dos a dos: \\[\\langle s_1, s_2 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) \\rangle = 1\\cdot(-1)+1\\cdot2+1\\cdot(-1)+0\\cdot0 = 0\\]\\[\\langle s_1, s_3 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) \\rangle = 1\\cdot1+1\\cdot0+1\\cdot(-1)+0\\cdot2 = 0\\]\\[\\langle s_1, s_4 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-1)+1\\cdot0+1\\cdot1+0\\cdot1 = 0\\]\\[\\langle s_2, s_3 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) \\rangle = (-1)\\cdot1+2\\cdot0+(-1)\\cdot(-1)+0\\cdot2 = 0\\]\\[\\langle s_2, s_4 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = (-1)\\cdot(-1)+2\\cdot0+(-1)\\cdot1+0\\cdot1 = 0\\]\\[\\langle s_3, s_4 \\rangle = \\langle \\left(\\begin{array}{c} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) \\rangle = 1\\cdot(-1)+0\\cdot0+(-1)\\cdot1+2\\cdot1 = 0\\] Tenemos, por tanto, un sistema de 4 vectores ortogonales en \\(\\mathbb{R}^{4}\\), y, como hemos comentado antes, son linealmente independientes, luego forman una base del espacio vectorial. 5.3 Qué es el complemento ortogonal de un subespacio Consideremos un espacio vectorial \\(V\\) con producto escalar \\(\\langle \\cdot,\\cdot \\rangle\\), y sea \\(U\\) un subespacio de \\(V\\). Llamamos complemento ortogonal de \\(U\\) al conjunto de vectores que son ortogonales a todos los de \\(U\\): \\[U^{\\perp} = \\{v\\in V: \\langle v, u \\rangle = 0\\,\\,\\text{para todo }u\\in U\\}\\] Tenemos las siguientes propiedades del complemento ortogonal de un subespacio: \\(U^{\\perp}\\) es un subespacio vectorial de \\(V\\). \\(U\\cap U^{\\perp} = \\{0\\}\\). Si \\(V\\) es de dimensión finita, \\(\\mathrm{dim}(U) + \\mathrm{dim}(U^{\\perp}) = \\mathrm{dim}(V)\\). Como consecuencia, este resultado nos dice que \\(V\\) se puede descomponer como la suma directa de \\(U\\) y de su complemento ortogonal: \\(U\\oplus U^{\\perp} = V\\). ¿Cómo calculamos el complemento ortogonal de un subespacio? Hemos de hallar los vectores que son ortogonales a un subespacio \\(U\\). Como hemos visto antes, un vector \\(v\\) es ortogonal a \\(U\\) si, y solo si, es ortogonal a todos los vectores de una base de \\(U\\). Así pues, partimos de que tenemos una base \\(\\mathcal{B} = \\{u_1,\\ldots,u_m\\}\\) de \\(U\\) y tomemos un vector genérico \\(v\\in V\\). Para que \\(v\\in U^{\\perp}\\), debe cumplirse que \\(\\langle v, u_i\\rangle = 0\\) para todo \\(i=1,\\ldots,m\\). Cada una de las restricciones \\(\\langle v, u_i\\rangle = 0\\) nos proporciona una ecuación lineal que debe verificar un vector para poder pertenecer a \\(U^{\\perp}\\). Tomamos, por tanto, todas las ecuaciones lineales que se deducen de las restricciones anteriores: ese sistema de ecuaciones nos representa las ecuaciones cartesianas del espacio \\(U^{\\perp}\\). En el caso concreto de un espacio \\(V=\\mathbb{R}^n\\) con el producto escalar euclídeo, podemos ver qué aspecto tienen las ecuaciones del tipo \\(\\langle v, u_i \\rangle = 0\\). Supongamos que los vectores \\(v\\) y \\(u_i\\) tienen por coordenadas, en la base canónica, las siguientes: \\[v = \\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots \\\\ x_n\\\\ \\end{array}\\right), \\quad u_i = \\left( \\begin{array}{c} \\alpha_{i,1}\\\\ \\alpha_{i,2}\\\\ \\vdots \\\\ \\alpha_{i,n}\\\\ \\end{array}\\right)\\] Entonces \\[ \\langle v, u_i \\rangle = 0 \\Leftrightarrow \\langle \\left( \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots \\\\ x_n\\\\ \\end{array}\\right) , \\left( \\begin{array}{c} \\alpha_{i,1}\\\\ \\alpha_{i,2}\\\\ \\vdots \\\\ \\alpha_{i,n}\\\\ \\end{array}\\right) \\rangle = 0 \\Leftrightarrow \\alpha_{i,1}x_1+\\alpha_{i,2}x_2+\\ldots+\\alpha_{i,n}x_n = 0 \\] Si nos fijamos, nos queda una ecuación lineal cuyas incógnitas son las coordenadas canónicas de \\(v\\) y cuyos coeficientes son las coordenadas de \\(u_i\\). Si repetimos el proceso para todo \\(i=1,\\ldots,m\\), tenemos el siguiente sistema de ecuaciones: \\[Av = 0\\quad\\text{donde}\\quad A = \\left( \\begin{array}{cccc} \\alpha_{1,1} &amp; \\alpha_{1,2} &amp; \\ldots &amp; \\alpha_{1,n} \\\\ \\alpha_{2,1} &amp; \\alpha_{2,2} &amp; \\ldots &amp; \\alpha_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\alpha_{m,1} &amp; \\alpha_{m,2} &amp; \\ldots &amp; \\alpha_{m,n} \\\\ \\end{array} \\right)\\] Es decir, las ecuaciones cartesianas de \\(U^{\\perp}\\) son las que tienen por matriz de coeficientes a las coordenadas de los vectores de la base de \\(U\\) puestos por filas. De hecho, si llamamos \\(B\\) a la matriz que resulta de poner por columnas los vectores de \\(\\mathcal{B}\\), entonces \\(A = B^{\\mathrm{t}}\\). A partir de esas ecuaciones cartesianas, ya podríamos encontrar una base del subespacio \\(U^{\\perp}\\). Ejemplo Consideremos el espacio \\(U\\) generado por la base siguiente: \\[\\mathcal{B} = \\{u_i\\} = \\left\\{\\left( \\begin{array}{c@{}} -2\\\\ 0\\\\ -2\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -2\\\\ 2 \\end{array} \\right) \\right\\}\\] Para hallar su complemento ortogonal, suponemos un vector genérico \\[v = \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in \\mathbb{R}^{4}\\] y hacemos su producto escalar por todos los vectores de \\(\\mathcal{B}\\), imponiendo que valga 0: \\[\\langle v, u_1 \\rangle = 0\\Leftrightarrow\\langle \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) , \\left(\\begin{array}{c} -2\\\\ 0\\\\ -2\\\\ 2 \\end{array}\\right) = 0\\Leftrightarrow\\ \\begin{array}{ccr} -2x-2z+ 2t &amp; = &amp; 0\\\\ \\end{array}\\]\\[\\langle v, u_2 \\rangle = 0\\Leftrightarrow\\langle \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ -1\\\\ -2\\\\ 2 \\end{array}\\right) = 0\\Leftrightarrow\\ \\begin{array}{ccr} -x-y-2z+ 2t &amp; = &amp; 0\\\\ \\end{array}\\] Hemos llegado, por tanto, al siguiente sistema de ecuaciones que representan las ecuaciones cartesianas de \\(U^{\\perp}\\): \\[\\begin{array}{rrrrcr} -2x &amp; &amp; -2z &amp; + 2t &amp; = &amp; 0\\\\ -x &amp; -y &amp; -2z &amp; + 2t &amp; = &amp; 0\\\\ \\end{array}\\] Nos podemos dar cuenta de que la matriz del sistema es la que resulta de poner los vectores de la base \\(\\mathcal{B}\\) por filas. Como siempre, podemos resolver el sistema por Gauss-Jordan para deducir la forma paramétrica de la solución, y, por tanto, un sistema generador de \\(U^{\\perp}\\): \\[\\left( \\begin{array}{cccc@{}} -2 &amp; 0 &amp; -2 &amp; 2\\\\ -1 &amp; -1 &amp; -2 &amp; 2 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc@{}} 1 &amp; 0 &amp; 1 &amp; -1\\\\ 0 &amp; 1 &amp; 1 &amp; -1 \\end{array} \\right) \\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} -1\\\\ -1\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} 1\\\\ 1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De aquí que una base del complemento ortogonal \\(U^{\\perp}\\) sea: \\[\\mathcal{B}&#39; = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Podemos comprobar fácilmente que los vectores de \\(\\mathcal{B}&#39;\\) son ortogonales a los de \\(\\mathcal{B}\\). 5.4 Cómo calculamos la proyección de un vector sobre un subespacio Consideremos un espacio euclídeo \\(\\left(V, \\langle\\cdot,\\cdot\\rangle\\right)\\). Consideremos un subespacio \\(U\\), del que conocemos una base ortogonal \\(\\mathcal{B}_U = \\{u_1,\\ldots,u_m\\}\\). Definimos la proyección ortogonal de un vector \\(v\\) sobre el subespacio \\(U\\), como: \\[\\mathrm{proy}_U(v) = \\frac{\\langle v, u_1\\rangle}{\\langle u_1, u_1 \\rangle}u_1 + \\frac{\\langle v, u_2\\rangle}{\\langle u_2, u_2 \\rangle}u_2 + \\ldots + \\frac{\\langle v, u_m\\rangle}{\\langle u_m, u_m \\rangle}u_m\\] Es decir, la proyección \\(\\mathrm{proy}_U(v)\\) es un vector del subespacio \\(U\\), ya que es combinación lineal de los elementos de su base, y además esta combinación lineal tiene como coeficientes el producto escalar de \\(v\\) por cada elemento de la base, dividido por la norma de \\(u_i\\) al cuadrado (recordemos que \\(\\|x\\| = \\sqrt{\\langle x,x \\rangle}\\)). Evidentemente, si los elementos de la base de \\(U\\) tienen norma 1, el denominador desaparece. La proyección de \\(v\\) sobre el subespacio \\(U\\) tiene la propiedad siguiente: Si \\(u\\) es un vector del subespacio \\(U\\), y \\(v\\in V\\), entonces \\(d(u, v) \\ge d(\\mathrm{proy}_U(v), v)\\), y la única forma para que se dé la igualdad es que \\(u = \\mathrm{proy}_U(v)\\). Esto significa que la proyección ortogonal de \\(v\\) sobre \\(U\\) es el vector de \\(U\\) más cercano (según la distancia \\(d\\)) a \\(v\\). Como caso particular, podríamos definir la proyección ortogonal de un vector \\(v\\) sobre otro vector \\(u\\) como \\[\\mathrm{proy}_u(v) = \\frac{\\langle v, u \\rangle}{\\langle u, u\\rangle}u\\] (realmente es la proyección del vector \\(v\\) sobre el subespacio generado por el vector \\(u\\)). Podemos entonces concluir que, si la base ortogonal de \\(U\\) es \\(\\mathcal{B}_U = \\{u_1,\\ldots,u_m\\}\\), entonces \\[\\mathrm{proy}_U(v) = \\mathrm{proy}_{u_1}(v)+\\ldots+\\mathrm{proy}_{u_m}(v)\\] A partir del hecho de que la base de \\(U\\) es ortogonal, podemos deducir que: Si \\(U\\) es un subespacio de \\(V\\), \\(v\\in V\\), y una base ortogonal de \\(U\\) es \\(\\mathcal{B}_U = \\{u_1,\\ldots,u_m\\}\\), entonces el vector \\(v - \\mathrm{proy}_U(v)\\) es ortogonal a \\(U\\), es decir, \\(v - \\mathrm{proy}_U(v) \\in U^{\\perp}\\) está en el complemento ortogonal de \\(U\\). Esto, junto con que \\(\\mathrm{proy}_U(v)\\in U\\), nos justifican el siguiente resultado, llamado teorema de la descomposición ortogonal: En un espacio euclídeo \\((V, \\langle\\cdot,\\cdot\\rangle)\\), dado un subespacio \\(U\\), y \\(v\\in V\\), existe una única descomposición de \\(v\\) como suma de dos vectores ortogonales, \\(v = v_1 + v_2\\), tales que \\(v_1\\in U\\) y \\(v_2\\in U^{\\perp}\\). Concretamente, \\(v_1 = \\mathrm{proy}_U(v)\\) y \\(v_2 = v - v_1 = \\mathrm{proy}_{U^{\\perp}}(v)\\). Podemos siempre descomponer un vector de \\(V\\) como suma de dos vectores, uno de \\(U\\) y otro de \\(U^{\\perp}\\), que coinciden con las proyecciones sobre ambos subespacios. Ejemplo Consideremos el subespacio \\(U\\) cuya base es \\[\\mathcal{B} = \\{u_i\\} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array} \\right) \\right\\}\\] Notemos que \\(\\mathcal{B}\\) es una base ortogonal. Consideremos el vector \\[v = \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right) \\] Buscamos la descomposición ortogonal de \\(v\\) con respecto al subespacio \\(U\\). Sabemos que podemos escribir \\(v = v_1 + v_2\\), donde \\(v_1 = \\mathrm{proy}_U(v)\\) y \\(v_2 = v - v_1 \\in U^{\\perp}\\). \\[v_1 = \\mathrm{proy}_U(v) = \\mathrm{proy}_{u_1}(v)+\\mathrm{proy}_{u_2}(v) = \\frac{\\langle v, u_1\\rangle}{\\langle u_1, u_1 \\rangle} u_1+\\frac{\\langle v, u_2\\rangle}{\\langle u_2, u_2 \\rangle} u_2\\] Si calculamos esas cantidades \\[\\langle v, u_1 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) \\rangle = (-1)\\cdot(-1)+1\\cdot1+(-2)\\cdot(-1)+0\\cdot(-1) = 4\\]\\[\\langle v, u_2 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array}\\right) , \\left(\\begin{array}{c} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array}\\right) \\rangle = (-1)\\cdot(-3)+1\\cdot(-5)+(-2)\\cdot1+0\\cdot(-3) = -4\\]\\[\\langle u_1, u_1 \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) , \\left(\\begin{array}{c} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) \\rangle = (-1)^2+1^2+(-1)^2+(-1)^2 = 4\\]\\[\\langle u_2, u_2 \\rangle = \\langle \\left(\\begin{array}{c} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array}\\right) , \\left(\\begin{array}{c} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array}\\right) \\rangle = (-3)^2+(-5)^2+1^2+(-3)^2 = 44\\] podemos sustituirlas en la expresión de más arriba y llegamos a \\[v_1 = 1\\cdot\\left(\\begin{array}{c} -1\\\\ 1\\\\ -1\\\\ -1 \\end{array}\\right) +(- \\frac{1}{11})\\cdot\\left(\\begin{array}{c} -3\\\\ -5\\\\ 1\\\\ -3 \\end{array}\\right) = \\left( \\begin{array}{c@{}} - \\frac{8}{11}\\\\ \\frac{16}{11}\\\\ - \\frac{12}{11}\\\\ - \\frac{8}{11} \\end{array} \\right) \\] Por tanto, \\[v_2 = \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right) - \\left( \\begin{array}{c@{}} - \\frac{8}{11}\\\\ \\frac{16}{11}\\\\ - \\frac{12}{11}\\\\ - \\frac{8}{11} \\end{array} \\right) = \\left( \\begin{array}{c@{}} - \\frac{3}{11}\\\\ - \\frac{5}{11}\\\\ - \\frac{10}{11}\\\\ \\frac{8}{11} \\end{array} \\right) \\] Por lo comentado anteriormente, sabemos que \\(v_2 = \\mathrm{proy}_{U^{\\perp}}(v)\\in U^{\\perp}\\), lo cual se podría comprobar haciendo el producto escalar \\(\\langle v_2, u_i \\rangle\\) y viendo que su valor es 0 para todo \\(i\\). 5.5 Qué es y qué utilidad tiene una base ortonormal Un vector \\(v\\in V\\) se dice unitario si \\(\\|v\\| = 1\\). Es fácil crear vectores unitarios: dado \\(u\\in V\\), si definimos \\(v = \\frac{1}{\\|u\\|}u\\), este vector tiene norma 1. A este proceso se le llama normalizar. Un sistema de vectores \\(\\{v_1,\\ldots,v_k\\}\\) se llama sistema ortonormal si es un sistema ortogonal y cada \\(v_i\\) es un vector unitario (\\(i=1,\\ldots, k\\)). En relación a lo comentado de las propiedades de los sistemas ortogonales, en nuestro caso podemos decir que un sistema ortonormal \\(\\{v_1,\\ldots,v_n\\}\\), donde \\(\\mathrm{dim}(V) = n\\), es una base ortonormal. El interés de una base ortonormal es que es sencillo calcular las coordenadas de cualquier vector con respecto a dicha base: Sea \\(\\mathcal{B} = \\{v_1,\\ldots,v_n\\}\\) una base ortonormal del espacio vectorial \\(V\\). Entonces, para cada \\(v\\in V\\), tenemos: \\[v = \\langle v, v_1 \\rangle v_1 + \\ldots \\langle v, v_n \\rangle v_n\\] luego las coordenadas de \\(v\\) en la base \\(\\mathcal{B}\\) las podemos calcular como: \\[[v]_{\\mathcal{B}} = \\left( \\begin{array}{c} \\langle v, v_1 \\rangle \\\\ \\langle v, v_2 \\rangle \\\\ \\vdots \\\\ \\langle v, v_n \\rangle \\\\ \\end{array} \\right)_{\\mathcal{B}}\\] Ejemplo Consideremos ahora el sistema de vectores ortogonales del ejemplo anterior: \\[S = \\{s_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 2\\\\ -1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ -1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array} \\right) \\right\\}\\] A partir de \\(S\\), formemos un sistema ortonormal \\(S&#39;\\), donde cada vector de \\(S&#39;\\) es un vector de \\(S\\), que se ha normalizado. \\[S&#39; = \\left\\{\\left( \\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3} \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3} \\end{array} \\right)\\right\\}\\] \\(S&#39;\\) forma una base ortonormal de \\(\\mathbb{R}^{4}\\). Si consideramos el vector \\[v = \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right) \\] podemos hallar sus coordenadas en la base \\(S&#39;\\), ya que cada coordenada de \\(v\\) será el producto escalar de \\(v\\) por el correspondiente vector de \\(S&#39;\\): \\[\\langle v, s&#39;_1 \\rangle = \\langle \\left( \\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array} \\right) \\rangle = \\frac{-2\\cdot\\sqrt{3}}{3}\\]\\[\\langle v, s&#39;_2 \\rangle = \\langle \\left( \\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0 \\end{array} \\right) \\rangle = \\frac{5\\cdot\\sqrt{6}}{6}\\]\\[\\langle v, s&#39;_3 \\rangle = \\langle \\left( \\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3} \\end{array} \\right) \\rangle = \\frac{\\sqrt{6}}{6}\\]\\[\\langle v, s&#39;_4 \\rangle = \\langle \\left( \\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3} \\end{array} \\right) \\rangle = \\frac{-\\sqrt{3}}{3}\\] Entonces, podremos escribir: \\[v = \\left( \\begin{array}{c} \\frac{-2\\cdot\\sqrt{3}}{3}\\\\ \\frac{5\\cdot\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{6}\\\\ \\frac{-\\sqrt{3}}{3} \\end{array} \\right)_{S&#39;}\\] 5.6 Cómo construimos una base ortonormal En las secciones anteriores hemos usado bases ortogonales y ortonormales, pero cabe preguntarnos si la existencia de tales bases está asegurada. La respuesta nos la da el siguiente resultado: Sea \\(V\\) un espacio vectorial euclídeo de dimensión finita. Entonces \\(V\\) tiene una base ortonormal. La demostración de este resultado teórico realmente nos presenta un método constructivo que permite hallar una base ortonormal del espacio \\(V\\) a partir de una base cualquiera. Método de Ortonormalización de Gram-Schmidt Consideremos una base \\(\\mathcal{B} = \\{v_1,\\ldots,v_n\\}\\) en \\(V\\). Vamos a utilizar el método de Gram-Schmidt para construir una base ortonormal a partir de ella. El proceso se divide en dos fases: En la primera fase, construiremos una base ortogonal \\(\\mathcal{B}&#39;\\). En un segundo paso, cada vector de \\(\\mathcal{B}&#39;\\) se normaliza, de forma que obtendremos una base ortonormal. Primera fase El proceso del primer paso es incremental. Llamamos \\(u_1 = v_1\\), el primer vector de la base \\(\\mathcal{B}\\). Ahora consideramos el segundo vector \\(v_2\\). Por un resultado teórico sobre las proyecciones que vimos antes, si llamamos \\(U_1 = \\mathcal{L}(\\{u_1\\})\\), entonces \\(v_2 - \\mathrm{proy}_{U_1}(v_2)\\) es un vector ortogonal a \\(U_1\\), en particular es ortogonal a \\(u_1\\). Llamemos entonces \\(u_2 = v_2 - \\mathrm{proy}_{U_1}(v_2)\\). Entonces, el sistema \\(\\{u_1, u_2\\}\\) es ortogonal. Llamamos ahora \\(U_2 = \\mathcal{L}(\\{u_1,u_2\\})\\). Entonces, por el mismo motivo que antes, si llamamos \\(u_3 = v_3 - \\mathrm{proy}_{U_2}(v_3)\\), tenemos que \\(u_3\\) es ortogonal a \\(U_2\\), luego \\(\\{u_1, u_2, u_3\\}\\) es un sistema ortogonal. De esta forma, sucesivamente, en el paso \\(m\\), vamos construyendo \\(U_m = \\mathcal{L}(\\{u_1,\\ldots,u_m\\})\\) y consideramos \\(u_{m+1} = v_{m+1} - \\mathrm{proy}_{U_m}(v_{m+1})\\), que es un vector ortogonal a \\(U_m\\) y, por tanto, \\(\\{u_1,\\ldots,u_{m+1}\\}\\) es un sistema ortogonal. Una vez se haya completado el proceso, se habrá construido el conjunto \\(\\mathcal{B}&#39; = \\{u_1,\\ldots,u_n\\}\\). Como estamos en un espacio \\(V\\) de dimensión \\(n\\), y tenemos \\(n\\) vectores ortogonales, luego linealmente independientes, éstos forman una base. Luego \\(\\mathcal{B}&#39;\\) es una base ortogonal. Segunda fase La segunda fase es más sencilla, ya que implica únicamente normalizar cada vector de \\(\\mathcal{B}&#39;\\). De esta forma, la base ortonormal es la formada por \\[\\left\\{\\frac{u_1}{\\|u_1\\|}, \\ldots, \\frac{u_n}{\\|u_n\\|}\\right\\}\\] Ejemplo Consideremos la base \\(\\mathcal{B}\\) en \\(\\mathbb{R}^{4}\\) dada por \\[\\mathcal{B} = \\{v_i\\} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ 0\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 1\\\\ -1 \\end{array} \\right) \\right\\}\\] Vamos a usar el método de Gram-Schmidt para encontrar una base ortonormal, a partir de \\(\\mathcal{B}\\). Fase 1 Llamamos \\[u_1 = v_1 = \\left(\\begin{array}{c} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array}\\right) \\] A partir de este momento, procedemos de forma recursiva: Construimos: \\[U_1 = \\mathcal{L}(\\{u_1\\}) = \\mathcal{L}(\\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) \\right\\})\\] Entonces \\[\\begin{array}{rcl}\\mathrm{proy}_{U_1}(v_2) &amp; = &amp; \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1 \\\\ &amp; = &amp; \\frac{0}{4} \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) \\\\ &amp; = &amp; \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right) \\\\\\end{array}\\] Y llamamos \\[u_2 = v_2 - \\mathrm{proy}_{U_1}(v_2) = \\left(\\begin{array}{c} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array}\\right) - \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) \\] Construimos: \\[U_2 = \\mathcal{L}(\\{u_1, u_2\\}) = \\mathcal{L}(\\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) \\right\\})\\] Entonces \\[\\begin{array}{rcl}\\mathrm{proy}_{U_2}(v_3) &amp; = &amp; \\frac{\\langle v_3, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1+\\frac{\\langle v_3, u_2 \\rangle}{\\langle u_2, u_2 \\rangle} u_2 \\\\ &amp; = &amp; \\frac{3}{4} \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) +\\frac{1}{4} \\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) \\\\ &amp; = &amp; \\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ -1\\\\ - \\frac{1}{2}\\\\ -1 \\end{array} \\right) \\\\\\end{array}\\] Y llamamos \\[u_3 = v_3 - \\mathrm{proy}_{U_2}(v_3) = \\left(\\begin{array}{c} -1\\\\ -1\\\\ 0\\\\ -1 \\end{array}\\right) - \\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ -1\\\\ - \\frac{1}{2}\\\\ -1 \\end{array} \\right) = \\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ 0\\\\ \\frac{1}{2}\\\\ 0 \\end{array} \\right) \\] Construimos: \\[U_3 = \\mathcal{L}(\\{u_1, u_2, u_3\\}) = \\mathcal{L}(\\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ 0\\\\ \\frac{1}{2}\\\\ 0 \\end{array} \\right) \\right\\})\\] Entonces \\[\\begin{array}{rcl}\\mathrm{proy}_{U_3}(v_4) &amp; = &amp; \\frac{\\langle v_4, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1+\\frac{\\langle v_4, u_2 \\rangle}{\\langle u_2, u_2 \\rangle} u_2+\\frac{\\langle v_4, u_3 \\rangle}{\\langle u_3, u_3 \\rangle} u_3 \\\\ &amp; = &amp; \\frac{0}{4} \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) +\\frac{0}{4} \\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) +\\frac{1}{\\frac{1}{2}} \\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ 0\\\\ \\frac{1}{2}\\\\ 0 \\end{array} \\right) \\\\ &amp; = &amp; \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) \\\\\\end{array}\\] Y llamamos \\[u_4 = v_4 - \\mathrm{proy}_{U_3}(v_4) = \\left(\\begin{array}{c} -1\\\\ 1\\\\ 1\\\\ -1 \\end{array}\\right) - \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ -1 \\end{array} \\right) \\] Por tanto, la base ortogonal a la que llegamos es \\[\\mathcal{B}&#39; = \\{u_i\\} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ 0\\\\ \\frac{1}{2}\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0\\\\ -1 \\end{array} \\right) \\right\\}\\] Fase 2 Ahora debemos multiplicar cada vector \\(u_i\\) en \\(\\mathcal{B}&#39;\\) por el inverso de su norma, y llegamos a los vectores \\(\\frac{1}{\\|u_i\\|}u_i\\), que nos forman la base ortonormal siguiente: \\[\\begin{array}{rcl}\\mathcal{B}&#39;&#39; &amp; = &amp; \\left\\{\\frac{1}{2}\\left( \\begin{array}{c} -1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right), \\frac{1}{2}\\left( \\begin{array}{c} 1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right), \\frac{1}{\\frac{\\sqrt{2}}{2}}\\left( \\begin{array}{c} \\frac{-1}{2}\\\\ 0\\\\ \\frac{1}{2}\\\\ 0 \\end{array} \\right), \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{c} 0\\\\ 1\\\\ 0\\\\ -1 \\end{array} \\right)\\right\\} = \\\\ &amp; = &amp; \\left\\{\\left( \\begin{array}{c} \\frac{-1}{2}\\\\ \\frac{-1}{2}\\\\ \\frac{-1}{2}\\\\ \\frac{-1}{2} \\end{array} \\right), \\left( \\begin{array}{c} \\frac{1}{2}\\\\ \\frac{-1}{2}\\\\ \\frac{1}{2}\\\\ \\frac{-1}{2} \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{2}}{2}\\\\ 0\\\\ \\frac{\\sqrt{2}}{2}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} 0\\\\ \\frac{\\sqrt{2}}{2}\\\\ 0\\\\ \\frac{-\\sqrt{2}}{2} \\end{array} \\right)\\right\\} \\\\\\end{array}\\] 5.7 Qué es una matriz ortogonal y una aplicación ortogonal Una matriz cuadrada \\(Q\\) se llama ortogonal si \\(Q^{\\text{t}}Q = I\\) o, equivalentemente, \\(Q^{-1} = Q^{\\text{t}}\\). Es decir, una matriz ortogonal es aquella cuya inversa es su propia traspuesta. Una matriz \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) es ortogonal si, y sólo si, sus columnas forman una base ortonormal de \\(\\mathbb{R}^n\\). Sea \\(A\\) una matriz ortogonal. Entonces \\(\\mathrm{det}(A) = \\pm 1\\). Un endomorfismo \\(f:V\\to V\\) se llama ortogonal si respeta el producto escalar, es decir, si \\(\\langle\\cdot,\\cdot\\rangle\\) es el producto escalar en \\(V\\), entonces \\(\\langle f(u), f(v) \\rangle = \\langle u, v \\rangle\\) para todo \\(u,v\\in V\\). Como la aplicación ortogonal conserva el producto escalar, también conserva todas las cantidades que hemos definido a partir de él: la norma, la distancia y el ángulo entre vectores de \\(V\\): \\(\\|f(v)\\| = \\|v\\|\\), \\(d(f(u), f(v)) = d(u,v)\\) y \\(\\mathrm{ang}(f(u),f(v)) = \\mathrm{ang}(u, v)\\) para todo \\(u,v\\in V\\). Una aplicación \\(f:V\\to V\\) es ortogonal si, y sólo si, la imagen de toda base ortonormal es de nuevo una base ortonormal de \\(V\\). ¿Qué relación existe entre matrices ortogonales y aplicaciones ortogonales? Sea \\(f:V\\to V\\) una aplicación lineal y sea \\(A\\) su matriz asociada respecto a una base ortonormal de \\(V\\). Entonces \\(f\\) es ortogonal si, y solo si, la matriz \\(A\\) es ortogonal. Ejemplo Como hemos comentado, si tomamos una base ortonormal y ponemos sus vectores por columnas, nos forman una matriz ortogonal. Por tanto, podemos considerar la base \\(\\mathcal{B}&#39;&#39;\\) del ejemplo anterior, y construir la matriz ortogonal asociada: \\[Q = \\left( \\begin{array}{c} \\frac{-1}{2}\\\\ \\frac{-1}{2}\\\\ \\frac{-1}{2}\\\\ \\frac{-1}{2} \\end{array} \\begin{array}{c} \\frac{1}{2}\\\\ \\frac{-1}{2}\\\\ \\frac{1}{2}\\\\ \\frac{-1}{2} \\end{array} \\begin{array}{c} \\frac{-\\sqrt{2}}{2}\\\\ 0\\\\ \\frac{\\sqrt{2}}{2}\\\\ 0 \\end{array} \\begin{array}{c} 0\\\\ \\frac{\\sqrt{2}}{2}\\\\ 0\\\\ \\frac{-\\sqrt{2}}{2} \\end{array} \\right)\\] Es fácil comprobar que, en este caso, \\(Q^{\\text{t}}\\ Q = I\\). Hay otros ejemplos de matrices ortogonales. En particular, una tipología concreta de matrices ortogonales en \\(\\mathbb{R}^2\\) es muy utilizada en la práctica. Llamemos: \\[R(\\alpha) = \\left( \\begin{array}{cc} \\cos(\\alpha) &amp; \\sin(\\alpha) \\\\ -\\sin(\\alpha) &amp; \\cos(\\alpha) \\\\ \\end{array}\\right)\\] Son las denominadas matrices de rotación. Están asociadas a la aplicación lineal que representa un giro o rotación de \\(\\alpha\\) radianes en sentido positivo (antihorario) alrededor del origen de coordenadas. Así \\[ \\left( \\begin{array}{c} x&#39; \\\\ y&#39; \\\\ \\end{array}\\right) = R(\\alpha)\\left( \\begin{array}{c} x \\\\ y \\\\ \\end{array}\\right)\\] son las coordenadas del vector girado, en función de las coordenadas del vector original. Estas matrices de rotación se pueden extender a dimensiones superiores. Por ejemplo, en \\(\\mathbb{R}^3\\), la rotación de \\(\\alpha\\) radianes alrededor del eje \\(X\\) se representa mediante la matriz ortogonal \\[R(\\alpha) = \\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos(\\alpha) &amp; \\sin(\\alpha) \\\\ 0 &amp; -\\sin(\\alpha) &amp; \\cos(\\alpha) \\\\ \\end{array}\\right) \\] 5.8 Qué es la diagonalización ortogonal Recordemos la definición de endomorfismo o de matriz diagonalizable. Una aplicación lineal \\(f:V\\to V\\) es diagonalizable si y solo si existe una base de \\(V\\) formada por autovectores de \\(f\\), lo cual equivale a que la representación matricial de \\(f\\) en dicha base sea una matriz diagonal \\(D\\). En representación matricial, esto se traduce en que una matriz \\(A\\in\\mathbb{M}_n(\\mathbb{R})\\) es diagonalizable si y solo si, existe una matriz regular \\(P\\) tal que \\(D = P^{-1}\\ A\\ P\\). Las columnas de \\(P\\) están formadas por los autovectores de la base de la definición. Veamos cómo influye el hecho de tener bases ortonormales en un espacio euclídeo en la diagonalización, lo que da lugar a la denominada diagonalización ortogonal. Diremos que una aplicación lineal \\(f:V\\to V\\) es diagonalizable ortogonalmente si y solo si existe una base ortonormal en \\(V\\) formada por autovectores de \\(f\\). En matrices, una matriz \\(A\\) se dice diagonalizable ortogonalmente si existe una matriz \\(P\\) ortogonal tal que \\(D = P^{\\text{t}}\\ A\\ P\\), donde \\(D\\), la matriz asociada a \\(f\\) en la base de autovectores, es diagonal con autovalores en la diagonal principal. Ejemplo Consideremos la matriz \\(A\\) dada por \\[A = \\left( \\begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; 0 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -1 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\] asociada a un endomorfismo \\(f:\\mathbb{R}^{4} \\to \\mathbb{R}^{4}\\) en la base canónica. Si tomamos la base \\[\\mathcal{B} = \\left\\{\\left( \\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{\\sqrt{6}}{3} \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3} \\end{array} \\right)\\right\\}\\] resulta ser una base ortonormal formada por autovectores de \\(f\\) (en la siguiente sección detallaremos el proceso completo). Por tanto, esta matriz (y el endomorfismo \\(f\\)) es diagonalizable ortogonalmente. De hecho, si consideramos como matriz \\(P\\) la del cambio de base de \\(\\mathcal{B}\\) a la canónica, podemos calcular la expresión diagonal, teniendo en cuenta que \\(P\\) es una matriz ortogonal: \\[\\begin{array}{rcl}D &amp; = &amp; P^{\\text{t}}\\ A\\ P = \\\\ &amp; = &amp; \\left( \\begin{array}{cccc@{}} \\frac{-\\sqrt{6}}{6} &amp; \\frac{\\sqrt{6}}{6} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{-\\sqrt{3}}{3}\\\\ \\frac{-\\sqrt{6}}{6} &amp; \\frac{-\\sqrt{6}}{6} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{6}}{3} &amp; 0 &amp; \\frac{\\sqrt{3}}{3} &amp; 0\\\\ 0 &amp; \\frac{\\sqrt{6}}{3} &amp; 0 &amp; \\frac{\\sqrt{3}}{3} \\end{array} \\right) ^{\\text{t}}\\ \\left( \\begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; 0 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -1 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\ \\left( \\begin{array}{cccc@{}} \\frac{-\\sqrt{6}}{6} &amp; \\frac{\\sqrt{6}}{6} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{-\\sqrt{3}}{3}\\\\ \\frac{-\\sqrt{6}}{6} &amp; \\frac{-\\sqrt{6}}{6} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{6}}{3} &amp; 0 &amp; \\frac{\\sqrt{3}}{3} &amp; 0\\\\ 0 &amp; \\frac{\\sqrt{6}}{3} &amp; 0 &amp; \\frac{\\sqrt{3}}{3} \\end{array} \\right) = \\\\ &amp; = &amp; \\left( \\begin{array}{cccc@{}} 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -3 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 3 \\end{array} \\right) \\end{array}\\] donde podemos comprobar que los elementos de la diagonal de \\(D\\) son autovalores de \\(A\\). 5.9 Qué matrices son diagonalizables ortogonalmente Supongamos una matriz \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) que sea diagonalizable ortogonalmente, es decir, tal que existe \\(P\\) ortogonal con \\(P^{\\text{t}}\\ A\\ P = D\\) una matriz diagonal. Como \\(D\\) es diagonal, en particular tenemos que \\(D^{\\text{t}} = D\\), luego: \\[P^{\\text{t}}\\ A\\ P = D = D^{\\text{t}} = (P^{\\text{t}}\\ A\\ P)^{\\text{t}} = P^{\\text{t}}\\ A^{\\text{t}}\\ \\left(P^{\\text{t}}\\right)^{\\text{t}} = P^{\\text{t}}\\ A^{\\text{t}}\\ P\\] Luego, multiplicando por \\(P\\) a la izquierda en los extremos de la igualdad, y por \\(P^{\\text{t}}\\) en la derecha, recordando que \\(P\\) es una matriz ortogonal, llegamos a que \\(A = A^{\\text{t}}\\). Esto significa que: Si una matriz \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) es diagonalizable ortogonalmente, entonces es simétrica. Por tanto, una matriz no simétrica podrá ser diagonalizable (dependiendo de si cumple los criterios ya estudiados), pero nunca puede ser diagonalizable ortogonalmente. Podemos estudiar entonces algunas de las propiedades de las matrices simétricas en lo que se refiere a diagonalización: Sea \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) una matriz simétrica. Entonces: Todos los autovalores de \\(A\\) son reales. Si \\(\\lambda_1\\) y \\(\\lambda_2\\) son dos autovalores distintos, y sus subespacios asociados son \\(U_{\\lambda_1}\\) y \\(U_{\\lambda_2}\\), respectivamente, entonces \\(U_{\\lambda_1}\\) es ortogonal a \\(U_{\\lambda_2}\\). La multiplicidad geométrica de cada autovalor de \\(A\\) coincide con su multiplicidad algebraica. En consecuencia Una matriz \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) es diagonalizable ortogonalmente si y solo si es simétrica. ¿Cómo diagonalizamos ortogonalmente una mtriz simétrica? El procedimiento es idéntico al que se conoce para diagonalizar una matriz cualquiera, con las siguientes puntualizaciones: Al ser una matriz simétrica, tenemos asegurada que es diagonalizable (incluso ortogonalmente). Al calcular la base del subespacio asociado a un autovalor \\(\\lambda\\), podemos hacer un paso extra, usando el método de Gram-Schmidt, para tener dicha base en forma ortonormal. Al finalizar, al igual que hacíamos en la digonalización estándar, podemos unir todas las bases de los distintos subespacios asociados a los autovalores y tenemos una base ortonormal del espacio vectorial \\(V\\). De forma esquemática, incluyendo todo lo que conocemos de diagonalización, queda: Calcular el polinomio característico \\(p(\\lambda)\\) de \\(A\\), y sus raíces, los autovalores de \\(A\\). Sean \\(\\lambda_1,\\ldots,\\lambda_k\\) los autovalores. Para cada autovalor \\(\\lambda\\): Determinar una base \\(\\mathcal{B}_{\\lambda}\\) del subespacio propio \\(U_{\\lambda}\\). Aplicar el método de Gram-Schmidt a \\(\\mathcal{B}_{\\lambda}\\) para obtener una base _ortonormal de \\(U_{\\lambda}\\) a la que llamaremos \\(\\mathcal{B}&#39;_{\\lambda}\\). Llamamos \\(\\mathcal{B} = \\mathcal{B}&#39;_{\\lambda_1}\\cup\\ldots\\cup\\mathcal{B}_{\\lambda_k}\\). Entonces \\(\\mathcal{B}\\) es una base ortonormal de \\(V\\). Además, la matriz \\(P\\) de cambio de base de \\(\\mathcal{B}\\) a la canónica es una matriz ortogonal, tiene por columnas las coordenadas de los vectores de \\(\\mathcal{B}\\), y verifica que \\(D = P^{\\text{t}}\\ A\\ P\\) es una matriz diagonal con los autovalores en la diagonal principal (con multiplicidades incluidas). Ejemplo Consideremos la matriz \\[A = \\left( \\begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; 0 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -1 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\] Como es simétrica, es diagonalizable ortogonalmente. Vamos a seguir los pasos vistos para encontrar su forma diagonal y la base ortonormal de autovectores. Paso 1: Encontrar los autovalores de \\(A\\) Para calcular los autovalores de \\(A\\), comenzamos por construir su polinomio característico: \\[\\begin{array}{rcl}p(\\lambda) &amp; = &amp; \\mathrm{det}(A - \\lambda\\ I) = \\\\ &amp; = &amp; \\mathrm{det}\\left(\\left( \\begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; 0 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -1 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) - \\lambda \\left( \\begin{array}{cccc@{}} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right) \\right) = \\\\ &amp; = &amp; \\mathrm{det}\\left( \\begin{array}{cccc@{}} -\\lambda &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; -\\lambda &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -1-\\lambda &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 1-\\lambda \\end{array} \\right) \\\\ &amp; = &amp; \\lambda^{4}-9\\lambda^{2}\\end{array}\\] Igualándolo a 0 y resolviendo la ecuación que queda, llegamos a que los autovalores son: \\(\\lambda = 0, -3, 3\\), ya que: \\[p(\\lambda) = \\lambda ^{2}\\cdot (\\lambda +3)\\cdot (\\lambda -3)\\] Paso 2: Encontrar una base ortonormal de cada subespacio propio Recorremos ahora cada autovalor, encontrando una base del subespacio propio asociado, y ortonormalizándola usando el método de Gram-Schmidt. Para el autovalor \\(\\lambda = 0\\): \\[\\begin{array}{rcl}(A - 0 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0&amp; \\Leftrightarrow &amp; \\left( \\begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; 0 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -1 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0 \\Leftrightarrow \\\\&amp; \\Leftrightarrow &amp; \\left\\{\\begin{array}{rrrrcr} &amp; -2y &amp; -z &amp; -t &amp; = &amp; 0\\\\ -2x &amp; &amp; -z &amp; + t &amp; = &amp; 0\\\\ -x &amp; -y &amp; -z &amp; &amp; = &amp; 0\\\\ -x &amp; + y &amp; &amp; + t &amp; = &amp; 0\\\\ \\end{array}\\right. \\\\\\end{array}\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de \\(U_{ 0 }\\): \\[\\left( \\begin{array}{cccc|c} 0 &amp; -2 &amp; -1 &amp; -1 &amp; 0\\\\ -2 &amp; 0 &amp; -1 &amp; 1 &amp; 0\\\\ -1 &amp; -1 &amp; -1 &amp; 0 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|c} -2 &amp; 0 &amp; -1 &amp; 1 &amp; 0\\\\ 0 &amp; -2 &amp; -1 &amp; -1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\]\\[\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} - \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] Luego una base de \\(U_{ 0 }\\) será: \\[\\mathcal{B}_{U_{0}} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] El siguiente paso es ortonormalizar esta base, mediante el método de Gram-Schmidt. Llamemos \\(v_i\\) a los vectores de esta base hallada. Llamamos \\[u_1 = v_1 = \\left(\\begin{array}{c} - \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 1\\\\ 0 \\end{array}\\right) \\] Procedemos con el resto de vectores: Construimos: \\[U_1 = \\mathcal{L}(\\{u_1\\}) = \\mathcal{L}(\\left\\{\\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 1\\\\ 0 \\end{array} \\right) \\right\\})\\] Entonces \\[\\mathrm{proy}_{U_1}(v_2) = \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1 = \\frac{0}{\\frac{3}{2}} \\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 1\\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right) \\] Y llamamos \\[u_2 = v_2 - \\mathrm{proy}_{U_1}(v_2) = \\left(\\begin{array}{c} \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 0\\\\ 1 \\end{array}\\right) - \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c@{}} \\frac{1}{2}\\\\ - \\frac{1}{2}\\\\ 0\\\\ 1 \\end{array} \\right) \\] Y así hemos llegado a un sistema ortogonal formado por los vectores \\(u_i\\). Debemos normalizar ahora los vectores de este sistema ortogonal que tenemos, multiplicando cada uno por el inverso de su norma, llegando a la siguiente base ortonormal de \\(U_{ 0 }\\). \\[\\mathcal{B}&#39;_{U_{0}} = \\left\\{\\frac{1}{\\frac{\\sqrt{6}}{2}}\\left( \\begin{array}{c} \\frac{-1}{2}\\\\ \\frac{-1}{2}\\\\ 1\\\\ 0 \\end{array} \\right), \\frac{1}{\\frac{\\sqrt{6}}{2}}\\left( \\begin{array}{c} \\frac{1}{2}\\\\ \\frac{-1}{2}\\\\ 0\\\\ 1 \\end{array} \\right)\\right\\} = \\left\\{\\left( \\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{\\sqrt{6}}{3} \\end{array} \\right)\\right\\}\\] Para el autovalor \\(\\lambda = -3\\): \\[\\begin{array}{rcl}(A + 3 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0&amp; \\Leftrightarrow &amp; \\left( \\begin{array}{cccc@{}} 3 &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; 3 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; 2 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 4 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0 \\Leftrightarrow \\\\&amp; \\Leftrightarrow &amp; \\left\\{\\begin{array}{rrrrcr} 3x &amp; -2y &amp; -z &amp; -t &amp; = &amp; 0\\\\ -2x &amp; + 3y &amp; -z &amp; + t &amp; = &amp; 0\\\\ -x &amp; -y &amp; + 2z &amp; &amp; = &amp; 0\\\\ -x &amp; + y &amp; &amp; + 4t &amp; = &amp; 0\\\\ \\end{array}\\right. \\\\\\end{array}\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de \\(U_{ -3 }\\): \\[\\left( \\begin{array}{cccc|c} 3 &amp; -2 &amp; -1 &amp; -1 &amp; 0\\\\ -2 &amp; 3 &amp; -1 &amp; 1 &amp; 0\\\\ -1 &amp; -1 &amp; 2 &amp; 0 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 4 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|c} 3 &amp; 0 &amp; -3 &amp; 0 &amp; 0\\\\ 0 &amp; \\frac{5}{3} &amp; - \\frac{5}{3} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; \\frac{18}{5} &amp; 0 \\end{array} \\right) \\]\\[\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array}\\right) {}\\] Luego una base de \\(U_{ -3 }\\) será: \\[\\mathcal{B}_{U_{-3}} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) \\right\\}\\] Como solo tenemos un vector en la base, ya forma un sistema ortogonal. Debemos normalizar ahora los vectores de este sistema ortogonal que tenemos, multiplicando cada uno por el inverso de su norma, llegando a la siguiente base ortonormal de \\(U_{ -3 }\\). \\[\\mathcal{B}&#39;_{U_{-3}} = \\left\\{\\frac{1}{\\sqrt{3}}\\left( \\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right)\\right\\} = \\left\\{\\left( \\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array} \\right)\\right\\}\\] Para el autovalor \\(\\lambda = 3\\): \\[\\begin{array}{rcl}(A -3 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0&amp; \\Leftrightarrow &amp; \\left( \\begin{array}{cccc@{}} -3 &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; -3 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -4 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; -2 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0 \\Leftrightarrow \\\\&amp; \\Leftrightarrow &amp; \\left\\{\\begin{array}{rrrrcr} -3x &amp; -2y &amp; -z &amp; -t &amp; = &amp; 0\\\\ -2x &amp; -3y &amp; -z &amp; + t &amp; = &amp; 0\\\\ -x &amp; -y &amp; -4z &amp; &amp; = &amp; 0\\\\ -x &amp; + y &amp; &amp; -2t &amp; = &amp; 0\\\\ \\end{array}\\right. \\\\\\end{array}\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de \\(U_{ 3 }\\): \\[\\left( \\begin{array}{cccc|c} -3 &amp; -2 &amp; -1 &amp; -1 &amp; 0\\\\ -2 &amp; -3 &amp; -1 &amp; 1 &amp; 0\\\\ -1 &amp; -1 &amp; -4 &amp; 0 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; -2 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|c} -3 &amp; 0 &amp; 0 &amp; -3 &amp; 0\\\\ 0 &amp; - \\frac{5}{3} &amp; 0 &amp; \\frac{5}{3} &amp; 0\\\\ 0 &amp; 0 &amp; - \\frac{18}{5} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\]\\[\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} -1\\\\ 1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] Luego una base de \\(U_{ 3 }\\) será: \\[\\mathcal{B}_{U_{3}} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Como solo tenemos un vector en la base, ya forma un sistema ortogonal. Debemos normalizar ahora los vectores de este sistema ortogonal que tenemos, multiplicando cada uno por el inverso de su norma, llegando a la siguiente base ortonormal de \\(U_{ 3 }\\). \\[\\mathcal{B}&#39;_{U_{3}} = \\left\\{\\frac{1}{\\sqrt{3}}\\left( \\begin{array}{c} -1\\\\ 1\\\\ 0\\\\ 1 \\end{array} \\right)\\right\\} = \\left\\{\\left( \\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3} \\end{array} \\right)\\right\\}\\] Paso 3: Hallar una base ortonormal formada por autovectores Definimos \\[\\mathcal{B} = \\mathcal{B}&#39;_{U_{0}} \\cup \\mathcal{B}&#39;_{U_{-3}} \\cup \\mathcal{B}&#39;_{U_{3}} = \\left\\{\\left( \\begin{array}{c} \\frac{-\\sqrt{6}}{6}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ \\frac{\\sqrt{6}}{3}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{6}}{6}\\\\ \\frac{-\\sqrt{6}}{6}\\\\ 0\\\\ \\frac{\\sqrt{6}}{3} \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0\\\\ \\frac{\\sqrt{3}}{3} \\end{array} \\right)\\right\\}\\] Es una base ortonormal de \\(\\mathbb{R}^{4}\\), formada únicamente por autovectores de la matriz \\(A\\). Además, la matriz \\(P\\) del cambio de base de esta base \\(\\mathcal{B}\\) a la canónica, formada por los vectores de \\(\\mathcal{B}\\) puestos por columnas, es ortogonal y nos proporciona la relación entre \\(A\\) y la matriz diagonal \\(D\\) de los autovalores: \\[\\begin{array}{rcl}D &amp; = &amp; P^{\\text{t}}\\ A\\ P = \\\\ &amp; = &amp; \\left( \\begin{array}{cccc@{}} \\frac{-\\sqrt{6}}{6} &amp; \\frac{-\\sqrt{6}}{6} &amp; \\frac{\\sqrt{6}}{3} &amp; 0\\\\ \\frac{\\sqrt{6}}{6} &amp; \\frac{-\\sqrt{6}}{6} &amp; 0 &amp; \\frac{\\sqrt{6}}{3}\\\\ \\frac{\\sqrt{3}}{3} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{\\sqrt{3}}{3} &amp; 0\\\\ \\frac{-\\sqrt{3}}{3} &amp; \\frac{\\sqrt{3}}{3} &amp; 0 &amp; \\frac{\\sqrt{3}}{3} \\end{array} \\right) \\ \\left( \\begin{array}{cccc@{}} 0 &amp; -2 &amp; -1 &amp; -1\\\\ -2 &amp; 0 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -1 &amp; 0\\\\ -1 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right) \\ \\left( \\begin{array}{cccc@{}} \\frac{-\\sqrt{6}}{6} &amp; \\frac{\\sqrt{6}}{6} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{-\\sqrt{3}}{3}\\\\ \\frac{-\\sqrt{6}}{6} &amp; \\frac{-\\sqrt{6}}{6} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{6}}{3} &amp; 0 &amp; \\frac{\\sqrt{3}}{3} &amp; 0\\\\ 0 &amp; \\frac{\\sqrt{6}}{3} &amp; 0 &amp; \\frac{\\sqrt{3}}{3} \\end{array} \\right) \\\\ &amp; = &amp; \\left( \\begin{array}{cccc@{}} 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -3 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 3 \\end{array} \\right) \\end{array}\\] Recordemos que en la diagonal de \\(D\\) se encuentran los autovalores (en el mismo orden en que se han puesto los autovectores en las columnas de \\(P\\)), con sus respectivas multiplicidades. "],
["problems.html", "6 Problemas Resueltos 6.1 Espacios Vectoriales 6.2 Aplicaciones lineales 6.3 Diagonalización 6.4 Espacios Euclídeos", " 6 Problemas Resueltos En este capítulo planteamos varios problemas que recorren la mayoría de los contenidos de este libro y que intentan poner en práctica todas las técnicas de cada tema. En cada problema resuelto, se hará referencia a la sección donde se ha dado la explicación que ayuda a la resolución del ejercicio. 6.1 Espacios Vectoriales Ejercicio Consideremos como espacio vectorial \\(V = \\mathbb{R}^{4}\\) y los subespacios siguientes: \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} 3x+ y-z+ 2t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\]\\[W = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) \\in\\mathbb{R}^{4}:\\begin{array}{ccr} 3x+ 4y &amp; = &amp; 0\\\\ 9x+ 4z+ 4t &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] Hallar la dimensión y una base de \\(U\\) y de \\(W\\). Hallar la dimensión y una base de \\(U\\cap W\\) y de \\(U+W\\). Comprobar el teorema de la dimensión. Encontrar el subespacio suplementario de \\(U\\). Consideremos el vector \\(v\\) dado por sus coordenadas en la base canónica \\(\\left(\\begin{array}{c} -5\\\\ -9\\\\ 1\\\\ -5 \\end{array}\\right)\\). Consideremos también la base: \\[\\mathcal{B}_{1} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ -1\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ 1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ -1\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ 1\\\\ 1 \\end{array} \\right) \\right\\}\\] Encontrar las coordenadas de \\(v\\) en la nueva base. Encontrar las matrices de cambio de base entre \\(\\mathcal{B}_1\\) y \\(\\mathcal{C}\\) (en ambos sentidos), siendo \\(\\mathcal{C}\\) la base canónica. Solución Para encontrar unas bases de \\(U\\) y de \\(W\\) podemos seguir las indicaciones sobre cómo encontrar un sistema generador y una base a partir de la descripción de un subespacio. En el caso del subsepacio \\(U\\), el sistema homogéneo dado por su ecuaciones cartesianas es: \\[\\left\\{\\begin{array}{rrrrcr} 3x &amp; + y &amp; -z &amp; + 2t &amp; = &amp; 0\\\\ \\end{array}\\right.\\] Este sistema, resuelto por Gauss, nos da las siguientes expresiones paramétricas: \\[\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} - \\frac{1}{3}\\\\ 1\\\\ 0\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} \\frac{1}{3}\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\delta\\left(\\begin{array}{c} - \\frac{2}{3}\\\\ 0\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De estas paramétricas podemos extraer un sistema generador y, a partir de él, la base de \\(U\\): \\[\\mathcal{B}_{U} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{1}{3}\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} \\frac{1}{3}\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} - \\frac{2}{3}\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Y, así, \\(\\mathrm{dim}(U) = 3\\). Repetimos todo el proceso para el subespacio \\(W\\). Tomamos su sistema homogéneo asociado: \\[\\left\\{\\begin{array}{rrrrcr} 3x &amp; + 4y &amp; &amp; &amp; = &amp; 0\\\\ 9x &amp; &amp; + 4z &amp; + 4t &amp; = &amp; 0\\\\ \\end{array}\\right.\\] Lo resolvemos por Gauss para obtener las ecuaciones paramétricas del conjunto solución: \\[\\left( \\begin{array}{cccc|c} 3 &amp; 4 &amp; 0 &amp; 0 &amp; 0\\\\ 9 &amp; 0 &amp; 4 &amp; 4 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|c} 3 &amp; 4 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; -12 &amp; 4 &amp; 4 &amp; 0 \\end{array} \\right) \\Rightarrow\\quad\\ \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} - \\frac{4}{3}\\\\ 1\\\\ 1\\\\ 2 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} 0\\\\ 0\\\\ 2\\\\ -2 \\end{array}\\right) {}\\] Por tanto, una base de \\(W\\) será: \\[\\mathcal{B}_{W} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{4}{3}\\\\ 1\\\\ 1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 2\\\\ -2 \\end{array} \\right) \\right\\}\\] Y, así, \\(\\mathrm{dim}(W) = 2\\). El subespacio intersección \\(U\\cap W\\) es el conjunto de vectores que verifican tanto las ecuaciones cartesianas de \\(U\\) como las de \\(W\\). Es decir, son el conjunto solución del sistema homogéneo: \\[\\left\\{\\begin{array}{rrrrcr} 3x &amp; + y &amp; -z &amp; + 2t &amp; = &amp; 0\\\\ 3x &amp; + 4y &amp; &amp; &amp; = &amp; 0\\\\ 9x &amp; &amp; + 4z &amp; + 4t &amp; = &amp; 0\\\\ \\end{array}\\right.\\] Este sistema, resuelto por Gauss, nos da las siguientes expresiones paramétricas: \\[\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} - \\frac{2}{3}\\\\ \\frac{1}{2}\\\\ \\frac{1}{2}\\\\ 1 \\end{array}\\right) {}\\] Por tanto, una base de \\(U\\cap W\\) será: \\[\\mathcal{B}_{U\\cap W} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{2}{3}\\\\ \\frac{1}{2}\\\\ \\frac{1}{2}\\\\ 1 \\end{array} \\right) \\right\\}\\] Y, así, \\(\\mathrm{dim}(U\\cap W) = 1\\). Para la suma \\(U+W\\), sabemos que \\(\\mathcal{B}_U\\cup\\mathcal{B}_W\\) es un sistema generador: \\[\\mathcal{B}_{U}\\cup\\mathcal{B}_{W} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{1}{3}\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} \\frac{1}{3}\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} - \\frac{2}{3}\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} - \\frac{4}{3}\\\\ 1\\\\ 1\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 2\\\\ -2 \\end{array} \\right) \\right\\}\\] Basta con usar eliminación gaussiana en ese conjunto de vectores para eliminar aquellos que sean linealmente dependientes. \\[ \\left(\\begin{array}{cccc} - \\frac{1}{3} &amp; 1 &amp; 0 &amp; 0\\\\ \\frac{1}{3} &amp; 0 &amp; 1 &amp; 0\\\\ - \\frac{2}{3} &amp; 0 &amp; 0 &amp; 1\\\\ - \\frac{4}{3} &amp; 1 &amp; 1 &amp; 2\\\\ 0 &amp; 0 &amp; 2 &amp; -2 \\end{array}\\right) \\sim \\left(\\begin{array}{cccc} - \\frac{1}{3} &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 2 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; -3 \\end{array}\\right) \\] Aquellas filas que hayan acabado siendo enteras de ceros, se corresponden con vectores que originalmente eran dependientes linealmente de los demás. Por tanto, eliminándolos, tenemos una base de \\(U+W\\): \\[\\mathcal{B}_{U + W} = \\left\\{\\left( \\begin{array}{c@{}} - \\frac{1}{3}\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} \\frac{1}{3}\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} - \\frac{2}{3}\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 2\\\\ -2 \\end{array} \\right) \\right\\}\\] Y, así, \\(\\mathrm{dim}(U+W) = 4\\). Podemos comprobar el teorema de la dimensión: \\[\\begin{array}{ccccccc} \\mathrm{dim}(U+W) &amp; = &amp; \\mathrm{dim}(U) &amp; + &amp; \\mathrm{dim}(W) &amp; - &amp; \\mathrm{dim}(U\\cap V) \\\\ 4 &amp; = &amp; 3 &amp; + &amp; 2 &amp; - &amp; 1 \\\\ \\end{array}\\] Calculemos el espacio suplementario de \\(U\\). Tomamos una base de \\(U\\), como la calculada antes, y le aplicamos Gauss para obtener un sistema de vectores en forma escalonada. \\[\\left( \\begin{array}{cccc@{}} - \\frac{1}{3} &amp; 1 &amp; 0 &amp; 0\\\\ \\frac{1}{3} &amp; 0 &amp; 1 &amp; 0\\\\ - \\frac{2}{3} &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc@{}} - \\frac{1}{3} &amp; 0 &amp; 0 &amp; \\frac{1}{2}\\\\ 0 &amp; 1 &amp; 0 &amp; - \\frac{1}{2}\\\\ 0 &amp; 0 &amp; 2 &amp; 1 \\end{array} \\right) \\] Siguiendo las instrucciones de la sección acerca del espacio suplementario, vamos a fijarnos en qué columnas están los pivotes: \\(\\{ 1, 2, 3 \\}\\). De la base canónica \\(\\mathcal{C}\\) eliminamos aquellos vectores que tienen un 1 en las mismas posiciones que los pivotes. Lo que nos quede, ésa es la base del espacio suplementario de \\(U\\): \\[\\mathcal{B}_{\\mathrm{supl}} =\\left\\{\\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Unos escalares \\(a, b, c, d\\) serán las coordenadas de \\(v\\) en la base \\(\\mathcal{B}_1\\) si se verifica: \\[\\left(\\begin{array}{c} -5\\\\ -9\\\\ 1\\\\ -5 \\end{array}\\right) = a\\left(\\begin{array}{c} 1\\\\ -1\\\\ -1\\\\ -1 \\end{array}\\right) {}+b\\left(\\begin{array}{c} -1\\\\ -1\\\\ 1\\\\ -1 \\end{array}\\right) {}+c\\left(\\begin{array}{c} 0\\\\ 0\\\\ -1\\\\ -1 \\end{array}\\right) {}+d\\left(\\begin{array}{c} -1\\\\ -1\\\\ 1\\\\ 1 \\end{array}\\right) {}\\] lo que equivale al siguiente sistema de ecuaciones lineales, que se puede resolver usando métodos Gaussianos, y nos da: \\[\\left\\{\\begin{array}{rrrrcr} a &amp; -b &amp; &amp; -d &amp; = &amp; -5\\\\ -a &amp; -b &amp; &amp; -d &amp; = &amp; -9\\\\ -a &amp; + b &amp; -c &amp; + d &amp; = &amp; 1\\\\ -a &amp; -b &amp; -c &amp; + d &amp; = &amp; -5\\\\ \\end{array}\\right.\\Rightarrow v = \\left(\\begin{array}{c} a\\\\ b\\\\ c\\\\ d \\end{array}\\right) _{\\mathcal{B}_1} = \\left(\\begin{array}{c} 2\\\\ 3\\\\ 4\\\\ 4 \\end{array}\\right) _{\\mathcal{B}_1}\\] El procedimiento para construir la matriz de cambio de base de una base \\(\\mathcal{B}\\) a otra \\(\\mathcal{B}&#39;\\) es el siguiente: Calcular las coordenadas de cada vector en \\(\\mathcal{B}\\) en la base \\(\\mathcal{B}&#39;\\). Eso supone la resolución de \\(n\\) sistemas de ecuaciones lineales, y el procedimiento es igual que el visto en la sección anterior. Construir la matriz poniendo, en forma de columna, las coordenadas calculadas: la primera columna se corresponde con las calculadas para el primer vector de \\(\\mathcal{B}\\), la segunda columna, para las del segundo, etc. Otro aspecto teórico importante es que la matriz de \\(\\mathcal{B}\\) a \\(\\mathcal{B}&#39;\\) es la inversa de la del cambio de base de \\(\\mathcal{B}&#39;\\) a \\(\\mathcal{B}\\). Si una de ellas es fácil de calcular, la otra se puede construir usando, por ejemplo, el método de Gauss-Jordan para el cálculo de inversas. Hay un caso fácil: el cambio de base desde \\(\\mathcal{B}_1\\) a la base canónica \\(\\mathcal{C}\\) es la propia matriz cuyas columnas son los vectores de \\(\\mathcal{B}_1\\): \\[P_{\\mathcal{B}_1\\to\\mathcal{C}} = \\left(\\begin{array}{cccc} 1 &amp; -1 &amp; 0 &amp; -1\\\\ -1 &amp; -1 &amp; 0 &amp; -1\\\\ -1 &amp; 1 &amp; -1 &amp; 1\\\\ -1 &amp; -1 &amp; -1 &amp; 1 \\end{array}\\right) \\] La matriz del cambio de base opuesto la vamos a calcular buscando la matriz inversa de la de \\(\\mathcal{B}_1\\) a \\(\\mathcal{C}\\). Para ello, se puede usar la eliminación de Gauss-Jordan: \\[\\left( \\begin{array}{cccc|cccc} 1 &amp; -1 &amp; 0 &amp; -1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\ -1 &amp; -1 &amp; 0 &amp; -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ -1 &amp; 1 &amp; -1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ -1 &amp; -1 &amp; -1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|cccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{2} &amp; - \\frac{1}{2} &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{2} &amp; - \\frac{1}{2}\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 0 &amp; -1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; - \\frac{1}{2} &amp; - \\frac{1}{2} &amp; - \\frac{1}{2} &amp; \\frac{1}{2} \\end{array} \\right) \\] Luego \\[P_{\\mathcal{C}\\to\\mathcal{B}_1} = \\left(\\begin{array}{cccc} \\frac{1}{2} &amp; - \\frac{1}{2} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; \\frac{1}{2} &amp; - \\frac{1}{2}\\\\ -1 &amp; 0 &amp; -1 &amp; 0\\\\ - \\frac{1}{2} &amp; - \\frac{1}{2} &amp; - \\frac{1}{2} &amp; \\frac{1}{2} \\end{array}\\right) \\] 6.2 Aplicaciones lineales Ejercicio Consideremos la aplicación \\(f:V=\\mathbb{R}^{3}\\to W=\\mathbb{R}^{2}\\) dada por: \\[f \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left( \\begin{array}{c} 2x+ y-z\\\\ 2x+ 2z\\\\ \\end{array} \\right)\\] Encontrar la matriz asociada a \\(f\\) en las bases canónicas y en las bases: \\[\\mathcal{B}_{1} = \\{v_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ -1 \\end{array} \\right) \\right\\}\\]\\[\\mathcal{B}_{2} = \\{w_i\\} = \\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 2 \\end{array} \\right) \\right\\}\\] Dar una base y la dimensión del núcleo y de la imagen de \\(f\\). Identificar si \\(f\\) es inyectiva o sobreyectiva. Comprobar el teorema de la dimensión de rango y nulidad. Calcular una base de \\(f(U)\\), donde \\(U\\) es el subespacio de \\(V\\) dado por: \\[U = \\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) \\in\\mathbb{R}^{3}:\\begin{array}{ccr} 2x+ 2y+ z &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] Solución Para hallar la matriz asociada a la aplicación \\(f\\) en las bases canónicas, basta con mirar los coeficientes de las variables \\(x,y,\\ldots\\) en la expresión de \\(f\\) y ponerlos por columnas. De esta forma: \\[A =\\left( \\begin{array}{ccc@{}} 2 &amp; 1 &amp; -1\\\\ 2 &amp; 0 &amp; 2 \\end{array} \\right) \\] Para calcular la matriz \\(A&#39;\\) asociada a \\(f\\) con respecto a las nuevas bases, tenemos dos opciones: Calcular la imagen de los vectores de \\(\\mathcal{B}_1\\) y encontrar sus coordenadas con respecto a la base \\(\\mathcal{B}_2\\), y ponerlas por columnas. Calcular las matrices \\(P\\) y \\(Q\\), de cambio de base desde la canónica en \\(V\\) y en \\(W\\) a \\(\\mathcal{B}_1\\) y \\(\\mathcal{B}_2\\), respectivamente, y usarlas para calcular \\(A&#39; = Q^{-1}\\ A\\ P\\). Ambos caminos son completamente equivalentes y, de hecho, uno se deduce del otro. Por tanto, en cada situación debemos ver cuál de los dos nos puede resultar más sencillo. En este caso, optamos por el camino 1. Llamemos \\(Y = (f(v_1)|f(v_2)|\\ldots|f(v_n))\\) a la matriz que tiene por columnas las imágenes de los vectores de \\(\\mathcal{B}_1\\) mediante \\(f\\): \\[Y = \\left( \\begin{array}{ccc@{}} 2 &amp; -1 &amp; 1\\\\ 2 &amp; -2 &amp; -2 \\end{array} \\right) \\] y \\(B_2 = (w_1|w_2|\\ldots|w_m)\\) a la matriz que tiene por columnas los vectores de \\(\\mathcal{B}_2\\). Entonces \\(A&#39; = B_2^{-1}\\ Y\\), que podemos calcular mediante Gauss-Jordan, partiendo de \\((B_2|Y)\\) y llegando a \\((I_m|A&#39;)\\): \\[\\left( \\begin{array}{cc|ccc} 2 &amp; 0 &amp; 2 &amp; -1 &amp; 1\\\\ 1 &amp; 2 &amp; 2 &amp; -2 &amp; -2 \\end{array} \\right) \\sim\\left( \\begin{array}{cc|ccc} 1 &amp; 0 &amp; 1 &amp; - \\frac{1}{2} &amp; \\frac{1}{2}\\\\ 0 &amp; 1 &amp; \\frac{1}{2} &amp; - \\frac{3}{4} &amp; - \\frac{5}{4} \\end{array} \\right) \\] Luego \\[A&#39; =\\left( \\begin{array}{ccc@{}} 1 &amp; - \\frac{1}{2} &amp; \\frac{1}{2}\\\\ \\frac{1}{2} &amp; - \\frac{3}{4} &amp; - \\frac{5}{4} \\end{array} \\right) \\] Comenzamos por estudiar el núcleo de \\(f\\). Sabemos que el núcleo \\(\\mathrm{Ker}\\ f\\) es el conjunto de vectores \\(v\\in V\\) tal que \\(f(v) = 0\\), y que eso coincide con el conjunto de soluciones del sistema de ecuaciones homogéneo cuya matriz de coeficientes es \\(A\\). Todo esto se resume en que las ecuaciones cartesianas de \\(\\mathrm{Ker}\\ f\\) son aquellas cuya matriz de coeficientes es \\(A\\): \\[\\mathrm{Ker}\\ f =\\left\\{\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) \\in V:\\begin{array}{ccr} 2x+ y-z &amp; = &amp; 0\\\\ 2x+ 2z &amp; = &amp; 0\\\\ \\end{array}\\right\\}\\] Resolvemos este sistema por Gauss para encontrar las ecuaciones paramétricas y, a partir de ahí, una base del núcleo: \\[\\left( \\begin{array}{ccc|c} 2 &amp; 1 &amp; -1 &amp; 0\\\\ 2 &amp; 0 &amp; 2 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{ccc|c} 2 &amp; 0 &amp; 2 &amp; 0\\\\ 0 &amp; -1 &amp; 3 &amp; 0 \\end{array} \\right) \\] Luego nos quedan las siguientes ecuaciones paramétricas para \\(\\mathrm{Ker}\\ f\\): \\[\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) =\\alpha\\left(\\begin{array}{c} -1\\\\ 3\\\\ 1 \\end{array}\\right) {}\\] Entonces la base del núcleo de \\(f\\) es: \\[\\mathcal{B}_{\\mathrm{Ker}\\ f} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 3\\\\ 1 \\end{array} \\right) \\right\\}\\] y \\(\\mathrm{dim}(\\mathrm{Ker}\\ f) = 1\\). Con respecto a la imagen de \\(f\\), hemos de recordar que \\(\\mathrm{Im}\\ f\\) es el subespacio de \\(W\\) generado por los vectores que forman las columnas de la matriz asociada \\(A\\). Por tanto, un sistema generador de \\(\\mathrm{Im}\\ f\\) será: \\[\\mathcal{G} =\\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 2 \\end{array} \\right) \\right\\}\\] Desde este sistema generador, eliminamos los vectores linealmente dependientes para obtener una base de \\(\\mathrm{Im}\\ f\\). Usamos Gauss con los vectores de \\(\\mathcal{G}\\) por filas, y eliminamos aquellos que, al final, se hayan convertido en filas de ceros: \\[\\left( \\begin{array}{cc@{}} 2 &amp; 2\\\\ 1 &amp; 0\\\\ -1 &amp; 2 \\end{array} \\right) \\sim\\left( \\begin{array}{cc@{}} 2 &amp; 2\\\\ 0 &amp; -1\\\\ 0 &amp; 0 \\end{array} \\right) \\] Luego \\[\\mathcal{B}_{\\mathrm{Im}\\ f} =\\left\\{\\left( \\begin{array}{c@{}} 2\\\\ 2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0 \\end{array} \\right) \\right\\}\\] y \\(\\mathrm{dim}(\\mathrm{Im}\\ f) = 2\\). Para determinar si \\(f\\) es inyectiva o sobreyectiva, nos fijaremos en las dimensiones de su núcleo y de su imagen: \\(f\\) será inyectiva si y solo si se cumple \\(\\mathrm{dim}(\\mathrm{Ker}\\ f) = 0\\). Sabemos que \\(\\mathrm{dim}(\\mathrm{Ker}\\ f) = 1\\), luego \\(f\\) no es inyectiva. \\(f\\) será sobreyectiva si y solo si se cumple que \\(\\mathrm{dim}(\\mathrm{Im}\\ f) = \\mathrm{dim}(W) = \\mathrm{dim}(\\mathbb{R}^{2})\\), y esto se cumple, como hemos visto en el apartado anterior, luego \\(f\\) es sobreyectiva. El teorema de la dimensión para núcleo e imagen dice que \\(\\mathrm{dim}(V) = \\mathrm{dim}(\\mathrm{Ker}\\ f) + \\mathrm{dim}(\\mathrm{Im}\\ f)\\). Lo podemos comprobar fácilmente: \\[\\begin{array}{ccccc} \\mathrm{dim}(\\mathbb{R}^{3}) &amp; = &amp; \\mathrm{dim}(\\mathrm{Ker}\\ f) &amp; + &amp; \\mathrm{dim}(\\mathrm{Im}\\ f) \\\\ 3 &amp; = &amp; 1 &amp; + &amp; 2 \\\\ \\end{array}\\] Dado el subespacio \\(U\\), debemos encontrar un sistema de vectores \\(\\mathcal{G}\\) que lo genere, calcular su imagen \\(f(\\mathcal{G})\\), y eso será un sistema generador de \\(f(U)\\). A partir de ahí, podemos extraer una base de \\(f(U)\\). Para calcular un sistema generador de \\(U\\), pasamos sus ecuaciones cartesianas a paramétricas, usando métodos Gaussianos: \\[\\left\\{\\begin{array}{rrrcr} 2x &amp; + 2y &amp; + z &amp; = &amp; 0\\\\ \\end{array}\\right.\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} -1\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} - \\frac{1}{2}\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De aquí podemos determinar una base de \\(U\\), si eliminamos sus vectores linealmente dependientes: \\[\\mathcal{B}_U = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} - \\frac{1}{2}\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Ahora procedemos a calcular \\(f(\\mathcal{B}_U)\\): \\[f(\\mathcal{B}_U) = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ 1 \\end{array} \\right) \\right\\}\\] que es un sistema generador de \\(f(U)\\). Eliminamos los vectores linealmente dependientes (en este caso es sencillo) para obtener una base de \\(f(U)\\): \\[\\mathcal{B}_{f(U)} =\\left\\{\\left( \\begin{array}{c@{}} -1\\\\ -2 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ 1 \\end{array} \\right) \\right\\}\\] y así \\(\\mathrm{dim}(f(U)) = 2\\). 6.3 Diagonalización Ejercicio Consideremos el siguiente endomorfismo \\(f:\\mathbb{R}^{3}\\to\\mathbb{R}^{3}\\), dado por \\[f\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\left(\\begin{array}{c} 5x-3z\\\\ x+ 8y+ z\\\\ -x+ 7z\\\\ \\end{array}\\right)\\] Hallar todos los autovalores de \\(f\\). Hallar los subespacios de autovectores de \\(f\\). Comprobar si \\(f\\) es diagonalizable y dar una base \\(\\mathcal{B}\\) de \\(\\mathbb{R}^{3}\\) tal que la matriz asociada a \\(f\\) en dicha base sea diagonal. Expresar y calcular, como suma de potencias de grado menor que \\(3\\), tanto \\(A^{-1}\\) como \\(A^4\\), siendo \\(A\\) la matriz asociada a \\(f\\) en la base canónica. Solución Como paso previo a todos los cálculos, debemos determinar la matriz asociada a \\(f\\) en las bases canónicas: \\[A = \\left( \\begin{array}{ccc@{}} 5 &amp; 0 &amp; -3\\\\ 1 &amp; 8 &amp; 1\\\\ -1 &amp; 0 &amp; 7 \\end{array} \\right) \\] Para calcular los autovalores de \\(f\\), o equivalentemente, los de \\(A\\), debemos construir el polinomio característico: \\[\\begin{array}{rcl}p(\\lambda) &amp; = &amp; \\mathrm{det}(A - \\lambda\\ I) = \\\\ &amp; = &amp; \\mathrm{det}\\left(\\left( \\begin{array}{ccc@{}} 5 &amp; 0 &amp; -3\\\\ 1 &amp; 8 &amp; 1\\\\ -1 &amp; 0 &amp; 7 \\end{array} \\right) - \\lambda \\left( \\begin{array}{ccc@{}} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right) \\right) = \\\\ &amp; = &amp; \\mathrm{det}\\left( \\begin{array}{ccc@{}} 5-\\lambda &amp; 0 &amp; -3\\\\ 1 &amp; 8-\\lambda &amp; 1\\\\ -1 &amp; 0 &amp; 7-\\lambda \\end{array} \\right) = \\\\ &amp; = &amp; \\lambda^{3}-20\\lambda^{2}+128\\lambda-256\\end{array}\\] Factorizamos el polinomio para hallar sus raíces (los autovalores) más fácilmente: \\[p(\\lambda) = (\\lambda -4)\\cdot (\\lambda -8)^{2}\\] Por tanto, los autovalores de \\(f\\) son \\(\\{4, 8\\}\\). Para calcular el subespacio asociado a un autovector \\(\\lambda\\), debemos hallar la solución del sistema homogéneo \\((A-\\lambda\\ I)x = 0\\): Para el autovalor \\(\\lambda = 4\\): \\[(A -4 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = 0\\Leftrightarrow\\left( \\begin{array}{ccc@{}} 1 &amp; 0 &amp; -3\\\\ 1 &amp; 4 &amp; 1\\\\ -1 &amp; 0 &amp; 3 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = 0\\Leftrightarrow\\left\\{\\begin{array}{rrrcr} x &amp; &amp; -3z &amp; = &amp; 0\\\\ x &amp; + 4y &amp; + z &amp; = &amp; 0\\\\ -x &amp; &amp; + 3z &amp; = &amp; 0\\\\ \\end{array}\\right.\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de su conjunto solución, que es \\(U_{ 4 }\\): \\[\\left( \\begin{array}{ccc|c} 1 &amp; 0 &amp; -3 &amp; 0\\\\ 1 &amp; 4 &amp; 1 &amp; 0\\\\ -1 &amp; 0 &amp; 3 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{ccc|c} 1 &amp; 0 &amp; -3 &amp; 0\\\\ 0 &amp; 4 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 3\\\\ -1\\\\ 1 \\end{array}\\right) {}\\] De aquí que una base de \\(U_{ 4 }\\) sea: \\[\\mathcal{B}_{U_{4}} = \\left\\{\\left( \\begin{array}{c@{}} 3\\\\ -1\\\\ 1 \\end{array} \\right) \\right\\}\\] - Para el autovalor \\(\\lambda = 8\\): \\[(A -8 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = 0\\Leftrightarrow\\left( \\begin{array}{ccc@{}} -3 &amp; 0 &amp; -3\\\\ 1 &amp; 0 &amp; 1\\\\ -1 &amp; 0 &amp; -1 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = 0\\Leftrightarrow\\left\\{\\begin{array}{rrrcr} -3x &amp; &amp; -3z &amp; = &amp; 0\\\\ x &amp; &amp; + z &amp; = &amp; 0\\\\ -x &amp; &amp; -z &amp; = &amp; 0\\\\ \\end{array}\\right.\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de su conjunto solución, que es \\(U_{ 8 }\\): \\[\\left( \\begin{array}{ccc|c} -3 &amp; 0 &amp; -3 &amp; 0\\\\ 1 &amp; 0 &amp; 1 &amp; 0\\\\ -1 &amp; 0 &amp; -1 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{ccc|c} -3 &amp; 0 &amp; -3 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 0\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} -1\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] De aquí que una base de \\(U_{ 8 }\\) sea: \\[\\mathcal{B}_{U_{8}} = \\left\\{\\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] Estudiamos si \\(f\\) es diagonalizable. Para ello, tenemos en cuenta la multiplicidad algebraica y la geométrica de cada autovalor, como presentamos en la siguiente tabla: \\[ \\begin{array}{ccc@{}} \\text{Autovalor} &amp; \\text{Mult. Algebraica} &amp; \\text{Mult. Geométrica}\\\\ 4 &amp; 1 &amp; 1\\\\ 8 &amp; 2 &amp; 2 \\end{array} \\] Con esto, queda demostrado que el endomorfismo \\(f\\) sí es diagonalizable. Además, la base \\(\\mathcal{B}\\) de autovectores es: \\[\\mathcal{B} = \\mathcal{B}_{4}\\cup\\ \\mathcal{B}_{8} = \\left\\{\\left( \\begin{array}{c@{}} 3\\\\ -1\\\\ 1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] La matriz de cambio de base de \\(\\mathcal{B}\\) a \\(\\mathcal{C}\\) es la que tiene los elementos de \\(\\mathcal{B}\\) por columnas: \\[P = \\left( \\begin{array}{ccc@{}} 3 &amp; 0 &amp; -1\\\\ -1 &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) \\] Y la matriz diagonal, asociada a \\(f\\) en la nueva base, es la que tiene los autovalores en la diagonal, que se calcula como: \\[\\begin{array}{rcl}D &amp; = &amp; P^{-1}\\ A\\ P = \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} 3 &amp; 0 &amp; -1\\\\ -1 &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) ^{-1}\\left( \\begin{array}{ccc@{}} 5 &amp; 0 &amp; -3\\\\ 1 &amp; 8 &amp; 1\\\\ -1 &amp; 0 &amp; 7 \\end{array} \\right) \\left( \\begin{array}{ccc@{}} 3 &amp; 0 &amp; -1\\\\ -1 &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 \\end{array} \\right) \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} 4 &amp; 0 &amp; 0\\\\ 0 &amp; 8 &amp; 0\\\\ 0 &amp; 0 &amp; 8 \\end{array} \\right) \\end{array}\\] Ahora aplicaremos el Teorema de Cayley-Hamilton para poder expresar la inversas de \\(A\\) y \\(A^4\\) como suma de potencias de grado menor que \\(3\\). El teorema me dice que la matriz \\(A\\) anula su polinomio característico, es decir: \\[p(A) = A^{3}-20A^{2}+128A-256I = 0\\] Despejando la identidad, nos queda: \\[\\begin{array}{rcl}\\displaystyle I &amp; = &amp; - \\frac{1}{256} \\left(A^{3}-20A^{2}+128A\\right) = \\\\ &amp; = &amp; \\left(\\frac{1}{256}A^{2}- \\frac{5}{64}A+\\frac{1}{2}I\\right)\\ A\\end{array}\\] Luego tenemos \\[\\begin{array}{rcl}A^{-1} &amp; = &amp; \\frac{1}{256}A^{2}- \\frac{5}{64}A+\\frac{1}{2}I =\\\\ &amp; = &amp; +\\frac{1}{256}\\left( \\begin{array}{ccc@{}} 28 &amp; 0 &amp; -36\\\\ 12 &amp; 64 &amp; 12\\\\ -12 &amp; 0 &amp; 52 \\end{array} \\right) - \\frac{5}{64}\\left( \\begin{array}{ccc@{}} 5 &amp; 0 &amp; -3\\\\ 1 &amp; 8 &amp; 1\\\\ -1 &amp; 0 &amp; 7 \\end{array} \\right) +\\frac{1}{2}\\left( \\begin{array}{ccc@{}} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right) =\\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} \\frac{7}{32} &amp; 0 &amp; \\frac{3}{32}\\\\ - \\frac{1}{32} &amp; \\frac{1}{8} &amp; - \\frac{1}{32}\\\\ \\frac{1}{32} &amp; 0 &amp; \\frac{5}{32} \\end{array} \\right) \\end{array}\\] Calculemos ahora \\(A^4\\). Tenemos que partir de despejar el término \\(A^{3}\\) de la igualdad que nos proporciona el Teorema de Cayley-Hamilton: \\[A^{3} = 20A^{2}-128A+256I\\] Para \\(A^4\\), tenemos: \\[A^4 = A\\cdot A^3 = A \\left(20A^{2}-128A+256I\\right) = 20A^{3}-128A^{2}+256A\\] Si queremos dejarlo como suma de potencias de \\(A\\) de grado menor que \\(3\\), debemos sustituir aquí \\(A^{3}\\) por la expresión que hemos despejado hace un momento. \\[\\begin{array}{rcl}A^4 &amp; = &amp; 20 \\left( 20A^{2}-128A+256I \\right)-128A^{2}+256A = \\\\ &amp; = &amp; 272A^{2}-2304A+5120I = \\\\ &amp; = &amp; 272\\left( \\begin{array}{ccc@{}} 28 &amp; 0 &amp; -36\\\\ 12 &amp; 64 &amp; 12\\\\ -12 &amp; 0 &amp; 52 \\end{array} \\right) -2304\\left( \\begin{array}{ccc@{}} 5 &amp; 0 &amp; -3\\\\ 1 &amp; 8 &amp; 1\\\\ -1 &amp; 0 &amp; 7 \\end{array} \\right) +5120\\left( \\begin{array}{ccc@{}} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right) = \\\\ &amp; = &amp; \\left( \\begin{array}{ccc@{}} 1216 &amp; 0 &amp; -2880\\\\ 960 &amp; 4096 &amp; 960\\\\ -960 &amp; 0 &amp; 3136 \\end{array} \\right) \\end{array}\\] 6.4 Espacios Euclídeos Ejercicio Consideremos los vectores \\[u = \\left(\\begin{array}{c} -1\\\\ 2\\\\ 0\\\\ -2 \\end{array}\\right) \\quad\\quad v= \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 2 \\end{array}\\right) \\] Hallar el ángulo que forman \\(u\\) y \\(v\\), la norma de cada uno de ellos, \\(d(u,v)\\), y normalizarlos. Encontrar el complemento ortogonal del subespacio generado por el siguiente sistema de vectores: \\[\\mathcal{B} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 0\\\\ -1 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 0\\\\ 0\\\\ 2\\\\ -2 \\end{array} \\right) \\right\\}\\] Diagonalizar ortogonalmente la matriz \\[A = \\left( \\begin{array}{cccc@{}} -1 &amp; 1 &amp; -2 &amp; 1\\\\ 1 &amp; -1 &amp; 2 &amp; -1\\\\ -2 &amp; 2 &amp; -4 &amp; 2\\\\ 1 &amp; -1 &amp; 2 &amp; -1 \\end{array} \\right) \\] hallando una base ortonormal de autovectores y expresando de forma explícita la matriz diagonal en función de la matriz \\(A\\) y de la matriz de cambio de base. Solución Vamos a calcular tanto el producto escalar de \\(u\\) y \\(v\\) como sus normas: \\[\\langle u, v \\rangle = \\langle \\left(\\begin{array}{c} -1\\\\ 2\\\\ 0\\\\ -2 \\end{array}\\right) , \\left(\\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 2 \\end{array}\\right) \\rangle = (-1)\\cdot1+2\\cdot1+0\\cdot1+(-2)\\cdot2 = -3\\]\\[\\|u\\| = \\sqrt{\\langle u, u \\rangle} = \\sqrt{(-1)^2+2^2+0^2+(-2)^2} = 3\\]\\[\\|v\\| = \\sqrt{\\langle v, v \\rangle} = \\sqrt{1^2+1^2+1^2+2^2} = \\sqrt{7}\\] Entonces \\[\\mathrm{ang}(u, v) = \\arccos\\frac{\\langle u, v \\rangle}{\\|u\\| \\|v\\|} = \\arccos\\frac{-3}{3\\sqrt{7}} = \\arccos \\frac{-\\sqrt{7}}{7}\\] Pasamos a normalizar cada vector: \\[\\frac{u}{\\|u\\|} = \\frac{1}{3}\\left( \\begin{array}{c} -1\\\\ 2\\\\ 0\\\\ -2 \\end{array} \\right) = \\left( \\begin{array}{c} \\frac{-1}{3}\\\\ \\frac{2}{3}\\\\ 0\\\\ \\frac{-2}{3} \\end{array} \\right),\\quad\\quad \\frac{v}{\\|v\\|} = \\frac{\\sqrt{7}}{7}\\left( \\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 2 \\end{array} \\right) = \\left( \\begin{array}{c} \\frac{\\sqrt{7}}{7}\\\\ \\frac{\\sqrt{7}}{7}\\\\ \\frac{\\sqrt{7}}{7}\\\\ \\frac{2\\cdot\\sqrt{7}}{7} \\end{array} \\right)\\] Por último, calculamos la distancia: \\[d(u,v) = \\|u-v\\| = \\left\\|\\left( \\begin{array}{c} -1\\\\ 2\\\\ 0\\\\ -2 \\end{array} \\right) - \\left( \\begin{array}{c} 1\\\\ 1\\\\ 1\\\\ 2 \\end{array} \\right)\\right\\| = \\left\\|\\left( \\begin{array}{c} -2\\\\ 1\\\\ -1\\\\ -4 \\end{array} \\right)\\right\\| = \\sqrt{22}\\] Para calcular el complemento ortogonal del subespacio \\(U\\), partimos de la base \\(\\mathcal{B}\\), y buscamos aquellos vectores \\(v =\\) de \\(V\\) cuyo producto escalar por los vectores de la base sea 0. Concretamente, usando el producto escalar usual en \\(\\mathbb{R}^{4}\\), esto equivale a buscar las soluciones del sistema de ecuaciones lineales homogéneo: \\[\\begin{array}{rrrrcr} -x &amp; + y &amp; &amp; -t &amp; = &amp; 0\\\\ &amp; &amp; 2z &amp; -2t &amp; = &amp; 0\\\\ \\end{array}\\] donde los coeficientes de cada fila son las coordenadas del vector correspondiente de \\(\\mathcal{B}\\). De esta forma, este sistema representa las ecuaciones cartesianas del espacio \\(U^{\\perp}\\). Podemos resolver el sistema usando Gauss-Jordan, y despejando para encontrar las ecuaciones paramétricas del conjunto solución: \\[\\left( \\begin{array}{cccc@{}} -1 &amp; 1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; 2 &amp; -2 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc@{}} -1 &amp; 1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; 2 &amp; -2 \\end{array} \\right) \\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array}\\right) {}\\] de donde podemos deducir una base para \\(U^{\\perp}\\): \\[\\mathcal{B}&#39; = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 0\\\\ 1\\\\ 1 \\end{array} \\right) \\right\\}\\] Comenzamos por hallar el polinomio característico de \\(A\\) y calculando sus autovalores. \\[\\begin{array}{rcl}p(\\lambda) &amp; = &amp; \\mathrm{det}(A - \\lambda\\ I) = \\\\ &amp; = &amp; \\mathrm{det}\\left(\\left( \\begin{array}{cccc@{}} -1 &amp; 1 &amp; -2 &amp; 1\\\\ 1 &amp; -1 &amp; 2 &amp; -1\\\\ -2 &amp; 2 &amp; -4 &amp; 2\\\\ 1 &amp; -1 &amp; 2 &amp; -1 \\end{array} \\right) - \\lambda \\left( \\begin{array}{cccc@{}} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right) \\right) = \\\\ &amp; = &amp; \\mathrm{det}\\left( \\begin{array}{cccc@{}} -1-\\lambda &amp; 1 &amp; -2 &amp; 1\\\\ 1 &amp; -1-\\lambda &amp; 2 &amp; -1\\\\ -2 &amp; 2 &amp; -4-\\lambda &amp; 2\\\\ 1 &amp; -1 &amp; 2 &amp; -1-\\lambda \\end{array} \\right) \\\\ &amp; = &amp; \\lambda^{4}+7\\lambda^{3}\\end{array}\\] Igualándolo a 0 y resolviendo la ecuación que queda, llegamos a que los autovalores son: \\(\\lambda = 0, -7\\), ya que: \\[p(\\lambda) = \\lambda ^{3}\\cdot (\\lambda +7)\\] Recorremos ahora cada autovalor, encontrando una base del subespacio propio asociado, y ortonormalizándola usando el método de Gram-Schmidt. Para el autovalor \\(\\lambda = 0\\): \\[\\begin{array}{rcl}(A - 0 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0&amp; \\Leftrightarrow &amp; \\left( \\begin{array}{cccc@{}} -1 &amp; 1 &amp; -2 &amp; 1\\\\ 1 &amp; -1 &amp; 2 &amp; -1\\\\ -2 &amp; 2 &amp; -4 &amp; 2\\\\ 1 &amp; -1 &amp; 2 &amp; -1 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0 \\Leftrightarrow \\\\&amp; \\Leftrightarrow &amp; \\left\\{\\begin{array}{rrrrcr} -x &amp; + y &amp; -2z &amp; + t &amp; = &amp; 0\\\\ x &amp; -y &amp; + 2z &amp; -t &amp; = &amp; 0\\\\ -2x &amp; + 2y &amp; -4z &amp; + 2t &amp; = &amp; 0\\\\ x &amp; -y &amp; + 2z &amp; -t &amp; = &amp; 0\\\\ \\end{array}\\right. \\\\\\end{array}\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de \\(U_{ 0 }\\): \\[\\left( \\begin{array}{cccc|c} -1 &amp; 1 &amp; -2 &amp; 1 &amp; 0\\\\ 1 &amp; -1 &amp; 2 &amp; -1 &amp; 0\\\\ -2 &amp; 2 &amp; -4 &amp; 2 &amp; 0\\\\ 1 &amp; -1 &amp; 2 &amp; -1 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|c} -1 &amp; 1 &amp; -2 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\]\\[\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array}\\right) {}+\\beta\\left(\\begin{array}{c} -2\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) {}+\\gamma\\left(\\begin{array}{c} 1\\\\ 0\\\\ 0\\\\ 1 \\end{array}\\right) {}\\] Luego una base de \\(U_{ 0 }\\) es: \\[\\mathcal{B}_{U_{0}} = \\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -2\\\\ 0\\\\ 1\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} 1\\\\ 0\\\\ 0\\\\ 1 \\end{array} \\right) \\right\\}\\] El siguiente paso es ortonormalizar esta base, mediante el método de Gram-Schmidt. Llamemos \\(v_i\\) a los vectores de esta base hallada. Llamamos \\[u_1 = v_1 = \\left(\\begin{array}{c} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array}\\right) \\] Procedemos con el resto de vectores: Construimos: \\[U_1 = \\mathcal{L}(\\{u_1\\}) = \\mathcal{L}(\\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) \\right\\})\\] Entonces \\[\\mathrm{proy}_{U_1}(v_2) = \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1 = \\frac{-2}{2} \\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ 0\\\\ 0 \\end{array} \\right) \\] Y llamamos \\[u_2 = v_2 - \\mathrm{proy}_{U_1}(v_2) = \\left(\\begin{array}{c} -2\\\\ 0\\\\ 1\\\\ 0 \\end{array}\\right) - \\left( \\begin{array}{c@{}} -1\\\\ -1\\\\ 0\\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) \\] Construimos: \\[U_2 = \\mathcal{L}(\\{u_1, u_2\\}) = \\mathcal{L}(\\left\\{\\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) , \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) \\right\\})\\] Entonces \\[\\mathrm{proy}_{U_2}(v_3) = \\frac{\\langle v_3, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1+\\frac{\\langle v_3, u_2 \\rangle}{\\langle u_2, u_2 \\rangle} u_2 = \\frac{1}{2} \\left( \\begin{array}{c@{}} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right) +\\frac{-1}{3} \\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c@{}} \\frac{5}{6}\\\\ \\frac{1}{6}\\\\ - \\frac{1}{3}\\\\ 0 \\end{array} \\right) \\] Y llamamos \\[u_3 = v_3 - \\mathrm{proy}_{U_2}(v_3) = \\left(\\begin{array}{c} 1\\\\ 0\\\\ 0\\\\ 1 \\end{array}\\right) - \\left( \\begin{array}{c@{}} \\frac{5}{6}\\\\ \\frac{1}{6}\\\\ - \\frac{1}{3}\\\\ 0 \\end{array} \\right) = \\left( \\begin{array}{c@{}} \\frac{1}{6}\\\\ - \\frac{1}{6}\\\\ \\frac{1}{3}\\\\ 1 \\end{array} \\right) \\] Y así hemos llegado a un sistema ortogonal formado por los vectores \\(u_i\\). Debemos normalizar ahora los vectores de este sistema ortogonal que tenemos, multiplicando cada uno por el inverso de su norma, llegando a la siguiente base ortonormal de \\(U_{ 0 }\\). \\[\\mathcal{B}&#39;_{U_{0}} = \\left\\{\\frac{1}{\\sqrt{2}}\\left( \\begin{array}{c} 1\\\\ 1\\\\ 0\\\\ 0 \\end{array} \\right), \\frac{1}{\\sqrt{3}}\\left( \\begin{array}{c} -1\\\\ 1\\\\ 1\\\\ 0 \\end{array} \\right), \\frac{1}{\\frac{\\sqrt{42}}{6}}\\left( \\begin{array}{c} \\frac{1}{6}\\\\ \\frac{-1}{6}\\\\ \\frac{1}{3}\\\\ 1 \\end{array} \\right)\\right\\} = \\left\\{\\left( \\begin{array}{c} \\frac{\\sqrt{2}}{2}\\\\ \\frac{\\sqrt{2}}{2}\\\\ 0\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{42}}{42}\\\\ \\frac{-\\sqrt{42}}{42}\\\\ \\frac{\\sqrt{42}}{21}\\\\ \\frac{\\sqrt{42}}{7} \\end{array} \\right)\\right\\}\\] Para el autovalor \\(\\lambda = -7\\): \\[\\begin{array}{rcl}(A + 7 I)\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0&amp; \\Leftrightarrow &amp; \\left( \\begin{array}{cccc@{}} 6 &amp; 1 &amp; -2 &amp; 1\\\\ 1 &amp; 6 &amp; 2 &amp; -1\\\\ -2 &amp; 2 &amp; 3 &amp; 2\\\\ 1 &amp; -1 &amp; 2 &amp; 6 \\end{array} \\right) \\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = 0 \\Leftrightarrow \\\\&amp; \\Leftrightarrow &amp; \\left\\{\\begin{array}{rrrrcr} 6x &amp; + y &amp; -2z &amp; + t &amp; = &amp; 0\\\\ x &amp; + 6y &amp; + 2z &amp; -t &amp; = &amp; 0\\\\ -2x &amp; + 2y &amp; + 3z &amp; + 2t &amp; = &amp; 0\\\\ x &amp; -y &amp; + 2z &amp; + 6t &amp; = &amp; 0\\\\ \\end{array}\\right. \\\\\\end{array}\\] Resolvemos este sistema por Gauss-Jordan, encontrando la forma paramétrica de \\(U_{ -7 }\\): \\[\\left( \\begin{array}{cccc|c} 6 &amp; 1 &amp; -2 &amp; 1 &amp; 0\\\\ 1 &amp; 6 &amp; 2 &amp; -1 &amp; 0\\\\ -2 &amp; 2 &amp; 3 &amp; 2 &amp; 0\\\\ 1 &amp; -1 &amp; 2 &amp; 6 &amp; 0 \\end{array} \\right) \\sim\\left( \\begin{array}{cccc|c} 6 &amp; 0 &amp; 0 &amp; 6 &amp; 0\\\\ 0 &amp; \\frac{35}{6} &amp; 0 &amp; - \\frac{35}{6} &amp; 0\\\\ 0 &amp; 0 &amp; \\frac{7}{5} &amp; \\frac{14}{5} &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\]\\[\\Rightarrow\\left(\\begin{array}{c} x\\\\ y\\\\ z\\\\ t \\end{array}\\right) = \\alpha\\left(\\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 1 \\end{array}\\right) {}\\] Luego una base de \\(U_{ -7 }\\) es: \\[\\mathcal{B}_{U_{-7}} = \\left\\{\\left( \\begin{array}{c@{}} -1\\\\ 1\\\\ -2\\\\ 1 \\end{array} \\right) \\right\\}\\] Como solo tenemos un vector en la base, ya forma un sistema ortogonal. Debemos normalizar ahora los vectores de este sistema ortogonal que tenemos, multiplicando cada uno por el inverso de su norma, llegando a la siguiente base ortonormal de \\(U_{ -7 }\\). \\[\\mathcal{B}&#39;_{U_{-7}} = \\left\\{\\frac{1}{\\sqrt{7}}\\left( \\begin{array}{c} -1\\\\ 1\\\\ -2\\\\ 1 \\end{array} \\right)\\right\\} = \\left\\{\\left( \\begin{array}{c} \\frac{-\\sqrt{7}}{7}\\\\ \\frac{\\sqrt{7}}{7}\\\\ \\frac{-2\\cdot\\sqrt{7}}{7}\\\\ \\frac{\\sqrt{7}}{7} \\end{array} \\right)\\right\\}\\] Definimos \\[\\mathcal{B} = \\mathcal{B}&#39;_{U_{0}} \\cup \\mathcal{B}&#39;_{U_{-7}} = \\left\\{\\left( \\begin{array}{c} \\frac{\\sqrt{2}}{2}\\\\ \\frac{\\sqrt{2}}{2}\\\\ 0\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ \\frac{\\sqrt{3}}{3}\\\\ 0 \\end{array} \\right), \\left( \\begin{array}{c} \\frac{\\sqrt{42}}{42}\\\\ \\frac{-\\sqrt{42}}{42}\\\\ \\frac{\\sqrt{42}}{21}\\\\ \\frac{\\sqrt{42}}{7} \\end{array} \\right), \\left( \\begin{array}{c} \\frac{-\\sqrt{7}}{7}\\\\ \\frac{\\sqrt{7}}{7}\\\\ \\frac{-2\\cdot\\sqrt{7}}{7}\\\\ \\frac{\\sqrt{7}}{7} \\end{array} \\right)\\right\\}\\] Es una base ortonormal de \\(\\mathbb{R}^{4}\\), formada únicamente por autovectores de la matriz \\(A\\). Además, la matriz \\(P\\) del cambio de base de esta base \\(\\mathcal{B}\\) a la canónica, formada por los vectores de \\(\\mathcal{B}\\) puestos por columnas, es ortogonal y nos proporciona la relación entre \\(A\\) y la matriz diagonal \\(D\\) de los autovalores: \\[\\begin{array}{rcl}D &amp; = &amp; P^{\\text{t}}\\ A\\ P = \\\\ &amp; = &amp; \\left( \\begin{array}{cccc@{}} \\frac{\\sqrt{2}}{2} &amp; \\frac{\\sqrt{2}}{2} &amp; 0 &amp; 0\\\\ \\frac{-\\sqrt{3}}{3} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{\\sqrt{3}}{3} &amp; 0\\\\ \\frac{\\sqrt{42}}{42} &amp; \\frac{-\\sqrt{42}}{42} &amp; \\frac{\\sqrt{42}}{21} &amp; \\frac{\\sqrt{42}}{7}\\\\ \\frac{-\\sqrt{7}}{7} &amp; \\frac{\\sqrt{7}}{7} &amp; \\frac{-2\\cdot\\sqrt{7}}{7} &amp; \\frac{\\sqrt{7}}{7} \\end{array} \\right) \\ \\left( \\begin{array}{cccc@{}} -1 &amp; 1 &amp; -2 &amp; 1\\\\ 1 &amp; -1 &amp; 2 &amp; -1\\\\ -2 &amp; 2 &amp; -4 &amp; 2\\\\ 1 &amp; -1 &amp; 2 &amp; -1 \\end{array} \\right) \\ \\left( \\begin{array}{cccc@{}} \\frac{\\sqrt{2}}{2} &amp; \\frac{-\\sqrt{3}}{3} &amp; \\frac{\\sqrt{42}}{42} &amp; \\frac{-\\sqrt{7}}{7}\\\\ \\frac{\\sqrt{2}}{2} &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{-\\sqrt{42}}{42} &amp; \\frac{\\sqrt{7}}{7}\\\\ 0 &amp; \\frac{\\sqrt{3}}{3} &amp; \\frac{\\sqrt{42}}{21} &amp; \\frac{-2\\cdot\\sqrt{7}}{7}\\\\ 0 &amp; 0 &amp; \\frac{\\sqrt{42}}{7} &amp; \\frac{\\sqrt{7}}{7} \\end{array} \\right) \\\\ &amp; = &amp; \\left( \\begin{array}{cccc@{}} 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; -7 \\end{array} \\right) \\end{array}\\] "]
]
